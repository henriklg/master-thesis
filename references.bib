
@book{ArtificialIntelligence16,
  title = {Artificial {{Intelligence}} : {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  abstract = {Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; and everything from microelectronic devices to robotic planetary explorers. The book is also big because we go into some depth.},
  language = {en},
  publisher = {{Malaysia; Pearson Education Limited,}},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2016},
  file = {/home/henrik/Zotero/storage/G5Z53ZUJ/Russell and Norvig - 2016 - Artificial Intelligence  A Modern Approach.html}
}

@misc{UnderstandingConvolutional19,
  title = {Understanding {{Convolutional Neural Networks}} through {{Visualizations}} in {{PyTorch}}},
  abstract = {Getting down to the nitty-gritty of CNNs},
  journal = {Towards Data Science},
  howpublished = {https://towardsdatascience.com/understanding-convolutional-neural-networks-through-visualizations-in-pytorch-b5444de08b91},
  author = {Kurama, Vihar},
  month = jan,
  year = {2019},
  file = {/home/henrik/Zotero/storage/KJXQZXET/understanding-convolutional-neural-networks-through-visualizations-in-pytorch-b5444de08b91.html}
}

@misc{PillCamCamera,
  title = {{{PillCam}} - {{A Camera Pill System}} for {{Automatic Screening}} of the {{Digestive System}} - {{Institutt}} for Informatikk.},
  abstract = {In this project, we aim to design and develop a system for analysing video from a camera pill. The pill is~swallowed and records video of the~digestive system - the goal is to be able to automatically detect cancer in the colon.},
  howpublished = {https://www.mn.uio.no/ifi/studier/masteroppgaver/nd/SRL-media-pill-cam.html},
  file = {/home/henrik/Zotero/storage/JAQ2XTAL/SRL-media-pill-cam.html}
}

@article{CancerStatistics10,
  title = {Cancer {{Statistics}}},
  volume = {60},
  issn = {1542-4863},
  abstract = {Each year, the American Cancer Society estimates the number of new cancer cases and deaths expected in the United States in the current year and compiles the most recent data regarding cancer incidence, mortality, and survival based on incidence data from the National Cancer Institute, the Centers for Disease Control and Prevention, and the North American Association of Central Cancer Registries and mortality data from the National Center for Health Statistics. Incidence and death rates are age-standardized to the 2000 US standard million population. A total of 1,529,560 new cancer cases and 569,490 deaths from cancer are projected to occur in the United States in 2010. Overall cancer incidence rates decreased in the most recent time period in both men (1.3\% per year from 2000 to 2006) and women (0.5\% per year from 1998 to 2006), largely due to decreases in the 3 major cancer sites in men (lung, prostate, and colon and rectum [colorectum]) and 2 major cancer sites in women (breast and colorectum). This decrease occurred in all racial/ethnic groups in both men and women with the exception of American Indian/Alaska Native women, in whom rates were stable. Among men, death rates for all races combined decreased by 21.0\% between 1990 and 2006, with decreases in lung, prostate, and colorectal cancer rates accounting for nearly 80\% of the total decrease. Among women, overall cancer death rates between 1991 and 2006 decreased by 12.3\%, with decreases in breast and colorectal cancer rates accounting for 60\% of the total decrease. The reduction in the overall cancer death rates translates to the avoidance of approximately 767,000 deaths from cancer over the 16-year period. This report also examines cancer incidence, mortality, and survival by site, sex, race/ethnicity, geographic area, and calendar year. Although progress has been made in reducing incidence and mortality rates and improving survival, cancer still accounts for more deaths than heart disease in persons younger than 85 years. Further progress can be accelerated by applying existing cancer control knowledge across all segments of the population and by supporting new discoveries in cancer prevention, early detection, and treatment. CA Cancer J Clin 2010. \textcopyright{} 2010 American Cancer Society, Inc.},
  language = {en},
  number = {5},
  journal = {CA: A Cancer Journal for Clinicians},
  doi = {10.3322/caac.20073},
  author = {Jemal, Ahmedin and Siegel, Rebecca and Xu, Jiaquan and Ward, Elizabeth},
  year = {2010},
  keywords = {cancer statistics},
  pages = {277-300},
  file = {/home/henrik/Zotero/storage/UDTTBEGD/Jemal et al. - 2010 - Cancer Statistics, 2010.pdf;/home/henrik/Zotero/storage/D9BH79GY/caac.html}
}

@article{PricingSurgeries12,
  title = {Pricing of Surgeries for Colon Cancer},
  volume = {118},
  issn = {1097-0142},
  abstract = {BACKGROUND: This study examined effects of health maintenance organization (HMO) penetration, hospital competition, and patient severity on the uptake of laparoscopic colectomy and its price relative to open surgery for colon cancer. METHODS: The MarketScan Database (data from 2002-2007) was used to identify admissions for privately insured colorectal cancer patients undergoing laparoscopic or open partial colectomy (n = 1035 and n = 6389, respectively). Patient and health plan characteristics were retrieved from these data; HMO market penetration rates and an index of hospital market concentration, the Herfindahl-Hirschman index (HHI), were derived from national databases. Logistic and logarithmic regressions were used to examine the odds of having laparoscopic colectomy, effect of covariates on colectomy prices, and the differential price of laparoscopy. RESULTS: Adoption of laparoscopy was highly sensitive to market forces, with a 10\% increase in HMO penetration leading to a 10.9\% increase in the likelihood of undergoing laparoscopic colectomy (adjusted odds ratio = 1.109; 95\% confidence interval [CI] = 1.062, 1.158) and a 10\% increase in HHI resulting in 6.6\% lower likelihood (adjusted odds ratio = 0.936; 95\% CI = 0.880, 0.996). Price models indicated that the price of laparoscopy was 7.6\% lower than that of open surgery (transformed coefficient = 0.927; 95\% CI = 0.895, 0.960). A 10\% increase in HMO penetration was associated with 1.6\% lower price (transformed coefficient = 0.985; 95\% CI = 0.977, 0.992), whereas a 10\% increase in HHI was associated with 1.6\% higher price (transformed coefficient = 1.016; 95\% CI = 1.006, 1.027; P {$<$} .001 for all comparisons). CONCLUSIONS: Laparoscopy was significantly associated with lower hospital prices. Moreover, laparoscopic surgery may result in cost savings, while market pressures contribute to its adoption. Cancer 2012. \textcopyright{} 2012 American Cancer Society.},
  language = {en},
  number = {23},
  journal = {Cancer},
  doi = {10.1002/cncr.27573},
  author = {Dor, Avi and Koroukian, Siran and Xu, Fang and Stulberg, Jonah and Delaney, Conor and Cooper, Gregory},
  year = {2012},
  keywords = {colon cancer,laparoscopy,medical prices,surgery,pricing},
  pages = {5741-5748},
  file = {/home/henrik/Zotero/storage/B5XYNSIT/Dor et al. - 2012 - Pricing of surgeries for colon cancer.pdf;/home/henrik/Zotero/storage/IJYE4VB6/cncr.html}
}

@article{HereditaryFamilial10,
  title = {Hereditary and {{Familial Colon Cancer}}},
  volume = {138},
  issn = {00165085},
  language = {en},
  number = {6},
  journal = {Gastroenterology},
  doi = {10.1053/j.gastro.2010.01.054},
  author = {Jasperson, Kory W. and Tuohy, Th{\'e}r{\`e}se M. and Neklason, Deborah W. and Burt, Randall W.},
  month = may,
  year = {2010},
  pages = {2044-2058},
  file = {/home/henrik/Zotero/storage/D6FU3Y83/Jasperson et al. - 2010 - Hereditary and Familial Colon Cancer.pdf}
}

@article{ColorectalCancer10,
  title = {Colorectal {{Cancer}}: {{National}} and {{International Perspective}} on the {{Burden}} of {{Disease}} and {{Public Health Impact}}},
  volume = {138},
  issn = {00165085},
  shorttitle = {Colorectal {{Cancer}}},
  language = {en},
  number = {6},
  journal = {Gastroenterology},
  doi = {10.1053/j.gastro.2010.01.056},
  author = {Gellad, Ziad F. and Provenzale, Dawn},
  month = may,
  year = {2010},
  pages = {2177-2190},
  file = {/home/henrik/Zotero/storage/SR6Z7LRC/Gellad and Provenzale - 2010 - Colorectal Cancer National and International Pers.pdf}
}

@article{ProgressChallenges10,
  title = {Progress and {{Challenges}} in {{Colorectal Cancer Screening}} and {{Surveillance}}},
  volume = {138},
  issn = {00165085},
  language = {en},
  number = {6},
  journal = {Gastroenterology},
  doi = {10.1053/j.gastro.2010.02.006},
  author = {Lieberman, David},
  month = may,
  year = {2010},
  pages = {2115-2126},
  file = {/home/henrik/Zotero/storage/5ZJHN63L/Lieberman - 2010 - Progress and Challenges in Colorectal Cancer Scree.pdf}
}

@article{CompetitiveNeural13,
  title = {A {{Competitive Neural Network}} for {{Multiple Object Tracking}} in {{Video Sequence Analysis}}},
  volume = {37},
  issn = {1370-4621, 1573-773X},
  abstract = {Tracking of moving objects in real situation is a challenging research issue, due to dynamic changes in objects or background appearance, illumination, shape and occlusions. In this paper, we deal with these difficulties by incorporating an adaptive feature weighting mechanism to the proposed growing competitive neural network for multiple objects tracking. The neural network takes advantage of the most relevant object features (information provided by the proposed adaptive feature weighting mechanism) in order to estimate the trajectories of the moving objects. The feature selection mechanism is based on a genetic algorithm, and the tracking algorithm is based on a growing competitive neural network where each unit is associated to each object in the scene. The proposed methods (object tracking and feature selection mechanism) are applied to detect the trajectories of moving vehicles in roads. Experimental results show the performance of the proposed system compared to the standard Kalman filter.},
  language = {en},
  number = {1},
  journal = {Neural Processing Letters},
  doi = {10.1007/s11063-012-9268-3},
  author = {{Luque-Baena}, Rafael M. and {Ortiz-de-Lazcano-Lobato}, Juan M. and {L{\'o}pez-Rubio}, Ezequiel and Dom{\'i}nguez, Enrique and J. Palomo, Esteban},
  month = feb,
  year = {2013},
  keywords = {object tracking},
  pages = {47-67},
  file = {/home/henrik/Zotero/storage/HC7K3L48/Luque-Baena et al. - 2013 - A Competitive Neural Network for Multiple Object T.pdf}
}

@article{ObjectTracking15,
  title = {Object {{Tracking Benchmark}}},
  volume = {37},
  issn = {0162-8828},
  abstract = {Object tracking has been one of the most important and active research areas in the field of computer vision. A large number of tracking algorithms have been proposed in recent years with demonstrated success. However, the set of sequences used for evaluation is often not sufficient or is sometimes biased for certain types of algorithms. Many datasets do not have common ground-truth object positions or extents, and this makes comparisons among the reported quantitative results difficult. In addition, the initial conditions or parameters of the evaluated tracking algorithms are not the same, and thus, the quantitative results reported in literature are incomparable or sometimes contradictory. To address these issues, we carry out an extensive evaluation of the state-of-the-art online object-tracking algorithms with various evaluation criteria to understand how these methods perform within the same framework. In this work, we first construct a large dataset with ground-truth object positions and extents for tracking and introduce the sequence attributes for the performance analysis. Second, we integrate most of the publicly available trackers into one code library with uniform input and output formats to facilitate large-scale performance evaluation. Third, we extensively evaluate the performance of 31 algorithms on 100 sequences with different initialization settings. By analyzing the quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.},
  number = {9},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2014.2388226},
  author = {Wu, Y. and Lim, J. and Yang, M.},
  month = sep,
  year = {2015},
  keywords = {object tracking,Algorithm design and analysis,benchmark dataset,Histograms,Object tracking,performance evaluation,Performance evaluation,Robustness,Target tracking},
  pages = {1834-1848},
  file = {/home/henrik/Zotero/storage/QI7DVTQK/Wu et al. - 2015 - Object Tracking Benchmark.pdf;/home/henrik/Zotero/storage/SA4AYB9C/7001050.html}
}

@misc{GOTURNDeep,
  title = {{{GOTURN}} : {{Deep Learning}} Based {{Object Tracking}} | {{Learn OpenCV}}},
  howpublished = {https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/}
}

@article{WirelessCapsule00,
  title = {Wireless Capsule Endoscopy},
  volume = {405},
  copyright = {2000 Macmillan Magazines Ltd.},
  issn = {1476-4687},
  abstract = {The discomfort of internal gastrointestinal examination may soon be a thing of the past.},
  language = {En},
  number = {6785},
  journal = {Nature},
  doi = {10.1038/35013140},
  author = {Iddan, Gavriel and Meron, Gavriel and Glukhovsky, Arkady and Swain, Paul},
  month = may,
  year = {2000},
  pages = {417},
  file = {/home/henrik/Zotero/storage/U6JJ6MIK/Iddan et al. - 2000 - Wireless capsule endoscopy.pdf;/home/henrik/Zotero/storage/HS7TCR72/35013140.html}
}

@misc{CapsuleEndoscopyAlanCrawford56a11c235f9b58b7d0bbcd15Jpg,
  title = {Capsule-{{Endoscopy}}-{{Alan}}-{{Crawford}}-56a11c235f9b58b7d0bbcd15.Jpg (2800\texttimes{}2100)},
  howpublished = {https://www.verywellhealth.com/thmb/IVrSo77yi8FT4dc0laOqgVbSIDg=/2800x2100/filters:fill(87E3EF,1)/Capsule-Endoscopy-Alan-Crawford-56a11c235f9b58b7d0bbcd15.jpg},
  file = {/home/henrik/Zotero/storage/QQEAC6K6/Capsule-Endoscopy-Alan-Crawford-56a11c235f9b58b7d0bbcd15.html}
}

@misc{MedicalPhysics,
  title = {Medical {{Physics}} - {{Endoscopes}}},
  howpublished = {http://www.genesis.net.au/\textasciitilde{}ajs/projects/medical\_physics/endoscopes/index.html},
  file = {/home/henrik/Zotero/storage/YTY6DUKB/index.html}
}

@article{NewMethod54,
  title = {A {{New Method}} of Transporting {{Optical Images}} without {{Aberrations}}},
  volume = {173},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {4392},
  journal = {Nature},
  doi = {10.1038/173039a0},
  author = {Van Heel, A. C. S.},
  month = jan,
  year = {1954},
  pages = {39-39},
  file = {/home/henrik/Zotero/storage/R3TXS4M8/Van Heel - 1954 - A New Method of transporting Optical Images withou.pdf}
}

@inproceedings{ExpertDriven15,
  address = {{Portland, Oregon}},
  title = {Expert Driven Semi-Supervised Elucidation Tool for Medical Endoscopic Videos},
  isbn = {978-1-4503-3351-1},
  abstract = {In this paper, we present a novel application for elucidating all kind of videos that require expert knowledge, e.g., sport videos, medical videos etc., focusing on endoscopic surgery and video capsule endoscopy. In the medical domain, the knowledge of experts for tagging and interpretation of videos is of high value. As a result of the stressful working environment of medical doctors, they often simply do not have time for extensive annotations. We therefore present a semisupervised method to gather the annotations in a very easy and time saving way for the experts and we show how this information can be used later on.},
  booktitle = {Proceedings of the 6th {{ACM Multimedia Systems Conference}}},
  publisher = {{ACM Press}},
  doi = {10.1145/2713168.2713184},
  author = {Albisser, Zeno and Riegler, Michael and Halvorsen, P{\aa}l and Zhou, Jiang and Griwodz, Carsten and Balasingham, Ilangko and Gurrin, Cathal},
  year = {2015},
  pages = {73-76},
  file = {/home/henrik/Zotero/storage/3V2GE6II/Albisser et al. - 2015 - Expert driven semi-supervised elucidation tool for.pdf}
}

@misc{ImageColon13,
  title = {Image of Colon},
  shorttitle = {Deutsch},
  author = {Dr.HH.Krause},
  month = jun,
  year = {2013},
  file = {/home/henrik/Zotero/storage/TSYSJMVF/FileNormales_Colon.html}
}

@misc{SmallIntestine13,
  title = {Small Intestine},
  shorttitle = {Deutsch},
  author = {{Dr.HH.Krause}},
  month = jun,
  year = {2013},
  file = {/home/henrik/Zotero/storage/9VVYEPIQ/FileDünndarm.html}
}

@article{DigestiveSystem,
  title = {Digestive System Diagram},
  copyright = {Creative Commons Attribution-ShareAlike License},
  shorttitle = {Digestive {{System}}},
  language = {en},
  journal = {Wikipedia},
  file = {/home/henrik/Zotero/storage/CAT5AHNQ/FileDigestive_system_diagram_edit.html}
}

@inproceedings{DeepConvolutional16,
  title = {A Deep Convolutional Neural Network for Bleeding Detection in {{Wireless Capsule Endoscopy}} Images},
  abstract = {Wireless Capsule Endoscopy (WCE) is a standard non-invasive modality for small bowel examination. Recently, the development of computer-aided diagnosis (CAD) systems for gastrointestinal (GI) bleeding detection in WCE image videos has become an active research area with the goal of relieving the workload of physicians. Existing methods based primarily on handcrafted features usually give insufficient accuracy for bleeding detection, due to their limited capability of feature representation. In this paper, we present a new automatic bleeding detection strategy based on a deep convolutional neural network and evaluate our method on an expanded dataset of 10,000 WCE images. Experimental results with an increase of around 2 percentage points in the Fi score demonstrate that our method outperforms the state-of-the-art approaches in WCE bleeding detection. The achieved Fi score is of up to 0.9955.},
  booktitle = {Proceedings of the 2016 38th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  doi = {10.1109/EMBC.2016.7590783},
  author = {Jia, X. and Meng, M. Q.-},
  month = aug,
  year = {2016},
  keywords = {wireless capsule endoscopy,automatic bleeding detection strategy,biomedical optical imaging,CAD,Capsule Endoscopy,computer-aided diagnosis systems,deep convolutional neural network,Diagnosis; Computer-Assisted,endoscopes,Endoscopes,Feature extraction,feature representation,Fi score,gastrointestinal bleeding detection,Gastrointestinal Hemorrhage,Hemorrhaging,Humans,medical disorders,medical image processing,neural nets,Neural Networks (Computer),small bowel examination,standard noninvasive modality,Support vector machines,Training,Videos,WCE bleeding detection,WCE image videos,Wireless communication},
  pages = {639-642},
  file = {/home/henrik/Zotero/storage/AL7M2D2G/Jia and Meng - 2016 - A deep convolutional neural network for bleeding d.pdf;/home/henrik/Zotero/storage/GK7TEZ73/7590783.html}
}

@inproceedings{ClassifyingDigestive15,
  title = {Classifying Digestive Organs in Wireless Capsule Endoscopy Images Based on Deep Convolutional Neural Network},
  abstract = {This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95\% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.},
  booktitle = {Proceedings of the 2015 {{IEEE International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  doi = {10.1109/ICDSP.2015.7252086},
  author = {Zou, Y. and Li, L. and Wang, Y. and Yu, J. and Li, Y. and Deng, W. J.},
  month = jul,
  year = {2015},
  keywords = {wireless capsule endoscopy,biomedical optical imaging,deep convolutional neural network,endoscopes,Endoscopes,Feature extraction,medical image processing,Training,Wireless communication,Accuracy,biological organs,brightness,complex digestive tract circumstance,Convolution,DCNN-WCE-CS,deep CNN-based WCE classification system,digestive organ classification problem,digestive organs classification,feature extraction,feedforward neural nets,human biological visual systems,image classification,Intestines,layer-wise hierarchy models,learning (artificial intelligence),luminance change,object recognition,parameter selection,semantic image feature recognition,training data,wireless capsule endoscopy images,detecting organs},
  pages = {1274-1278},
  file = {/home/henrik/Zotero/storage/7N333XQA/Zou et al. - 2015 - Classifying digestive organs in wireless capsule e.pdf;/home/henrik/Zotero/storage/X9LW93CT/7252086.html}
}

@article{NotesBackpropagation16,
  title = {Notes on Backpropagation},
  author = {Sadowski, Peter},
  year = {2016},
  file = {/home/henrik/Zotero/storage/SC592RJV/Sadowski - 2016 - Notes on backpropagation.pdf}
}

@inproceedings{LesionDetection15,
  title = {Lesion Detection of Endoscopy Images Based on Convolutional Neural Network Features},
  abstract = {Since gastroscopy is able to observe the interior of gastrointestinal tract directly, it has been widely used for gastrointestinal examination. But it is hard for clinicians to accurately detect gastrointestinal disease due to its great dependence on doctors experiences. Therefore, a computer-aided lesion detection system can offer great help for clinicians. In this paper, we propose a new scheme for endoscopy image lesion detection. A trainable feature extractor based on convolutional neural network (CNN) is utilized to get more generic features for endoscopy images. And features are fed to support vector machine (SVM) to enhance the generalization ability. Experiments show that the proposed scheme outperforms the previous conventional methods based on color and texture features.},
  booktitle = {Proceedings of the 2015 8th {{International Congress}} on {{Image}} and {{Signal Processing}} ({{CISP}})},
  doi = {10.1109/CISP.2015.7407907},
  author = {Zhu, R. and Zhang, R. and Xue, D.},
  month = oct,
  year = {2015},
  keywords = {cancer,Histograms,endoscopes,Endoscopes,Feature extraction,medical image processing,neural nets,Support vector machines,feature extraction,CNN,color features,computer-aided lesion detection system,convolutional neural network features,detect gastrointestinal disease,endoscopy image lesion detection,gastrointestinal examination,Gastrointestinal tract,gastrointestinal tract directly,Image color analysis,image colour analysis,image texture,Lesions,support vector machine,support vector machines,SVM,texture features,trainable feature extractor},
  pages = {372-376},
  file = {/home/henrik/Zotero/storage/9ILDKALR/Zhu et al. - 2015 - Lesion detection of endoscopy images based on conv.pdf;/home/henrik/Zotero/storage/XMXK62TU/7407907.html}
}

@article{DeepEndoVO18,
  title = {Deep {{EndoVO}}: {{A}} Recurrent Convolutional Neural Network ({{RCNN}}) Based Visual Odometry Approach for Endoscopic Capsule Robots},
  volume = {275},
  issn = {0925-2312},
  shorttitle = {Deep {{EndoVO}}},
  abstract = {Ingestible wireless capsule endoscopy is an emerging minimally invasive diagnostic technology for inspection of the GI tract and diagnosis of a wide range of diseases and pathologies. Medical device companies and many research groups have recently made substantial progresses in converting passive capsule endoscopes to active capsule robots, enabling more accurate, precise, and intuitive detection of the location and size of the diseased areas. Since a reliable real time pose estimation functionality is crucial for actively controlled endoscopic capsule robots, in this study, we propose a monocular visual odometry (VO) method for endoscopic capsule robot operations. Our method lies on the application of the deep recurrent convolutional neural networks (RCNNs) for the visual odometry task, where convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are used for the feature extraction and inference of dynamics across the frames, respectively. Detailed analyses and evaluations made on a real pig stomach dataset proves that our system achieves high translational and rotational accuracies for different types of endoscopic capsule robot trajectories.},
  journal = {Neurocomputing},
  doi = {10.1016/j.neucom.2017.10.014},
  author = {Turan, Mehmet and Almalioglu, Yasin and Araujo, Helder and Konukoglu, Ender and Sitti, Metin},
  month = jan,
  year = {2018},
  keywords = {CNN,Endoscopic capsule robot,Localization,LSTM,RCNN,Sequential deep learning,Visual odometry},
  pages = {1861-1870},
  file = {/home/henrik/Zotero/storage/IW7GIJDZ/Turan et al. - 2018 - Deep EndoVO A recurrent convolutional neural netw.pdf;/home/henrik/Zotero/storage/HV6QI5IW/S092523121731665X.html}
}

@article{DeepLearning17,
  title = {Deep Learning for Polyp Recognition in Wireless Capsule Endoscopy Images},
  volume = {44},
  issn = {00942405},
  abstract = {Purpose: Wireless capsule endoscopy (WCE) enables physicians to examine the digestive tract without any surgical operations, at the cost of a large volume of images to be analyzed. In the computer-aided diagnosis of WCE images, the main challenge arises from the difficulty of robust characterization of images. This study aims to provide discriminative description of WCE images and assist physicians to recognize polyp images automatically.
Methods: We propose a novel deep feature learning method, named stacked sparse autoencoder with image manifold constraint (SSAEIM), to recognize polyps in the WCE images. Our SSAEIM differs from the traditional sparse autoencoder (SAE) by introducing an image manifold constraint, which is constructed by a nearest neighbor graph and represents intrinsic structures of images. The image manifold constraint enforces that images within the same category share similar learned features and images in different categories should be kept far away. Thus, the learned features preserve large intervariances and small intravariances among images.
Results: The average overall recognition accuracy (ORA) of our method for WCE images is 98.00\%. The accuracies for polyps, bubbles, turbid images, and clear images are 98.00\%, 99.50\%, 99.00\%, and 95.50\%, respectively. Moreover, the comparison results show that our SSAEIM outperforms existing polyp recognition methods with relative higher ORA.
Conclusion: The comprehensive results have demonstrated that the proposed SSAEIM can provide descriptive characterization for WCE images and recognize polyps in a WCE video accurately. This method could be further utilized in the clinical trials to help physicians from the tedious image reading work. \textcopyright{} 2017 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.12147]},
  language = {en},
  number = {4},
  journal = {Medical Physics},
  doi = {10.1002/mp.12147},
  author = {Yuan, Yixuan and Meng, Max Q.-H.},
  month = apr,
  year = {2017},
  pages = {1379-1389},
  file = {/home/henrik/Zotero/storage/SAXC9CNE/Yuan and Meng - 2017 - Deep learning for polyp recognition in wireless ca.pdf}
}

@inproceedings{IdentificationUlcers09,
  title = {Identification of Ulcers in {{Wireless Capsule Endoscopy}} Videos},
  abstract = {Wireless Capsule Endoscopy (WCE) is a non invasive procedure which is used to view the lower gastrointestinal tract. Physicians can detect diseases such as bleeding, Crohn's disease, peptic ulcers, and colon cancer. In this paper a methodology is presented to identify peptic ulcers in the small intestine automatically. It first performs color transformation into the HSV color space; it utilizes log Gabor filters to find meaningful regions. A segmentation scheme is used to extract color information of these meaningful regions in the original RGB color space. Additionally, texture information is extracted and together with color values are fed into an artificial neural network for classification. Illustrative results from the methodology are also given in this paper.},
  booktitle = {Proceedings of the 2009 {{IEEE International Symposium}} on {{Biomedical Imaging}}: {{From Nano}} to {{Macro}}},
  doi = {10.1109/ISBI.2009.5193107},
  author = {Karargyris, A. and Bourbakis, N.},
  month = jun,
  year = {2009},
  keywords = {colon cancer,endoscopes,Endoscopes,Hemorrhaging,medical image processing,neural nets,Videos,biological organs,feature extraction,image classification,Intestines,Gastrointestinal tract,artificial neural network,bleeding,Cancer detection,Colon,color information extraction,Crohn's disease,Data mining,diseases,Diseases,fuzzy least squares support vector machines,fuzzy region segmentation,Gabor filters,gastrointestinal tract,HSV color space,image segmentation,log Gabor filters,log-Gabor filters,noninvasive procedure,peptic ulcers,RGB color space,segmentation scheme,small intestine,texture,texture information,ulcers,video signal processing,Wireless capsule Endoscopy Imaging,wireless capsule endoscopy videos},
  pages = {554-557},
  file = {/home/henrik/Zotero/storage/6VBSP4UY/Karargyris and Bourbakis - 2009 - Identification of ulcers in Wireless Capsule Endos.pdf;/home/henrik/Zotero/storage/BW92FMMW/5193107.html}
}

@article{DeepConvolutional13,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6034},
  primaryClass = {cs},
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  journal = {arXiv:1312.6034 [cs]},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/henrik/Zotero/storage/DSPZ2LDD/Simonyan et al. - 2013 - Deep Inside Convolutional Networks Visualising Im.pdf;/home/henrik/Zotero/storage/628HFNB7/1312.html}
}

@article{CNNbasedSegmentation17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.03056},
  primaryClass = {cs},
  title = {{{CNN}}-Based {{Segmentation}} of {{Medical Imaging Data}}},
  abstract = {Convolutional neural networks have been applied to a wide variety of computer vision tasks. Recent advances in semantic segmentation have enabled their application to medical image segmentation. While most CNNs use two-dimensional kernels, recent CNN-based publications on medical image segmentation featured three-dimensional kernels, allowing full access to the three-dimensional structure of medical images. Though closely related to semantic segmentation, medical image segmentation includes specific challenges that need to be addressed, such as the scarcity of labelled data, the high class imbalance found in the ground truth and the high memory demand of three-dimensional images. In this work, a CNN-based method with three-dimensional filters is demonstrated and applied to hand and brain MRI. Two modifications to an existing CNN architecture are discussed, along with methods on addressing the aforementioned challenges. While most of the existing literature on medical image segmentation focuses on soft tissue and the major organs, this work is validated on data both from the central nervous system as well as the bones of the hand.},
  journal = {arXiv:1701.03056 [cs]},
  author = {Kayalibay, Baris and Jensen, Grady and {van der Smagt}, Patrick},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Betina},
  file = {/home/henrik/Zotero/storage/TPRA2E2Y/Kayalibay et al. - 2017 - CNN-based Segmentation of Medical Imaging Data.pdf;/home/henrik/Zotero/storage/X8T859BV/1701.html}
}

@inproceedings{UNetConvolutional15,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  isbn = {978-3-319-24574-4},
  shorttitle = {U-{{Net}}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  language = {en},
  booktitle = {Proceedings of the {{Medical Image Computing}} and {{Computer}}-{{Assisted Intervention}}},
  publisher = {{Springer International Publishing}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  keywords = {Betina,Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  pages = {234-241},
  file = {/home/henrik/Zotero/storage/HVJEMHFB/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@incollection{ImageNetClassification12,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  pages = {1097--1105},
  file = {/home/henrik/Zotero/storage/8C5IIC5A/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf;/home/henrik/Zotero/storage/ID4BRJ6C/4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}

@article{DeepReinforcement17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.08936},
  primaryClass = {cs},
  title = {Deep {{Reinforcement Learning}} for {{Visual Object Tracking}} in {{Videos}}},
  abstract = {In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than real-time. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.},
  journal = {arXiv:1701.08936 [cs]},
  author = {Zhang, Da and Maei, Hamid and Wang, Xin and Wang, Yuan-Fang},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/henrik/Zotero/storage/7VJDK8ZD/Zhang et al. - 2017 - Deep Reinforcement Learning for Visual Object Trac.pdf;/home/henrik/Zotero/storage/NGVQ2SIY/1701.html}
}

@inproceedings{FullyConvolutional15,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431-3440},
  file = {/home/henrik/Zotero/storage/DEGCVEFG/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf;/home/henrik/Zotero/storage/DIC8VB4W/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html}
}

@article{ShapeShading94,
  title = {Shape from Shading Using Linear Approximation},
  volume = {12},
  issn = {0262-8856},
  abstract = {In this paper, we present an extremely simple algorithm for shape from shading, which can be implemented in 25 lines of C code{${_\ast}{_\ast}$}C code and some images can be obtained by anonymous ftp from under the directory /pub/shading.. The algorithm is very fast, taking 0.2 seconds on a Sun SparcStation-1 for a 128 \texttimes{} 128 image, and is purely local and highly parallelizable (parallel implementation included). In our approach, we employ a linear approximation of the reflectance function, as used by others. However, the main difference is that we first use the discrete approximations for surface normal, p and q, using finite differences, and then linearize the reflectance function in depth, Z(x, y), instead of p and q. The algorithm has been tested on several synthetic and real images of both Lambertian and specular surfaces, and good results have been obtained.},
  number = {8},
  journal = {Image and Vision Computing},
  doi = {10.1016/0262-8856(94)90002-7},
  author = {{Ping-Sing}, Tsai and Shah, Mubarak},
  month = oct,
  year = {1994},
  keywords = {3D shape,physics-based vision,shape from shading},
  pages = {487-498},
  file = {/home/henrik/Zotero/storage/RP6KRHXZ/0262885694900027.html}
}

@misc{DemystifyingConvolutional18,
  title = {Demystifying {{Convolutional Neural Networks}}},
  abstract = {An Intuitive Explanation of Convolutional Neural Networks.},
  language = {en},
  journal = {Medium},
  howpublished = {https://medium.com/@eternalzer0dayx/demystifying-convolutional-neural-networks-ca17bdc75559},
  author = {Zerium, Aegeus},
  month = sep,
  year = {2018},
  file = {/home/henrik/Zotero/storage/BFDGM3IN/demystifying-convolutional-neural-networks-ca17bdc75559.html}
}

@article{DynamicRouting17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1710.09829},
  primaryClass = {cs},
  title = {Dynamic {{Routing Between Capsules}}},
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  journal = {arXiv:1710.09829 [cs]},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  month = oct,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/henrik/Zotero/storage/VAK5X6YX/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf;/home/henrik/Zotero/storage/756F7S8K/1710.html}
}

@article{BleedingDetection19,
  title = {Bleeding Detection in Wireless Capsule Endoscopy Videos \textemdash{} {{Color}} versus Texture Features},
  volume = {20},
  issn = {1526-9914},
  abstract = {Abstract Wireless capsule endoscopy (WCE) is an effective technology that can be used to make a gastrointestinal (GI) tract diagnosis of various lesions and abnormalities. Due to a long time required to pass through the GI tract, the resulting WCE data stream contains a large number of frames which leads to a tedious job for clinical experts to perform a visual check of each and every frame of a complete patient?s video footage. In this paper, an automated technique for bleeding detection based on color and texture features is proposed. The approach combines the color information which is an essential feature for initial detection of frame with bleeding. Additionally, it uses the texture which plays an important role to extract more information from the lesion captured in the frames and allows the system to distinguish finely between borderline cases. The detection algorithm utilizes machine-learning-based classification methods, and it can efficiently distinguish between bleeding and nonbleeding frames and perform pixel-level segmentation of bleeding areas in WCE frames. The performed experimental studies demonstrate the performance of the proposed bleeding detection method in terms of detection accuracy, where we are at least as good as the state-of-the-art approaches. In this research, we have conducted a broad comparison of a number of different state-of-the-art features and classification methods that allows building an efficient and flexible WCE video processing system.},
  number = {8},
  journal = {Journal of Applied Clinical Medical Physics},
  doi = {10.1002/acm2.12662},
  author = {Pogorelov, Konstantin and Suman, Shipra and Azmadi Hussin, Fawnizu and Saeed Malik, Aamir and Ostroukhova, Olga and Riegler, Michael and Halvorsen, P{\aa}l and Hooi Ho, Shiaw and Goh, Khean-Lee},
  month = aug,
  year = {2019},
  keywords = {wireless capsule endoscopy,bleeding detection,color feature,machine learning,texture feature},
  pages = {141-154},
  file = {/home/henrik/Zotero/storage/YABA4NNF/Pogorelov et al. - 2019 - Bleeding detection in wireless capsule endoscopy v.pdf;/home/henrik/Zotero/storage/IRVPWGI2/acm2.html}
}

@phdthesis{MimirAutomatic18,
  title = {{Mimir: An Automatic Reporting and Reasoning System for Screening of the Gastrointestinal Tract Using Deep Neural Networks}},
  shorttitle = {{Mimir}},
  abstract = {Data is arguably one of the most valuable resources available today. More
than ever, data is collected on such a large scale that we do not have
the capacity to process it efficiently. In healthcare alone, there is an
estimated 162 exabyte of data throughout the world, which is growing at
the speed of approximately 2.5 exabytes per year [18]. Medical data in
and of itself can be used for many things, such as patient follow-ups or
recommendations. Nevertheless, to enable the use of this information to
its fullest potential, we need sophisticated data analysis methods such as
statistics or machine learning. Machine learning is a field where machines
learn from data without explicitly being programmed. This process is
often applied through supervised learning (machines learning from labeled
data), unsupervised learning (machines learning from unlabeled data), or
semi-supervised (machines learning from a combination of labeled and
unlabeled data). Over the past few years, this field has been dominated
by a growing class of algorithms known as deep learning. Inspired by
the neurological connections in the animal brain, deep learning has made
immense strides in the production of state-of-the-art results within many
areas of data analytics [4]. Nowadays, deep learning based methods have
become a popular topic within the medical field as well [7]. This has
brought up some specific challenges which may make the application of
these methods difficult, such as the lack of data or poor understanding
of their internal workings. The latter issue, namely that deep learning is
something of a ``black box'', is one of the biggest hurdles since it hinders
the application of deep learning from being used in hospitals due to lack
of trust and understanding. For this reason, we developed a medical
reporting system, which focuses on transparency and understanding of
its internal processes. In this thesis, we present this system and show
how it may aid us in the development and understanding of deep neural
networks.},
  language = {nob},
  author = {Hicks, Steven Alexander},
  year = {2018},
  file = {/home/henrik/Zotero/storage/M6ILEV7V/Hicks - 2018 - Mimir An Automatic Reporting and Reasoning System.pdf;/home/henrik/Zotero/storage/69JSKP4S/65179.html}
}

@phdthesis{MachineLearning19,
  title = {A {{Machine Learning Approach To Improve Consistency In User}}-{{Driven Medical Image Analysis}}},
  abstract = {The work we present in this thesis stems out from the need of standardisation of training image analysts. We will particularly focus on the training
of image analysis using a growingly popular medical imaging technique,
T1 mapping MRI. Its usefulness relies on the ability to detect abnormalities
in the cardiac tissue myocardial structure due to a range of pathologies in a
non invasive and mostly contrast-agent free manner. T1 mapping is not yet
a routinely used clinical imaging modality, but as more evidence of its potential is published, it is foreseen by experts to soon become a fundamental
clinical tool.},
  school = {University of Oslo},
  author = {Eriksen, Edvarda},
  year = {2019}
}

@phdthesis{UnsupervisedPreprocessing19,
  type = {Master's {{Thesis}}},
  title = {Unsupervised Preprocessing of Medical Imaging Data with Generative Adversarial Networks},
  abstract = {As an attempt to address the challenge of improving the field of computer-aided
diagnosis, our work explores ways to help existing models to increase their accuracy
when it comes to finding anomalies in medical images. In this thesis, we tackle the
problems associated with the misclassification of data based on overlays and other
artefacts in the medical image data.
We will look at how we can use machine learning to develop a system to increase
the classification accuracy of existing models, as well as going in-depth into the topic
of preprocessing to see if it has a place in modern classification models based on deep
learning.},
  school = {University of Oslo},
  author = {Kirker{\o}d, Mathias},
  year = {2019},
  file = {/home/henrik/Zotero/storage/VACL4IVN/Kirkerød - 2019 - Unsupervised preprocessing of medical imaging data.pdf}
}

@phdthesis{HyperparameterOptimization18,
  title = {Hyperparameter Optimization Using {{Bayesian}} Optimization on Transfer Learning for Medical Image Classification},
  abstract = {The field of medicine has a history of adopting new technology. Video equipment and sensors are used to visualize areas of interest in the human allowing for doctors to make diagnoses based on imagery observations. However, the detection rate of the doctors towards diseases and abnormalities is heavily dependent on the experience and state of mind of the doctor doing the examination. Computer-aided detection systems are systems designed to aid the doctor in improving the detection rate, and they are using or experimenting with machine learning. Deep convolutional neural networks, a type of machine learning, are shown to be highly efficient at image detection, classification, and analysis. However, these networks require large datasets to train properly. Transfer learning is a training technique where we use a pre-trained machine learning model and transfer some of the attained knowledge from other application domains over to a new model. This way, we can use small datasets and train a model in much shorter time. In this respect, transfer learning works fine but has many configurations called hyperparameters which are often not optimized. Our work aims to address the lack of automatic hyperparameter optimization for transfer learning by experiments utilizing a known hyperparameter optimization method and creating a system for running those experiments. First, we decided to focus on the field of gastroenterology by utilizing two publicly available datasets showing images from the gastrointestinal tract. Next, we used a specific transfer learning method and chose hyperparameters suitable for automatic optimization. The optimization method we chose was Bayesian optimization because of its reputation for being one of the best methods for hyperparameter optimization. However, Bayesian optimization has hyperparameters of its own, and there are also different versions of Bayesian optimization. We chose to limit the thesis, so we use standard Bayesian optimization with standard parameters. We created a system for running automatic experiments of three different hyperparameter optimization strategies. With the system, we ran a set of experiments for each dataset. Between the strategies, one was successful in achieving a high validation accuracy, while the others were considered failures. Compared to baselines, our best models was around 10\% better. With these experiments, we demonstrated that automatic hyperparameter optimization is an effective strategy for increasing performance in transfer learning and that the best hyperparameters are nontrivial to select manually.},
  language = {eng},
  author = {Borgli, Rune Johan},
  year = {2018},
  file = {/home/henrik/Zotero/storage/F3CTSEMU/Borgli - 2018 - Hyperparameter optimization using Bayesian optimiz.pdf;/home/henrik/Zotero/storage/DGPRG2IR/64146.html}
}

@phdthesis{MedicalMultimedia17,
  type = {Master's {{Thesis}}},
  title = {A {{Medical Multimedia Real}}-{{Time Polyp Detection System}} Using {{Low Computational Resources}}},
  abstract = {In this research, the focal point has been on real-time polyp detection on computers
with low computational resources with the help of open source libraries such as LIRE
Lucene, OpenCV.},
  school = {University of Oslo},
  author = {Khan, Asif Qayyum},
  year = {2017},
  file = {/home/henrik/Zotero/storage/5RYV6QNB/Khan - 2017 - A Medical Multimedia Real-Time Polyp Detection Sys.pdf}
}

@phdthesis{PolypDetection17,
  type = {Master's {{Thesis}}},
  title = {Polyp {{Detection}}: {{Effect}} of {{Early}} and {{Late Feature Fusion}}},
  shorttitle = {Polyp {{Detection}}},
  abstract = {In this thesis, we look at a specific component of these learning
methods and how they affect performance in aiding medical systems. This
component, called feature fusion, has two widely adopted variations: early
fusion and late fusion. We seek to compare the performance of early and
late fusion for medical diagnosis problems through image datasets, and
provide some insight to data scientists on how our results can help their
decision when building a practical system.},
  school = {University of Oslo},
  author = {Asskali, Salman},
  year = {2017},
  file = {/home/henrik/Zotero/storage/TAPV4X24/Asskali - 2017 - Polyp Detection Effect of Early and Late Feature .pdf}
}

@phdthesis{PolypDetection17a,
  title = {Polyp {{Detection}} Using {{Neural Networks}} - {{Data Enhancement}} and {{Training Optimization}}},
  abstract = {Colorectal cancer is the third most common type of cancer diagnosed for men and the second most for women. Today's main methods of examination are expensive, time consuming and intrusive for the patient. Recent technologies, such as CAD and ACD, aims to increase automation in the screening and examination processes. CAD could aid medical professionals during examinations by providing a second opinion, while ACD could be used to screen entire populations, and thus relieving pressure on the health care system. In recent years, neural networks have gained traction among researchers in topics regarding recognition, and we believe it can be utilized in these automated systems. In this thesis, we examine the performance of neural networks for polyp detection. We also explore how data enhancement affect the training and evaluation of the networks, and if it can be used to increase the polyp detection rate. Finally, we experiment with how various training techniques can be used to increase performance. We conclude that neural networks are suitable for polyp detection. We show how data enhancement and training optimization can be used to increase different aspects of the performance. We discuss what aspects are suitable for different scenarios. At the end, we also discuss how our system can be used to detect polyps per frame, per sequence and per polyp, and what the results of our system look like using the different metrics. Detection per frame can be considered a computer science viewpoint, while detection per sequence or per polyp is more of a medical field viewpoint.},
  author = {Jensen, Rune},
  year = {2017},
  keywords = {Artificial neural network,Biological Neural Networks,Classification,Computer Assisted Diagnosis,Computer-aided design,Endoscopy of stomach,Greater,Machine learning,Mathematical optimization,Neoplasms,Neural Network Simulation,Object detection,Open-source software,polyps,Population,TensorFlow,Tract (literature),Traction TeamPage,Urinary tract infection,Weight},
  file = {/home/henrik/Zotero/storage/DG89EWW9/Jensen - 2017 - Polyp Detection using Neural Networks - Data Enhan.pdf}
}

@phdthesis{AutomaticAnalysis17,
  title = {Automatic {{Analysis}} of {{Endoscopic Videos}}},
  abstract = {The human digestive system can be affected by many types of diseases. For example, three of the six most common cancer types (esophagus, stomach and colorectal) are located in the gastrointestinal tract. Colorectal cancer (CRC) is the third most common cancer in men and the second most common cancer in women worldwide, and Norway has one of the highest incidences of this cancer. Early detection is vital for the prognosis, level of treatment and survival. EIR is a multimedia system with the main objective of supporting doctors in gastrointestinal tract disease detection, both as a live examination system and an offline system for VCE. However, the detection and automatic analysis subsystem within EIR today consists of two parts; the detection subsystem and the localisation subsystem. Recent advances in machine learning, particularly deep learning, have provided excellent object detection models. This thesis explores the possibility of using a deep neural network at the base of the detection and automatic analysis subsystem in EIR, specifically by using You only look once (YOLO). YOLO is a state-of-the-art, real-time object detection system that was used together with the ASU Mayo Clinic polyp database to detect CRC precursors called polyps. The YOLO system reaches a satisfactory detection accuracy, while still being able to process videos in real-time. The proposed system and EIR is compared using the standard metrics of recall, precision and F1-score. When compared, it is clear that the system still has room for improvement in regard to its precision.},
  language = {eng},
  school = {University of Oslo},
  author = {H{\o}iland, Torbj{\o}rn Nesb{\o}},
  year = {2017},
  file = {/home/henrik/Zotero/storage/SUFDRZJD/Høiland - 2017 - Automatic Analysis of Endoscopic Videos.pdf;/home/henrik/Zotero/storage/EKD2IYKW/56885.html}
}

@phdthesis{ComputerAidedScreening15,
  title = {Computer-{{Aided Screening}} of {{Capsule Endoscopy Videos}}},
  abstract = {Colon cancer accounts for almost 10\% of all cancer cases worldwide. It is also the fourth most common cause of death from cancer globally. However, many cases of colon cancer could be prevented by early screening and removal of colon polyps - a common precursor of colon cancer. In this respect, capsule endoscopy is a non-invasive screening method with the potential to significantly reduce the cost of screening as well as the discomfort caused for the patient using traditional endoscopy examination. The financial cost of evaluating the recorded video footage, as well as the availability of specialists, currently prevents the deployment of capsule endoscopy for mass screening. With this work, we research solutions for automating the evaluation of capsule endoscopy video sequences using machine learning, image recognition and extraction of global image features. Rather than focusing on a single approach, we build tools that can be used for conducting further experiments with different methods and algorithms. We present the prototype of an integrated software solution that can be used for collecting videos from hospitals, annotating videos, tracking objects in video sequences, build- ing training and testing datasets, training classifiers and eventually, testing and evaluating the generated classifiers. We evaluate our software by training classifiers that are based on three different image recognition approaches. We also test the generated classifiers with different datasets and thereby evaluate the different approaches for their feasibility of being used to recognize colon polyps. Our main conclusion is that state of the art image recognition methods, such as the use of Haar- features or Histogram of oriented Gradients based detectors, are not suitable for detecting lesions in the intestine because of the enormous variety of possible appearances and orientations of such lesions. Global image features such as Joint Composite Descriptor on the other hand, lead to very promising results. Performing leave-one-out-cross-validation with all 20 videos of the ASU-Mayo Clinic polyp database, our system achieves a weighted average precision of 93.9\% and a weighted average recall of 98.5\%.},
  language = {eng},
  school = {University of Oslo},
  author = {Albisser, Zeno},
  year = {2015},
  file = {/home/henrik/Zotero/storage/9W6IJWWT/Albisser - 2015 - Computer-Aided Screening of Capsule Endoscopy Vide.pdf;/home/henrik/Zotero/storage/2MKQ2Z9T/47642.html}
}

@misc{MachineLearning18,
  title = {Machine {{Learning}} Is {{Fun}}!},
  abstract = {The world's easiest introduction to Machine Learning},
  language = {en},
  journal = {Medium},
  howpublished = {https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471},
  author = {Geitgey, Adam},
  month = nov,
  year = {2018},
  file = {/home/henrik/Zotero/storage/J4ZYW6CC/machine-learning-is-fun-80ea3ec3c471.html}
}

@article{ResnetResnet16,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1603.08029},
  primaryClass = {cs, stat},
  title = {Resnet in {{Resnet}}: {{Generalizing Residual Architectures}}},
  shorttitle = {Resnet in {{Resnet}}},
  abstract = {Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.},
  journal = {arXiv:1603.08029 [cs, stat]},
  author = {Targ, Sasha and Almeida, Diogo and Lyman, Kevin},
  month = mar,
  year = {2016},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/henrik/Zotero/storage/QVYQJAZT/Targ et al. - 2016 - Resnet in Resnet Generalizing Residual Architectu.pdf;/home/henrik/Zotero/storage/YWN2LB44/1603.html}
}

@inproceedings{Inceptionv4InceptionResNet17,
  title = {Inception-v4, {{Inception}}-{{ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  language = {en},
  booktitle = {Thirty-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  month = feb,
  year = {2017},
  file = {/home/henrik/Zotero/storage/77HQJ9QB/Szegedy et al. - 2017 - Inception-v4, Inception-ResNet and the Impact of R.pdf;/home/henrik/Zotero/storage/PTK4TJCC/14806.html}
}

@inproceedings{DeepResidual16,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770-778},
  file = {/home/henrik/Zotero/storage/6VQCAMZ3/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;/home/henrik/Zotero/storage/J6SVH7QJ/He_Deep_Residual_Learning_CVPR_2016_paper.html}
}


