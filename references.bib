
@article{3DCNNLSTM19,
  title = {A {{3D}}-{{CNN}} and {{LSTM Based Multi}}-{{Task Learning Architecture}} for {{Action Recognition}}},
  author = {Ouyang, Xi and Xu, Shuangjie and Zhang, Chaoyun and Zhou, Pan and Yang, Yang and Liu, Guanghui and Li, Xuelong},
  year = {2019},
  volume = {7},
  pages = {40757--40770},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2906654},
  abstract = {Multi-task learning (MTL) is a machine learning method to share knowledge for multiple related machine learning tasks via learning those tasks jointly. It has been shown to be capable of effectively improving the generalization capability of each single task (learning just one task at a time). In this paper, we propose a novel MTL architecture that first combines 3D convolutional neural networks (3D CNN) plus the long short-term memory (LSTM) networks together with the MTL mechanism, tailored to information sharing of video inputs. We split each video into several clips and apply the hybrid deep model of 3D CNN and LSTM to extract the sequential features of those video clips. Therefore, our MTL model can share visual knowledge based on those video-clip features among different categories more efficiently. We evaluate our method on three popular public action recognition video datasets. The experimental results show that our novel MTL method can efficiently share detailed information in video clips among multiple action categories and outperforms other multi-task methods.},
  file = {/home/henriklg/Zotero/storage/3FWWG5BC/Ouyang et al. - 2019 - A 3D-CNN and LSTM Based Multi-Task Learning Archit.pdf;/home/henriklg/Zotero/storage/MF8PRAAB/8677269.html},
  journal = {IEEE Access},
  keywords = {3D CNN,3D convolutional neural networks,3D-CNN,Action recognition,Computer vision,convolutional neural nets,Deep learning,feature extraction,Feature extraction,generalization capability,hybrid deep model,information sharing,learning (artificial intelligence),long short-term memory,LSTM,LSTM based multitask learning architecture,Matrix decomposition,MTL architecture,multi-task learning,multiple action categories,multiple related machine learning tasks,Pattern recognition,public action recognition video datasets,sequential features,single task,Task analysis,Three-dimensional displays,video inputs,video signal processing,video-clip features}
}

@article{3DConvolutional13,
  title = {{{3D Convolutional Neural Networks}} for {{Human Action Recognition}}},
  author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  year = {2013},
  month = jan,
  volume = {35},
  pages = {221--231},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2012.59},
  abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
  file = {/home/henriklg/Zotero/storage/3I6IMF55/Ji et al. - 2013 - 3D Convolutional Neural Networks for Human Action .pdf;/home/henriklg/Zotero/storage/NQULXMQK/6165309.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {3D CNN model,3D convolution,3D convolutional neural networks,action recognition,airport surveillance videos,Algorithms,automated human action recognition,baseline methods,complex handcrafted features,Computational modeling,Computer architecture,convolutional neural networks,Decision Support Techniques,Deep learning,deep model,feature extraction,Feature extraction,feature representation,gesture recognition,high-level features,image classification,Image Interpretation; Computer-Assisted,image motion analysis,image representation,Imaging; Three-Dimensional,Kernel,model combination,motion information encoding,Movement,neural nets,Neural Networks (Computer),Pattern Recognition; Automated,Solid modeling,spatial dimensions,spatiotemporal phenomena,Subtraction Technique,temporal dimensions,Three dimensional displays,video surveillance,Videos},
  number = {1}
}

@article{ADADELTAAdaptive12,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  author = {Zeiler, Matthew D.},
  year = {2012},
  month = dec,
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  archivePrefix = {arXiv},
  eprint = {1212.5701},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/PTLY3CGP/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf;/home/henriklg/Zotero/storage/ZYNDRCSC/1212.html},
  journal = {arXiv:1212.5701 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{AdamMethod17,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/EM9YB3UN/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;/home/henriklg/Zotero/storage/BUI8ULZQ/1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{AdaptiveSubgradient11,
  title = {Adaptive {{Subgradient Methods}} for {{Online Learning}} and {{Stochastic Optimization}}},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  volume = {12},
  pages = {2121--2159},
  issn = {1533-7928},
  file = {/home/henriklg/Zotero/storage/V5Y7QRCR/Duchi et al. - 2011 - Adaptive Subgradient Methods for Online Learning a.pdf;/home/henriklg/Zotero/storage/ZVRFVUGU/duchi11a.html},
  journal = {Journal of Machine Learning Research},
  number = {61}
}

@inproceedings{AdaptiveWeight13,
  title = {Adaptive {{Weight Optimization}} for {{Classification}} of {{Imbalanced Data}}},
  booktitle = {Intelligence {{Science}} and {{Big Data Engineering}}},
  author = {Huang, Wenhao and Song, Guojie and Li, Man and Hu, Weisong and Xie, Kunqing},
  editor = {Sun, Changyin and Fang, Fang and Zhou, Zhi-Hua and Yang, Wankou and Liu, Zhi-Yong},
  year = {2013},
  pages = {546--553},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-42057-3_69},
  abstract = {One popular approach for imbalance learning is weighting samples in rare classes with high cost and then applying cost-sensitive learning methods to deal with imbalance in classes. Weight of a class is usually determined by proportion of samples in each class in training set. This paper analyzes that sample proportions of training set and testing set may vary in some range and it would compromise performance of learned classifier. This problem becomes serious when class distribution is extremely high imbalanced. Based on the analysis, an adaptive weighting approach aiming at finding a group of proper weights for classes is proposed. We employ evolutionary algorithm to optimize weight configuration to ensure overall performance of classifier in both training set and possible testing sets. Experimental results on a wide variety of datasets demonstrate that our approach could achieve better performances.},
  isbn = {978-3-642-42057-3},
  keywords = {Adaptive Weighting,Class Imbalance Problem,Imbalanced Data,Minority Class,Particle Swarm Optimization},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@patent{AreaBased99,
  title = {Area Based Interpolation for Image Scaling},
  author = {Wong, Ping Wah and Herley, Cormac},
  year = {1999},
  month = mar,
  assignee = {HP Inc},
  file = {/home/henriklg/Zotero/storage/SZNFRBFE/Wong and Herley - 1999 - Area based interpolation for image scaling.pdf},
  keywords = {area,degree,filter,image,polynomial},
  nationality = {US},
  number = {US5889895A}
}

@book{ArtificialIntelligence16,
  title = {Artificial {{Intelligence}} : {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2016},
  publisher = {{Malaysia; Pearson Education Limited,}},
  abstract = {Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; and everything from microelectronic devices to robotic planetary explorers. The book is also big because we go into some depth.},
  file = {/home/henriklg/Zotero/storage/G5Z53ZUJ/Russell and Norvig - 2016 - Artificial Intelligence  A Modern Approach.html},
  language = {en}
}

@inproceedings{AutoAugmentLearning19,
  title = {{{AutoAugment}}: {{Learning Augmentation Strategies From Data}}},
  shorttitle = {{{AutoAugment}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
  year = {2019},
  pages = {113--123},
  file = {/home/henriklg/Zotero/storage/UDSV4KU8/Cubuk et al. - 2019 - AutoAugment Learning Augmentation Strategies From.pdf;/home/henriklg/Zotero/storage/B4HNJFS5/Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.html}
}

@article{AutomatedPolyp16,
  title = {Automated {{Polyp Detection}} in {{Colonoscopy Videos Using Shape}} and {{Context Information}}},
  author = {Tajbakhsh, Nima and Gurudu, Suryakanth R. and Liang, Jianming},
  year = {2016},
  month = feb,
  volume = {35},
  pages = {630--644},
  issn = {1558-254X},
  doi = {10.1109/TMI.2015.2487997},
  abstract = {This paper presents the culmination of our research in designing a system for computer-aided detection (CAD) of polyps in colonoscopy videos. Our system is based on a hybrid context-shape approach, which utilizes context information to remove non-polyp structures and shape information to reliably localize polyps. Specifically, given a colonoscopy image, we first obtain a crude edge map. Second, we remove non-polyp edges from the edge map using our unique feature extraction and edge classification scheme. Third, we localize polyp candidates with probabilistic confidence scores in the refined edge maps using our novel voting scheme. The suggested CAD system has been tested using two public polyp databases, CVC-ColonDB, containing 300 colonoscopy images with a total of 300 polyp instances from 15 unique polyps, and ASU-Mayo database, which is our collection of colonoscopy videos containing 19,400 frames and a total of 5,200 polyp instances from 10 unique polyps. We have evaluated our system using free-response receiver operating characteristic (FROC) analysis. At 0.1 false positives per frame, our system achieves a sensitivity of 88.0\% for CVC-ColonDB and a sensitivity of 48\% for the ASU-Mayo database. In addition, we have evaluated our system using a new detection latency analysis where latency is defined as the time from the first appearance of a polyp in the colonoscopy video to the time of its first detection by our system. At 0.05 false positives per frame, our system yields a polyp detection latency of 0.3 seconds.},
  file = {/home/henriklg/Zotero/storage/DXKC2VUG/7294676.html},
  journal = {IEEE Transactions on Medical Imaging},
  keywords = {Algorithms,ASU-Mayo database,automated polyp detection,boundary classification,Cancer,Colonic Polyps,Colonoscopy,colonoscopy image,colonoscopy videos,computer-aided detection,context information,crude edge map,CVC-ColonDB,Dataset,detection latency,Discrete cosine transforms,edge classification scheme,edge detection,edge voting,endoscopes,feature extraction,Feature extraction,free-response receiver operating characteristic analysis,Humans,hybrid context-shape approach,image classification,Image color analysis,Image edge detection,Image Interpretation; Computer-Assisted,Machine Learning,medical image processing,nonpolyp edge removal,Optical colonoscopy,Pattern Recognition; Automated,polyp detection,probabilistic confidence scores,public polyp databases,refined edge maps,sensitivity analysis,Shape,shape information,Video Recording,video signal processing,voting scheme},
  number = {2}
}

@phdthesis{AutomaticAnalysis17,
  title = {Automatic {{Analysis}} of {{Endoscopic Videos}}},
  author = {H\o iland, Torbj\o rn Nesb\o},
  year = {2017},
  abstract = {The human digestive system can be affected by many types of diseases. For example, three of the six most common cancer types (esophagus, stomach and colorectal) are located in the gastrointestinal tract. Colorectal cancer (CRC) is the third most common cancer in men and the second most common cancer in women worldwide, and Norway has one of the highest incidences of this cancer. Early detection is vital for the prognosis, level of treatment and survival. EIR is a multimedia system with the main objective of supporting doctors in gastrointestinal tract disease detection, both as a live examination system and an offline system for VCE. However, the detection and automatic analysis subsystem within EIR today consists of two parts; the detection subsystem and the localisation subsystem. Recent advances in machine learning, particularly deep learning, have provided excellent object detection models. This thesis explores the possibility of using a deep neural network at the base of the detection and automatic analysis subsystem in EIR, specifically by using You only look once (YOLO). YOLO is a state-of-the-art, real-time object detection system that was used together with the ASU Mayo Clinic polyp database to detect CRC precursors called polyps. The YOLO system reaches a satisfactory detection accuracy, while still being able to process videos in real-time. The proposed system and EIR is compared using the standard metrics of recall, precision and F1-score. When compared, it is clear that the system still has room for improvement in regard to its precision.},
  file = {/home/henriklg/Zotero/storage/SUFDRZJD/Høiland - 2017 - Automatic Analysis of Endoscopic Videos.pdf;/home/henriklg/Zotero/storage/EKD2IYKW/56885.html},
  language = {eng},
  school = {University of Oslo}
}

@article{AutomaticPolyp12,
  title = {Towards Automatic Polyp Detection with a Polyp Appearance Model},
  author = {Bernal, J. and S{\'a}nchez, J. and Vilari{\~n}o, F.},
  year = {2012},
  month = sep,
  volume = {45},
  pages = {3166--3182},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2012.03.002},
  abstract = {This work aims at automatic polyp detection by using a model of polyp appearance in the context of the analysis of colonoscopy videos. Our method consists of three stages: region segmentation, region description and region classification. The performance of our region segmentation method guarantees that if a polyp is present in the image, it will be exclusively and totally contained in a single region. The output of the algorithm also defines which regions can be considered as non-informative. We define as our region descriptor the novel Sector Accumulation-Depth of Valleys Accumulation (SA-DOVA), which provides a necessary but not sufficient condition for the polyp presence. Finally, we classify our segmented regions according to the maximal values of the SA-DOVA descriptor. Our preliminary classification results are promising, especially when classifying those parts of the image that do not contain a polyp inside.},
  file = {/home/henriklg/Zotero/storage/7D24GAM2/Bernal et al. - 2012 - Towards automatic polyp detection with a polyp app.pdf;/home/henriklg/Zotero/storage/55572SY5/S0031320312001185.html},
  journal = {Pattern Recognition},
  keywords = {Colonoscopy,Dataset,Polyp detection,Region segmentation,SA-DOVA descriptor},
  language = {en},
  number = {9},
  series = {Best {{Papers}} of {{Iberian Conference}} on {{Pattern Recognition}} and {{Image Analysis}} ({{IbPRIA}}'2011)}
}

@article{BleedingDetection19,
  title = {Bleeding Detection in Wireless Capsule Endoscopy Videos \textemdash{} {{Color}} versus Texture Features},
  author = {Pogorelov, Konstantin and Suman, Shipra and Azmadi Hussin, Fawnizu and Saeed Malik, Aamir and Ostroukhova, Olga and Riegler, Michael and Halvorsen, P\aa l and Hooi Ho, Shiaw and Goh, Khean-Lee},
  year = {2019},
  month = aug,
  volume = {20},
  pages = {141--154},
  issn = {1526-9914},
  doi = {10.1002/acm2.12662},
  abstract = {Abstract Wireless capsule endoscopy (WCE) is an effective technology that can be used to make a gastrointestinal (GI) tract diagnosis of various lesions and abnormalities. Due to a long time required to pass through the GI tract, the resulting WCE data stream contains a large number of frames which leads to a tedious job for clinical experts to perform a visual check of each and every frame of a complete patient?s video footage. In this paper, an automated technique for bleeding detection based on color and texture features is proposed. The approach combines the color information which is an essential feature for initial detection of frame with bleeding. Additionally, it uses the texture which plays an important role to extract more information from the lesion captured in the frames and allows the system to distinguish finely between borderline cases. The detection algorithm utilizes machine-learning-based classification methods, and it can efficiently distinguish between bleeding and nonbleeding frames and perform pixel-level segmentation of bleeding areas in WCE frames. The performed experimental studies demonstrate the performance of the proposed bleeding detection method in terms of detection accuracy, where we are at least as good as the state-of-the-art approaches. In this research, we have conducted a broad comparison of a number of different state-of-the-art features and classification methods that allows building an efficient and flexible WCE video processing system.},
  file = {/home/henriklg/Zotero/storage/YABA4NNF/Pogorelov et al. - 2019 - Bleeding detection in wireless capsule endoscopy v.pdf;/home/henriklg/Zotero/storage/IRVPWGI2/acm2.html},
  journal = {Journal of Applied Clinical Medical Physics},
  keywords = {bleeding detection,color feature,machine learning,texture feature,wireless capsule endoscopy},
  number = {8}
}

@article{CADCAP2520,
  title = {{{CAD}}-{{CAP}}: A 25,000-Image Database Serving the Development of Artificial Intelligence for Capsule Endoscopy},
  shorttitle = {{{CAD}}-{{CAP}}},
  author = {Leenhardt, Romain and Li, Cynthia and Mouel, Jean-Philippe Le and Rahmi, Gabriel and Saurin, Jean Christophe and Cholet, Franck and Boureille, Arnaud and Amiot, Xavier and Delvaux, Michel and Duburque, Clotilde and Leandri, Chlo{\'e} and G{\'e}rard, Romain and Lecleire, St{\'e}phane and Mesli, Farida and {Nion-Larmurier}, Isabelle and Romain, Olivier and {Sacher-Huvelin}, Sylvie and {Simon-Shane}, Camille and Vanbiervliet, Geoffroy and Marteau, Philippe and Histace, Aymeric and Dray, Xavier},
  year = {2020},
  month = mar,
  volume = {08},
  pages = {E415-E420},
  publisher = {{\textcopyright{} Georg Thieme Verlag KG}},
  issn = {2364-3722, 2196-9736},
  doi = {10.1055/a-1035-9088},
  abstract = {Background and study aims Capsule endoscopy (CE) is the preferred method for small bowel (SB) exploration. With a mean number of 50,000 SB frames per video, SBCE reading is time-consuming and tedious (30 to 60 minutes per video). We describe a large, multicenter database named CAD-CAP (Computer-Assisted Diagnosis for CAPsule Endoscopy, CAD-CAP). This database aims to serve the development of CAD tools for CE reading.

  Materials and methods Twelve French endoscopy centers were involved. All available third-generation SB-CE videos (Pillcam, Medtronic) were retrospectively selected from these centers and deidentified. Any pathological frame was extracted and included in the database. Manual segmentation of findings within these frames was performed by two pre-med students trained and supervised by an expert reader. All frames were then classified by type and clinical relevance by a panel of three expert readers. An automated extraction process was also developed to create a dataset of normal, proofread, control images from normal, complete, SB-CE videos.

  Results Four-thousand-one-hundred-and-seventy-four SB-CE were included. Of them, 1,480 videos (35 \%) containing at least one pathological finding were selected. Findings from 5,184 frames (with their short video sequences) were extracted and delimited: 718 frames with fresh blood, 3,097 frames with vascular lesions, and 1,369 frames with inflammatory and ulcerative lesions. Twenty-thousand normal frames were extracted from 206 SB-CE normal videos. CAD-CAP has already been used for development of automated tools for angiectasia detection and also for two international challenges on medical computerized analysis.},
  copyright = {\textcopyright{} Georg Thieme Verlag KG Stuttgart {$\cdot$} New York},
  file = {/home/henriklg/Zotero/storage/GQSH4SJ8/Leenhardt et al. - 2020 - CAD-CAP a 25,000-image database serving the devel.pdf;/home/henriklg/Zotero/storage/9DCND7WL/a-1035-9088.html},
  journal = {Endoscopy International Open},
  keywords = {Dataset,Original article},
  language = {en},
  number = {3}
}

@article{CancerStatistics10,
  title = {Cancer {{Statistics}}},
  author = {Jemal, Ahmedin and Siegel, Rebecca and Xu, Jiaquan and Ward, Elizabeth},
  year = {2010},
  volume = {60},
  pages = {277--300},
  issn = {1542-4863},
  doi = {10.3322/caac.20073},
  abstract = {Each year, the American Cancer Society estimates the number of new cancer cases and deaths expected in the United States in the current year and compiles the most recent data regarding cancer incidence, mortality, and survival based on incidence data from the National Cancer Institute, the Centers for Disease Control and Prevention, and the North American Association of Central Cancer Registries and mortality data from the National Center for Health Statistics. Incidence and death rates are age-standardized to the 2000 US standard million population. A total of 1,529,560 new cancer cases and 569,490 deaths from cancer are projected to occur in the United States in 2010. Overall cancer incidence rates decreased in the most recent time period in both men (1.3\% per year from 2000 to 2006) and women (0.5\% per year from 1998 to 2006), largely due to decreases in the 3 major cancer sites in men (lung, prostate, and colon and rectum [colorectum]) and 2 major cancer sites in women (breast and colorectum). This decrease occurred in all racial/ethnic groups in both men and women with the exception of American Indian/Alaska Native women, in whom rates were stable. Among men, death rates for all races combined decreased by 21.0\% between 1990 and 2006, with decreases in lung, prostate, and colorectal cancer rates accounting for nearly 80\% of the total decrease. Among women, overall cancer death rates between 1991 and 2006 decreased by 12.3\%, with decreases in breast and colorectal cancer rates accounting for 60\% of the total decrease. The reduction in the overall cancer death rates translates to the avoidance of approximately 767,000 deaths from cancer over the 16-year period. This report also examines cancer incidence, mortality, and survival by site, sex, race/ethnicity, geographic area, and calendar year. Although progress has been made in reducing incidence and mortality rates and improving survival, cancer still accounts for more deaths than heart disease in persons younger than 85 years. Further progress can be accelerated by applying existing cancer control knowledge across all segments of the population and by supporting new discoveries in cancer prevention, early detection, and treatment. CA Cancer J Clin 2010. \textcopyright{} 2010 American Cancer Society, Inc.},
  file = {/home/henriklg/Zotero/storage/UDTTBEGD/Jemal et al. - 2010 - Cancer Statistics, 2010.pdf;/home/henriklg/Zotero/storage/D9BH79GY/caac.html},
  journal = {CA: A Cancer Journal for Clinicians},
  keywords = {cancer statistics},
  language = {en},
  number = {5}
}

@inproceedings{ClassificationBreast18,
  title = {Classification of {{Breast Cancer Risk Factors Using Several Resampling Approaches}}},
  booktitle = {2018 17th {{IEEE International Conference}} on {{Machine Learning}} and {{Applications}} ({{ICMLA}})},
  author = {Kabir, Md Faisal and Ludwig, Simone},
  year = {2018},
  month = dec,
  pages = {1243--1248},
  doi = {10.1109/ICMLA.2018.00202},
  abstract = {Breast cancer is the most common cancer in women worldwide and the second most common cancer overall. Predicting the risk of breast cancer occurrence is an important challenge for clinical oncologists as it has direct influence in daily practice and clinical service. Classification is one of the supervised learning models that is applied in medical domains. Achieving better performance on real data that contains imbalance characteristics is a very challenging task. Machine learning researchers have been using various techniques to obtain higher accuracy, generally by correctly identifying majority class samples while ignoring the instances of the minority class. However, in most of the cases the concept of the minority class instances usually is of higher interest than the majority class. In this research, we applied three different classification techniques on a real world breast cancer risk factors data set. First, we applied specified classification techniques on breast cancer data without applying any resampling technique. Second, since the data is imbalanced meaning data has an unequal distribution between the classes, we applied several resampling methods to get better performance before applying the classifiers. The experimental results show significant improvement on using a resampling method as compared to applying no resampling technique, particularly for the minority class.},
  file = {/home/henriklg/Zotero/storage/RKFUE4R2/Kabir and Ludwig - 2018 - Classification of Breast Cancer Risk Factors Using.pdf;/home/henriklg/Zotero/storage/P8KQ94JD/8614227.html},
  keywords = {breast cancer,Breast cancer,breast cancer risk factor classification,cancer,class imbalance,classification,clinical oncologists,clinical service,daily practice,Data models,Decision trees,imbalanced data,machine learning,medical computing,pattern classification,Radio frequency,resampling method,risk factors,sampling methods,supervised learning,Training data,Vegetation}
}

@inproceedings{ClassifyingDigestive15,
  title = {Classifying Digestive Organs in Wireless Capsule Endoscopy Images Based on Deep Convolutional Neural Network},
  booktitle = {Proceedings of the 2015 {{IEEE International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  author = {Zou, Y. and Li, L. and Wang, Y. and Yu, J. and Li, Y. and Deng, W. J.},
  year = {2015},
  month = jul,
  pages = {1274--1278},
  doi = {10.1109/ICDSP.2015.7252086},
  abstract = {This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95\% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.},
  file = {/home/henriklg/Zotero/storage/7N333XQA/Zou et al. - 2015 - Classifying digestive organs in wireless capsule e.pdf;/home/henriklg/Zotero/storage/X9LW93CT/7252086.html},
  keywords = {Accuracy,biological organs,biomedical optical imaging,brightness,complex digestive tract circumstance,Convolution,DCNN-WCE-CS,deep CNN-based WCE classification system,deep convolutional neural network,detecting organs,digestive organ classification problem,digestive organs classification,endoscopes,Endoscopes,feature extraction,Feature extraction,feedforward neural nets,human biological visual systems,image classification,Intestines,layer-wise hierarchy models,learning (artificial intelligence),luminance change,medical image processing,object recognition,parameter selection,semantic image feature recognition,Training,training data,wireless capsule endoscopy,wireless capsule endoscopy images,Wireless communication}
}

@article{CNNbasedSegmentation17,
  title = {{{CNN}}-Based {{Segmentation}} of {{Medical Imaging Data}}},
  author = {Kayalibay, Baris and Jensen, Grady and {van der Smagt}, Patrick},
  year = {2017},
  month = jan,
  abstract = {Convolutional neural networks have been applied to a wide variety of computer vision tasks. Recent advances in semantic segmentation have enabled their application to medical image segmentation. While most CNNs use two-dimensional kernels, recent CNN-based publications on medical image segmentation featured three-dimensional kernels, allowing full access to the three-dimensional structure of medical images. Though closely related to semantic segmentation, medical image segmentation includes specific challenges that need to be addressed, such as the scarcity of labelled data, the high class imbalance found in the ground truth and the high memory demand of three-dimensional images. In this work, a CNN-based method with three-dimensional filters is demonstrated and applied to hand and brain MRI. Two modifications to an existing CNN architecture are discussed, along with methods on addressing the aforementioned challenges. While most of the existing literature on medical image segmentation focuses on soft tissue and the major organs, this work is validated on data both from the central nervous system as well as the bones of the hand.},
  archivePrefix = {arXiv},
  eprint = {1701.03056},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/TPRA2E2Y/Kayalibay et al. - 2017 - CNN-based Segmentation of Medical Imaging Data.pdf;/home/henriklg/Zotero/storage/X8T859BV/1701.html},
  journal = {arXiv:1701.03056 [cs]},
  keywords = {Betina,Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{ColorectalCancer10,
  title = {Colorectal {{Cancer}}: {{National}} and {{International Perspective}} on the {{Burden}} of {{Disease}} and {{Public Health Impact}}},
  shorttitle = {Colorectal {{Cancer}}},
  author = {Gellad, Ziad F. and Provenzale, Dawn},
  year = {2010},
  month = may,
  volume = {138},
  pages = {2177--2190},
  issn = {00165085},
  doi = {10.1053/j.gastro.2010.01.056},
  file = {/home/henriklg/Zotero/storage/SR6Z7LRC/Gellad and Provenzale - 2010 - Colorectal Cancer National and International Pers.pdf},
  journal = {Gastroenterology},
  language = {en},
  number = {6}
}

@article{ColorectalCancer14,
  title = {Colorectal Cancer Statistics, 2014},
  author = {Siegel, Rebecca and DeSantis, Carol and Jemal, Ahmedin},
  year = {2014},
  volume = {64},
  pages = {104--117},
  issn = {1542-4863},
  doi = {10.3322/caac.21220},
  abstract = {Colorectal cancer is the third most common cancer and the third leading cause of cancer death in men and women in the United States. This article provides an overview of colorectal cancer statistics, including the most current data on incidence, survival, and mortality rates and trends. Incidence data were provided by the National Cancer Institute's Surveillance, Epidemiology, and End Results program and the North American Association of Central Cancer Registries. Mortality data were provided by the National Center for Health Statistics. In 2014, an estimated 71,830 men and 65,000 women will be diagnosed with colorectal cancer and 26,270 men and 24,040 women will die of the disease. Greater than one-third of all deaths (29\% in men and 43\% in women) will occur in individuals aged 80 years and older. There is substantial variation in tumor location by age. For example, 26\% of colorectal cancers in women aged younger than 50 years occur in the proximal colon, compared with 56\% of cases in women aged 80 years and older. Incidence and death rates are highest in blacks and lowest in Asians/Pacific Islanders; among males during 2006 through 2010, death rates in blacks (29.4 per 100,000 population) were more than double those in Asians/Pacific Islanders (13.1) and 50\% higher than those in non-Hispanic whites (19.2). Overall, incidence rates decreased by approximately 3\% per year during the past decade (2001\textendash 2010). Notably, the largest drops occurred in adults aged 65 and older. For instance, rates for tumors located in the distal colon decreased by more than 5\% per year. In contrast, rates increased during this time period among adults younger than 50 years. Colorectal cancer death rates declined by approximately 2\% per year during the 1990s and by approximately 3\% per year during the past decade. Progress in reducing colorectal cancer death rates can be accelerated by improving access to and use of screening and standard treatment in all populations. CA Cancer J Clin 2014;64:104\textendash 117. \textcopyright{} 2014 American Cancer Society.},
  copyright = {\textcopyright{} 2014 American Cancer Society, Inc.},
  file = {/home/henriklg/Zotero/storage/CLNIEME8/Siegel et al. - 2014 - Colorectal cancer statistics, 2014.pdf;/home/henriklg/Zotero/storage/AJM5WZMV/caac.html},
  journal = {CA: A Cancer Journal for Clinicians},
  keywords = {colon and rectum neoplasms,epidemiology,health disparities,screening and early detection},
  language = {en},
  number = {2}
}

@article{CompetitiveNeural13,
  title = {A {{Competitive Neural Network}} for {{Multiple Object Tracking}} in {{Video Sequence Analysis}}},
  author = {{Luque-Baena}, Rafael M. and {Ortiz-de-Lazcano-Lobato}, Juan M. and {L{\'o}pez-Rubio}, Ezequiel and Dom{\'i}nguez, Enrique and J. Palomo, Esteban},
  year = {2013},
  month = feb,
  volume = {37},
  pages = {47--67},
  issn = {1370-4621, 1573-773X},
  doi = {10.1007/s11063-012-9268-3},
  abstract = {Tracking of moving objects in real situation is a challenging research issue, due to dynamic changes in objects or background appearance, illumination, shape and occlusions. In this paper, we deal with these difficulties by incorporating an adaptive feature weighting mechanism to the proposed growing competitive neural network for multiple objects tracking. The neural network takes advantage of the most relevant object features (information provided by the proposed adaptive feature weighting mechanism) in order to estimate the trajectories of the moving objects. The feature selection mechanism is based on a genetic algorithm, and the tracking algorithm is based on a growing competitive neural network where each unit is associated to each object in the scene. The proposed methods (object tracking and feature selection mechanism) are applied to detect the trajectories of moving vehicles in roads. Experimental results show the performance of the proposed system compared to the standard Kalman filter.},
  file = {/home/henriklg/Zotero/storage/HC7K3L48/Luque-Baena et al. - 2013 - A Competitive Neural Network for Multiple Object T.pdf},
  journal = {Neural Processing Letters},
  keywords = {object tracking},
  language = {en},
  number = {1}
}

@article{ComputerAidedClassification16,
  title = {Computer-{{Aided Classification}} of {{Gastrointestinal Lesions}} in {{Regular Colonoscopy}}},
  author = {Mesejo, Pablo and Pizarro, Daniel and Abergel, Armand and Rouquette, Olivier and Beorchia, Sylvain and Poincloux, Laurent and Bartoli, Adrien},
  year = {2016},
  month = sep,
  volume = {35},
  pages = {2051--2063},
  issn = {1558-254X},
  doi = {10.1109/TMI.2016.2547947},
  abstract = {We have developed a technique to study how good computers can be at diagnosing gastrointestinal lesions from regular (white light and narrow banded) colonoscopic videos compared to two levels of clinical knowledge (expert and beginner). Our technique includes a novel tissue classification approach which may save clinician's time by avoiding chromoendoscopy, a time-consuming staining procedure using indigo carmine. Our technique also discriminates the severity of individual lesions in patients with many polyps, so that the gastroenterologist can directly focus on those requiring polypectomy. Technically, we have designed and developed a framework combining machine learning and computer vision algorithms, which performs a virtual biopsy of hyperplastic lesions, serrated adenomas and adenomas. Serrated adenomas are very difficult to classify due to their mixed/hybrid nature and recent studies indicate that they can lead to colorectal cancer through the alternate serrated pathway. Our approach is the first step to avoid systematic biopsy for suspected hyperplastic tissues. We also propose a database of colonoscopic videos showing gastrointestinal lesions with ground truth collected from both expert image inspection and histology. We not only compare our system with the expert predictions, but we also study if the use of 3D shape features improves classification accuracy, and compare our technique's performance with three competitor methods.},
  file = {/home/henriklg/Zotero/storage/AH75YMQS/Mesejo et al. - 2016 - Computer-Aided Classification of Gastrointestinal .pdf;/home/henriklg/Zotero/storage/MKPJQB99/7442848.html},
  journal = {IEEE Transactions on Medical Imaging},
  keywords = {Adenoma,Biopsy,cancer,Cancer,chromoendoscopy,Colonic Polyps,colonoscopic video,colonoscopy,Colonoscopy,colorectal cancer,Colorectal Neoplasms,computer vision algorithm,computer-aided decision support system,Dataset,endoscopes,ensemble classifiers,Feature extraction,gastrointestinal lesion computer-aided classification,histology,Humans,hyperplastic lesion virtual biopsy,image inspection,indigo carmine,learning (artificial intelligence),Lesions,machine learning,medical image processing,polypectomy,serrated adenoma,Shape,structure-from-motion,Three-dimensional displays,tissue classification,Videos,virtual biopsy},
  number = {9}
}

@phdthesis{ComputerAidedScreening15,
  title = {Computer-{{Aided Screening}} of {{Capsule Endoscopy Videos}}},
  author = {Albisser, Zeno},
  year = {2015},
  abstract = {Colon cancer accounts for almost 10\% of all cancer cases worldwide. It is also the fourth most common cause of death from cancer globally. However, many cases of colon cancer could be prevented by early screening and removal of colon polyps - a common precursor of colon cancer. In this respect, capsule endoscopy is a non-invasive screening method with the potential to significantly reduce the cost of screening as well as the discomfort caused for the patient using traditional endoscopy examination. The financial cost of evaluating the recorded video footage, as well as the availability of specialists, currently prevents the deployment of capsule endoscopy for mass screening. With this work, we research solutions for automating the evaluation of capsule endoscopy video sequences using machine learning, image recognition and extraction of global image features. Rather than focusing on a single approach, we build tools that can be used for conducting further experiments with different methods and algorithms. We present the prototype of an integrated software solution that can be used for collecting videos from hospitals, annotating videos, tracking objects in video sequences, build- ing training and testing datasets, training classifiers and eventually, testing and evaluating the generated classifiers. We evaluate our software by training classifiers that are based on three different image recognition approaches. We also test the generated classifiers with different datasets and thereby evaluate the different approaches for their feasibility of being used to recognize colon polyps. Our main conclusion is that state of the art image recognition methods, such as the use of Haar- features or Histogram of oriented Gradients based detectors, are not suitable for detecting lesions in the intestine because of the enormous variety of possible appearances and orientations of such lesions. Global image features such as Joint Composite Descriptor on the other hand, lead to very promising results. Performing leave-one-out-cross-validation with all 20 videos of the ASU-Mayo Clinic polyp database, our system achieves a weighted average precision of 93.9\% and a weighted average recall of 98.5\%.},
  file = {/home/henriklg/Zotero/storage/9W6IJWWT/Albisser - 2015 - Computer-Aided Screening of Capsule Endoscopy Vide.pdf;/home/henriklg/Zotero/storage/2MKQ2Z9T/47642.html},
  language = {eng},
  school = {University of Oslo}
}

@inproceedings{ComputingDiscipline88,
  title = {Computing as a Discipline: Preliminary Report of the {{ACM}} Task Force on the Core of Computer Science},
  shorttitle = {Computing as a Discipline},
  booktitle = {Proceedings of the Nineteenth {{SIGCSE}} Technical Symposium on {{Computer}} Science Education},
  author = {Denning, Peter and Comer, Douglas E. and Gries, David and Mulder, Michael C. and Tucker, Allen B. and Turner, A. Joe and Young, Paul R.},
  year = {1988},
  month = feb,
  pages = {41},
  publisher = {{Association for Computing Machinery}},
  address = {{Atlanta, Georgia, USA}},
  doi = {10.1145/52964.52975},
  abstract = {It is ACM's 40th year and an old debate continues. Is computer science a science? An engineering discipline? Or merely a technology, an inventor and purveyor of computing commodities? What is the intellectual substance of the discipline? Is it lasting, or will it fade within a generation? Do core curricula in computer science and engineering accurately reflect the field? How can theory and lab work be integrated in a computing curriculum? We project an image of a technology-oriented discipline whose fundamentals are in mathematics and engineering \textemdash{} for example, we represent algorithms as the most basic objects of concern and programming and hardware design as the primary activities. The view that ``computer science equals programming'' is especially strong in our curricula: the introductory course is programming, the technology is in our core courses, and the science is in our electives. This view blocks progress in reorganizing the curriculum and turns away the best students, who want a greater challenge. It denies a coherent approach to making experimental and theoretical computer science integral and harmonious parts of a curriculum. Those in the discipline know that computer science encompasses far more than programming. The emphasis on programming arises from our long-standing belief that programming languages are excellent vehicles for gaining access to the rest of the field \textemdash{} but this belief limits out ability to speak about the discipline in terms that reveal its full breadth and richness. The field has matured enough that it is now possible to describe its intellectual substance in a new and compelling way. In the spring of 1986, ACM President Adele Goldberg and ACM Education Board Chairman Robert Aiken appointed this task force with the enthusiastic cooperation of the IEEE Computer Society. At the same time, the Computer Society formed a task force on computing laboratories with the enthusiastic cooperation of the ACM. The charter of the task force has three components: We immediately extended our task to encompass computer science and computer engineering, for we came to the conclusion that in the core material there is no fundamental difference between the two fields. We use the phrase ``discipline of computing'' to embrace all of computer science and engineering. The rest of this paper is a summary of the recommendation. The description of the discipline is presented in a series of passes, starting from a short definition and culminating with a matrix as shown in the figure. The short definition: Computer science and engineering is the systematic study of algorithmic processes that describe and transform information: their theory, analysis, design, efficiency, implementation, and application. The fundamental question underlying all of computing is, ``What can be (efficiently) automated?'' The detailed description of the field fills in each of the 27 cells in the matrix with significant issues and accomplishments. (That description occupies about 16 pages of the report.) For the curriculum model, we recommend that the introductory course consist of regular lectures and a closely coordinated weekly laboratory. The lectures emphasize fundamentals; the laboratories emphasize technology and know-how. The pattern of closely coordinated lectures and labs can be repeated where appropriate in other courses. The recommended model is traditional in the physical sciences and in engineering: lectures emphasize enduring principles and concepts while laboratories emphasize the transient material and skills relating to the current technology.},
  file = {/home/henriklg/Downloads/acm-paper.pdf},
  isbn = {978-0-89791-256-3},
  keywords = {Research method},
  series = {{{SIGCSE}} '88}
}

@article{ComputingDiscipline89,
  title = {Computing as a Discipline},
  author = {Comer, D. E. and Gries, David and Mulder, Michael C. and Tucker, Allen and Turner, A. Joe and Young, Paul R. and Denning, Peter J.},
  year = {1989},
  month = jan,
  volume = {32},
  pages = {9--23},
  issn = {0001-0782},
  doi = {10.1145/63238.63239},
  abstract = {The final report of the Task Force on the Core of Computer Science presents a new intellectual framework for the discipline of computing and a new basis for computing curricula. This report has been endorsed and approved for release by the ACM Education Board.},
  file = {/home/henriklg/Downloads/63238.63239.pdf},
  journal = {Communications of the ACM},
  keywords = {Research method},
  number = {1}
}

@article{CornerNetDetecting19,
  title = {{{CornerNet}}: {{Detecting Objects}} as {{Paired Keypoints}}},
  shorttitle = {{{CornerNet}}},
  author = {Law, Hei and Deng, Jia},
  year = {2019},
  month = mar,
  abstract = {We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2\% AP on MS COCO, outperforming all existing one-stage detectors.},
  archivePrefix = {arXiv},
  eprint = {1808.01244},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/F53IPUZ6/Law and Deng - 2019 - CornerNet Detecting Objects as Paired Keypoints.pdf;/home/henriklg/Zotero/storage/8KNWZKHW/1808.html},
  journal = {arXiv:1808.01244 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{CrossDatasetValidation15,
  title = {Cross-{{Dataset Validation}} of {{Feature Sets}} in {{Musical Instrument Classification}}},
  booktitle = {2015 {{IEEE International Conference}} on {{Data Mining Workshop}} ({{ICDMW}})},
  author = {Donnelly, Patrick J. and Sheppard, John W.},
  year = {2015},
  month = nov,
  pages = {94--101},
  issn = {2375-9259},
  doi = {10.1109/ICDMW.2015.213},
  abstract = {Automatically identifying the musical instruments present in audio recordings is a complex and difficult task. Although the focus has recently shifted to identifying instruments in a polyphonic setting, the task of identifying solo instruments has not been solved. Most empirical studies recognizing musical instruments use only a single dataset in the experiments, despiteevidence that mapproaches do not generalize from one dataset to another dataset. In this work, we present a method for data driven learning of spectral filters for use in feature extraction from audio recordings of solo musical instruments and discuss the extensibility of this approach to polyphonic mixtures of instruments. We examine four datasets of musical instrument sounds that have 13 instruments in common. We demonstrate cross-dataset validation by showing that a feature extraction scheme learned from one dataset can be used successfully for feature extraction and classification on another dataset.},
  file = {/home/henriklg/Zotero/storage/T2BINKYU/Donnelly and Sheppard - 2015 - Cross-Dataset Validation of Feature Sets in Musica.pdf;/home/henriklg/Zotero/storage/P7T8DPNQ/7395658.html},
  keywords = {audio recording,audio recordings,binary relevance classification,classification,cross-dataset validation,crossdataset validation,data driven learning,feature extraction,Feature extraction,feature sets,Harmonic analysis,instrument recognition,Instruments,k-nearest neighbor,learning (artificial intelligence),machine learning,music,music information retrieval,musical instrument classification,musical instruments,musical note separation,optical filters,polyphonic setting,Source separation,spectral filters,Standards,timbre,Time-frequency analysis,Training}
}

@article{CurrentControversies19,
  title = {Current {{Controversies Concerning Capsule Endoscopy}}},
  author = {Cave, David R. and Hakimian, Shahrad and Patel, Krunal},
  year = {2019},
  month = nov,
  volume = {64},
  pages = {3040--3047},
  issn = {1573-2568},
  doi = {10.1007/s10620-019-05791-4},
  abstract = {Video capsule endoscopy became a reality in 2001. This device enabled us to directly view the mucosa of the small intestine for the first time. The main indications for the video capsule remain the detection of small intestinal bleeding and iron deficiency anemia, diagnosis and management of Crohn's disease, and detection of tumors. The device is extraordinarily safe and can be used in the very young to the very old. However, there remain several areas of controversy and difficulty. These are covered in this article and include details of indications and contraindications, whether to prepare patients, whether or not to use simethicone and prokinetics. Detection of location of the capsule remains a major engineering challenge. Reading the videos reliably and quickly remains challenging. However, artificial intelligence and machine learning are already on the horizon to provide assistance. New uses for capsule endoscopy promise more accurate diagnosis and hence improved management of acute gastrointestinal bleeding. The colon capsule may eventually help those who refuse conventional colonoscopy, and robotically controlled capsules may be helpful in screening for serious disease in patients with upper abdominal complaints. The advent of the broadening use of video capsule endoscopy is, though it will be controversial, embraced by some and derided by others; such is the nature of technological development. In the long run, if the use of the video capsule, based on sound evidence-based studies, can be shown to improve the care of our patients and reduce the cost of health care, its use will continue to expand.},
  journal = {Digestive Diseases and Sciences},
  keywords = {Controversies,Indications,Small intestine,Video capsule endoscopy},
  language = {eng},
  number = {11},
  pmid = {31468267}
}

@inproceedings{DataAugmentation18,
  title = {Data Augmentation for Improving Deep Learning in Image Classification Problem},
  booktitle = {2018 {{International Interdisciplinary PhD Workshop}} ({{IIPhDW}})},
  author = {Mikolajczyk, Agnieszka and Grochowski, Michal},
  year = {2018},
  month = may,
  pages = {117--122},
  publisher = {{IEEE}},
  address = {{Swinouj\'scie}},
  doi = {10.1109/IIPHDW.2018.8388338},
  abstract = {These days deep learning is the fastest-growing field in the field of Machine Learning (ML) and Deep Neural Networks (DNN). Among many of DNN structures, the Convolutional Neural Networks (CNN) are currently the main tool used for the image analysis and classification purposes. Although great achievements and perspectives, deep neural networks and accompanying learning algorithms have some relevant challenges to tackle. In this paper, we have focused on the most frequently mentioned problem in the field of machine learning, that is the lack of sufficient amount of the training data or uneven class balance within the datasets. One of the ways of dealing with this problem is so called data augmentation. In the paper we have compared and analyzed multiple methods of data augmentation in the task of image classification, starting from classical image transformations like rotating, cropping, zooming, histogram based methods and finishing at Style Transfer and Generative Adversarial Networks, along with the representative examples. Next, we presented our own method of data augmentation based on image style transfer. The method allows to generate the new images of high perceptual quality that combine the content of a base image with the appearance of another ones. The newly created images can be used to pre-train the given neural network in order to improve the training process efficiency. Proposed method is validated on the three medical case studies: skin melanomas diagnosis, histopathological images and breast magnetic resonance imaging (MRI) scans analysis, utilizing the image classification in order to provide a diagnose. In such kind of problems the data deficiency is one of the most relevant issues. Finally, we discuss the advantages and disadvantages of the methods being analyzed.},
  file = {/home/henriklg/Zotero/storage/L9I6LNEH/Mikolajczyk and Grochowski - 2018 - Data augmentation for improving deep learning in i.pdf},
  isbn = {978-1-5386-6143-7},
  language = {en}
}

@article{DeepConvolutional13,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2013},
  month = dec,
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archivePrefix = {arXiv},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/DSPZ2LDD/Simonyan et al. - 2013 - Deep Inside Convolutional Networks Visualising Im.pdf;/home/henriklg/Zotero/storage/628HFNB7/1312.html},
  journal = {arXiv:1312.6034 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{DeepConvolutional16,
  title = {A Deep Convolutional Neural Network for Bleeding Detection in {{Wireless Capsule Endoscopy}} Images},
  booktitle = {Proceedings of the 2016 38th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Jia, X. and Meng, M. Q.-},
  year = {2016},
  month = aug,
  pages = {639--642},
  doi = {10.1109/EMBC.2016.7590783},
  abstract = {Wireless Capsule Endoscopy (WCE) is a standard non-invasive modality for small bowel examination. Recently, the development of computer-aided diagnosis (CAD) systems for gastrointestinal (GI) bleeding detection in WCE image videos has become an active research area with the goal of relieving the workload of physicians. Existing methods based primarily on handcrafted features usually give insufficient accuracy for bleeding detection, due to their limited capability of feature representation. In this paper, we present a new automatic bleeding detection strategy based on a deep convolutional neural network and evaluate our method on an expanded dataset of 10,000 WCE images. Experimental results with an increase of around 2 percentage points in the Fi score demonstrate that our method outperforms the state-of-the-art approaches in WCE bleeding detection. The achieved Fi score is of up to 0.9955.},
  file = {/home/henriklg/Zotero/storage/AL7M2D2G/Jia and Meng - 2016 - A deep convolutional neural network for bleeding d.pdf;/home/henriklg/Zotero/storage/GK7TEZ73/7590783.html},
  keywords = {automatic bleeding detection strategy,biomedical optical imaging,CAD,Capsule Endoscopy,computer-aided diagnosis systems,deep convolutional neural network,Diagnosis; Computer-Assisted,endoscopes,Endoscopes,Feature extraction,feature representation,Fi score,gastrointestinal bleeding detection,Gastrointestinal Hemorrhage,Hemorrhaging,Humans,medical disorders,medical image processing,neural nets,Neural Networks (Computer),small bowel examination,standard noninvasive modality,Support vector machines,Training,Videos,WCE bleeding detection,WCE image videos,wireless capsule endoscopy,Wireless communication}
}

@phdthesis{DeepEIRHolistic19,
  title = {{{DeepEIR}}: {{A Holistic Medical Multimedia System}} for {{Gastrointestinal Tract Disease Detection}} and {{Localization}}},
  author = {Pogorelov, Konstantin},
  year = {2019},
  file = {/home/henriklg/Zotero/storage/3D7RN3PI/Pogorelov - DeepEIR A Holistic Medical Multimedia System for .pdf},
  language = {en}
}

@article{DeepEndoVO18,
  title = {Deep {{EndoVO}}: {{A}} Recurrent Convolutional Neural Network ({{RCNN}}) Based Visual Odometry Approach for Endoscopic Capsule Robots},
  shorttitle = {Deep {{EndoVO}}},
  author = {Turan, Mehmet and Almalioglu, Yasin and Araujo, Helder and Konukoglu, Ender and Sitti, Metin},
  year = {2018},
  month = jan,
  volume = {275},
  pages = {1861--1870},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.10.014},
  abstract = {Ingestible wireless capsule endoscopy is an emerging minimally invasive diagnostic technology for inspection of the GI tract and diagnosis of a wide range of diseases and pathologies. Medical device companies and many research groups have recently made substantial progresses in converting passive capsule endoscopes to active capsule robots, enabling more accurate, precise, and intuitive detection of the location and size of the diseased areas. Since a reliable real time pose estimation functionality is crucial for actively controlled endoscopic capsule robots, in this study, we propose a monocular visual odometry (VO) method for endoscopic capsule robot operations. Our method lies on the application of the deep recurrent convolutional neural networks (RCNNs) for the visual odometry task, where convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are used for the feature extraction and inference of dynamics across the frames, respectively. Detailed analyses and evaluations made on a real pig stomach dataset proves that our system achieves high translational and rotational accuracies for different types of endoscopic capsule robot trajectories.},
  file = {/home/henriklg/Zotero/storage/IW7GIJDZ/Turan et al. - 2018 - Deep EndoVO A recurrent convolutional neural netw.pdf;/home/henriklg/Zotero/storage/HV6QI5IW/S092523121731665X.html},
  journal = {Neurocomputing},
  keywords = {CNN,Endoscopic capsule robot,Localization,LSTM,RCNN,Sequential deep learning,Visual odometry}
}

@article{DeepLearning17,
  title = {Deep Learning for Polyp Recognition in Wireless Capsule Endoscopy Images},
  author = {Yuan, Yixuan and Meng, Max Q.-H.},
  year = {2017},
  month = apr,
  volume = {44},
  pages = {1379--1389},
  issn = {00942405},
  doi = {10.1002/mp.12147},
  abstract = {Purpose: Wireless capsule endoscopy (WCE) enables physicians to examine the digestive tract without any surgical operations, at the cost of a large volume of images to be analyzed. In the computer-aided diagnosis of WCE images, the main challenge arises from the difficulty of robust characterization of images. This study aims to provide discriminative description of WCE images and assist physicians to recognize polyp images automatically.
Methods: We propose a novel deep feature learning method, named stacked sparse autoencoder with image manifold constraint (SSAEIM), to recognize polyps in the WCE images. Our SSAEIM differs from the traditional sparse autoencoder (SAE) by introducing an image manifold constraint, which is constructed by a nearest neighbor graph and represents intrinsic structures of images. The image manifold constraint enforces that images within the same category share similar learned features and images in different categories should be kept far away. Thus, the learned features preserve large intervariances and small intravariances among images.
Results: The average overall recognition accuracy (ORA) of our method for WCE images is 98.00\%. The accuracies for polyps, bubbles, turbid images, and clear images are 98.00\%, 99.50\%, 99.00\%, and 95.50\%, respectively. Moreover, the comparison results show that our SSAEIM outperforms existing polyp recognition methods with relative higher ORA.
Conclusion: The comprehensive results have demonstrated that the proposed SSAEIM can provide descriptive characterization for WCE images and recognize polyps in a WCE video accurately. This method could be further utilized in the clinical trials to help physicians from the tedious image reading work. \textcopyright{} 2017 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.12147]},
  file = {/home/henriklg/Zotero/storage/SAXC9CNE/Yuan and Meng - 2017 - Deep learning for polyp recognition in wireless ca.pdf},
  journal = {Medical Physics},
  language = {en},
  number = {4}
}

@article{DeepLearning19,
  title = {Deep {{Learning}} for {{Fall Detection}}: {{Three}}-{{Dimensional CNN Combined With LSTM}} on {{Video Kinematic Data}}},
  shorttitle = {Deep {{Learning}} for {{Fall Detection}}},
  author = {Lu, Na and Wu, Yidan and Feng, Li and Song, Jinbo},
  year = {2019},
  month = jan,
  volume = {23},
  pages = {314--323},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2018.2808281},
  abstract = {Fall detection is an important public healthcare problem. Timely detection could enable instant delivery of medical service to the injured. A popular nonintrusive solution for fall detection is based on videos obtained through ambient camera, and the corresponding methods usually require a large dataset to train a classifier and are inclined to be influenced by the image quality. However, it is hard to collect fall data and instead simulated falls are recorded to construct the training dataset, which is restricted to limited quantity. To address these problems, a three-dimensional convolutional neural network (3-D CNN) based method for fall detection is developed, which only uses video kinematic data to train an automatic feature extractor and could circumvent the requirement for large fall dataset of deep learning solution. 2-D CNN could only encode spatial information, and the employed 3-D convolution could extract motion feature from temporal sequence, which is important for fall detection. To further locate the region of interest in each frame, a long short-term memory (LSTM) based spatial visual attention scheme is incorporated. Sports dataset Sports-1 M with no fall examples is employed to train the 3-D CNN, which is then combined with LSTM to train a classifier with fall dataset. Experiments have verified the proposed scheme on fall detection benchmark with high accuracy as 100\%. Superior performance has also been obtained on other activity databases.},
  file = {/home/henriklg/Zotero/storage/QBM7SJRA/Lu et al. - 2019 - Deep Learning for Fall Detection Three-Dimensiona.pdf;/home/henriklg/Zotero/storage/I2F5YGT5/8295206.html},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  keywords = {2D CNN,3D CNN based method,3D convolution,Activity recognition,ambient camera,automatic feature extractor,Convolution,convolutional neural network,Convolutional neural networks,Data mining,deep learning,fall data collection,fall dataset,fall detection,fall detection benchmark,feature extraction,Feature extraction,health care,image classification,image motion analysis,image quality,image representation,image sequences,learning (artificial intelligence),long short-term memory based spatial visual attention scheme,LSTM,Machine learning,medical image processing,medical service,motion feature extraction,object detection,public healthcare problem,recurrent neural nets,sport dataset,Sports-1 M,temporal sequence,three-dimensional CNN,three-dimensional convolutional neural network,Three-dimensional displays,Two dimensional displays,video cameras,video kinematic data,video signal processing,visual attention},
  number = {1}
}

@article{DeepReinforcement17,
  title = {Deep {{Reinforcement Learning}} for {{Visual Object Tracking}} in {{Videos}}},
  author = {Zhang, Da and Maei, Hamid and Wang, Xin and Wang, Yuan-Fang},
  year = {2017},
  month = jan,
  abstract = {In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than real-time. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.},
  archivePrefix = {arXiv},
  eprint = {1701.08936},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/7VJDK8ZD/Zhang et al. - 2017 - Deep Reinforcement Learning for Visual Object Trac.pdf;/home/henriklg/Zotero/storage/NGVQ2SIY/1701.html},
  journal = {arXiv:1701.08936 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{DeepResidual15,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/X2BBHVY2/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/henriklg/Zotero/storage/3HIXBC68/1512.html},
  journal = {arXiv:1512.03385 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{DeepResidual15a,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/BPBUDDYX/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/henriklg/Zotero/storage/NYBA9MPY/1512.html},
  journal = {arXiv:1512.03385 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{DeepResidual16,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  pages = {770--778},
  file = {/home/henriklg/Zotero/storage/XD98QUQR/He et al. - 2016 - Deep Residual Learning for Image Recognition.pdf;/home/henriklg/Zotero/storage/3XYHWVUQ/He_Deep_Residual_Learning_CVPR_2016_paper.html}
}

@misc{DemystifyingConvolutional18,
  title = {Demystifying {{Convolutional Neural Networks}}},
  author = {Zerium, Aegeus},
  year = {2018},
  month = sep,
  abstract = {An Intuitive Explanation of Convolutional Neural Networks.},
  file = {/home/henriklg/Zotero/storage/BFDGM3IN/demystifying-convolutional-neural-networks-ca17bdc75559.html},
  howpublished = {https://medium.com/@eternalzer0dayx/demystifying-convolutional-neural-networks-ca17bdc75559},
  journal = {Medium},
  language = {en}
}

@article{DistillingKnowledge15,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archivePrefix = {arXiv},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/Y5IYLRJ5/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;/home/henriklg/Zotero/storage/RI7Y29HZ/1503.html},
  journal = {arXiv:1503.02531 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{DonDecay18,
  title = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  year = {2018},
  month = feb,
  abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \$\textbackslash epsilon\$ and scaling the batch size \$B \textbackslash propto \textbackslash epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B \textbackslash propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1\textbackslash\%\$ validation accuracy in under 30 minutes.},
  archivePrefix = {arXiv},
  eprint = {1711.00489},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/WFAWIGFF/Smith et al. - 2018 - Don't Decay the Learning Rate, Increase the Batch .pdf;/home/henriklg/Zotero/storage/GL3VP3YU/1711.html},
  journal = {arXiv:1711.00489 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{DropoutSimple00,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  pages = {30},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ``thinned'' networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {/home/henriklg/Zotero/storage/2ZMGMXY5/Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks f.pdf},
  language = {en}
}

@article{DynamicRouting17,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  year = {2017},
  month = oct,
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archivePrefix = {arXiv},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/VAK5X6YX/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf;/home/henriklg/Zotero/storage/756F7S8K/1710.html},
  journal = {arXiv:1710.09829 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{EffectsVarying18,
  title = {Effects of {{Varying Resolution}} on {{Performance}} of {{CNN}} Based {{Image Classification An Experimental Study}}},
  author = {Kannojia, Suresh and Jaiswal, Gaurav},
  year = {2018},
  month = sep,
  volume = {6},
  pages = {451--456},
  doi = {10.26438/ijcse/v6i9.451456},
  file = {/home/henriklg/Zotero/storage/H5WHJJQW/Kannojia and Jaiswal - 2018 - Effects of Varying Resolution on Performance of CN.pdf},
  journal = {International Journal of Computer Sciences and Engineering}
}

@article{EfficientNetRethinking19,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archivePrefix = {arXiv},
  eprint = {1905.11946},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/L46UWD3M/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf;/home/henriklg/Zotero/storage/SPSEGYK4/1905.html},
  journal = {arXiv:1905.11946 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{EliminatingRacial13,
  title = {Eliminating {{Racial Disparities}} in {{Colorectal Cancer}} in the {{Real World}}: {{It Took}} a {{Village}}},
  shorttitle = {Eliminating {{Racial Disparities}} in {{Colorectal Cancer}} in the {{Real World}}},
  author = {Grubbs, Stephen S. and Polite, Blase N. and Carney, John and Bowser, William and Rogers, Jill and Katurakes, Nora and Hess, Paula and Paskett, Electra D.},
  year = {2013},
  month = apr,
  volume = {31},
  pages = {1928--1930},
  issn = {0732-183X},
  doi = {10.1200/JCO.2012.47.8412},
  file = {/home/henriklg/Zotero/storage/T9ZSKCDJ/Grubbs et al. - 2013 - Eliminating Racial Disparities in Colorectal Cance.pdf;/home/henriklg/Zotero/storage/XXI74TWR/JCO.2012.47.html},
  journal = {Journal of Clinical Oncology},
  number = {16}
}

@article{EmbeddedDetection14,
  title = {Toward Embedded Detection of Polyps in {{WCE}} Images for Early Diagnosis of Colorectal Cancer},
  author = {Silva, Juan and Histace, Aymeric and Romain, Olivier and Dray, Xavier and Granado, Bertrand},
  year = {2014},
  month = mar,
  volume = {9},
  pages = {283--293},
  issn = {1861-6429},
  doi = {10.1007/s11548-013-0926-3},
  abstract = {Wireless capsule endoscopy (WCE) is commonly used for noninvasive gastrointestinal tract evaluation, including the detection of mucosal polyps. A new embeddable method for polyp detection in wireless capsule endoscopic images was developed and tested.},
  file = {/home/henriklg/Zotero/storage/GB6JBXRT/Silva et al. - 2014 - Toward embedded detection of polyps in WCE images .pdf},
  journal = {International Journal of Computer Assisted Radiology and Surgery},
  keywords = {Dataset},
  language = {en},
  number = {2}
}

@article{EndoscopistCan07,
  title = {Endoscopist {{Can Be More Powerful}} than {{Age}} and {{Male Gender}} in {{Predicting Adenoma Detection}} at {{Colonoscopy}}},
  author = {Chen, Shawn and Rex, Douglas},
  year = {2007},
  month = apr,
  volume = {102},
  pages = {856--861},
  issn = {0002-9270},
  abstract = {BACKGROUND AND AIMSBoth advancing age and male gender are known predictors of adenomas and large adenomas at colonoscopy. However, the importance of endoscopist compared with both age and gender as predictors of adenomas is not known. In this study, we assessed the adenoma detection rates of nine en},
  file = {/home/henriklg/Zotero/storage/6ZHDU3JR/pubmed.html},
  journal = {American Journal of Gastroenterology},
  language = {ENGLISH},
  number = {4},
  pmid = {17222317}
}

@inproceedings{ExpertDriven15,
  title = {Expert Driven Semi-Supervised Elucidation Tool for Medical Endoscopic Videos},
  booktitle = {Proceedings of the 6th {{ACM Multimedia Systems Conference}}},
  author = {Albisser, Zeno and Riegler, Michael and Halvorsen, P\aa l and Zhou, Jiang and Griwodz, Carsten and Balasingham, Ilangko and Gurrin, Cathal},
  year = {2015},
  pages = {73--76},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon}},
  doi = {10.1145/2713168.2713184},
  abstract = {In this paper, we present a novel application for elucidating all kind of videos that require expert knowledge, e.g., sport videos, medical videos etc., focusing on endoscopic surgery and video capsule endoscopy. In the medical domain, the knowledge of experts for tagging and interpretation of videos is of high value. As a result of the stressful working environment of medical doctors, they often simply do not have time for extensive annotations. We therefore present a semisupervised method to gather the annotations in a very easy and time saving way for the experts and we show how this information can be used later on.},
  file = {/home/henriklg/Zotero/storage/3V2GE6II/Albisser et al. - 2015 - Expert driven semi-supervised elucidation tool for.pdf},
  isbn = {978-1-4503-3351-1}
}

@article{ExploringClinical16,
  title = {Exploring the Clinical Potential of an Automatic Colonic Polyp Detection Method Based on the Creation of Energy Maps},
  author = {{Fern{\'a}ndez-Esparrach}, Gl{\`o}ria and Bernal, Jorge and {L{\'o}pez-Cer{\'o}n}, Maria and C{\'o}rdova, Henry and {S{\'a}nchez-Montes}, Cristina and de Miguel, Cristina Rodr{\'i}guez and S{\'a}nchez, Francisco Javier},
  year = {2016},
  month = sep,
  volume = {48},
  pages = {837--842},
  publisher = {{\textcopyright{} Georg Thieme Verlag KG}},
  issn = {0013-726X, 1438-8812},
  doi = {10.1055/s-0042-108434},
  abstract = {Background and aims: Polyp miss-rate is a drawback of colonoscopy that increases significantly for small polyps. We explored the efficacy of an automatic computer-vision method for polyp detection.

  Methods: Our method relies on a model that defines polyp boundaries as valleys of image intensity. Valley information is integrated into energy maps that represent the likelihood of the presence of a polyp.

  Results: In 24 videos containing polyps from routine colonoscopies, all polyps were detected in at least one frame. The mean of the maximum values on the energy map was higher for frames with polyps than without (P  Conclusion: Energy maps performed well for colonic polyp detection, indicating their potential applicability in clinical practice.},
  copyright = {\textcopyright{} Georg Thieme Verlag KG Stuttgart {$\cdot$} New York},
  file = {/home/henriklg/Zotero/storage/FB9RTKY5/login.html},
  journal = {Endoscopy},
  keywords = {Dataset,Innovations and brief communications},
  language = {en},
  number = {9}
}

@inproceedings{FullyConvolutional15,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431--3440},
  file = {/home/henriklg/Zotero/storage/DEGCVEFG/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf;/home/henriklg/Zotero/storage/DIC8VB4W/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html}
}

@misc{GastrointestinalImage17,
  title = {Gastrointestinal {{Image ANAlysis}} ({{GIANA}}) {{Angiodysplasia D}}\textbackslash\&{{L}} Challenge},
  author = {{Bernal, J {and} Aymeric, H}},
  year = {2017},
  abstract = {Gastrointestinal Image Analysis Subchallenge, part of MICCAI 2018 EndoVis Challenge},
  file = {/home/henriklg/Zotero/storage/8GQMJAKV/home.html},
  howpublished = {https://endovissub2017-giana.grand-challenge.org/home/},
  journal = {grand-challenge.org},
  keywords = {Dataset},
  language = {en}
}

@article{GeneralInefficiency03,
  title = {The General Inefficiency of Batch Training for Gradient Descent Learning},
  author = {Wilson, D. Randall and Martinez, Tony R.},
  year = {2003},
  month = dec,
  volume = {16},
  pages = {1429--1451},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(03)00138-2},
  abstract = {Gradient descent training of neural networks can be done in either a batch or on-line manner. A widely held myth in the neural network community is that batch training is as fast or faster and/or more `correct' than on-line training because it supposedly uses a better approximation of the true gradient for its weight updates. This paper explains why batch training is almost always slower than on-line training\textemdash often orders of magnitude slower\textemdash especially on large training sets. The main reason is due to the ability of on-line training to follow curves in the error surface throughout each epoch, which allows it to safely use a larger learning rate and thus converge with less iterations through the training data. Empirical results on a large (20,000-instance) speech recognition task and on 26 other learning tasks demonstrate that convergence can be reached significantly faster using on-line training than batch training, with no apparent difference in accuracy.},
  file = {/home/henriklg/Zotero/storage/V8YABVW7/Wilson and Martinez - 2003 - The general inefficiency of batch training for gra.pdf;/home/henriklg/Zotero/storage/6JC9J539/S0893608003001382.html},
  journal = {Neural Networks},
  keywords = {Backpropagation,Batch training,Generalization,Gradient descent,Learning rate,On-line training,Optimization,Stochastic approximation},
  language = {en},
  number = {10}
}

@article{GenerativeImage19,
  title = {Generative {{Image Translation}} for {{Data Augmentation}} in {{Colorectal Histopathology Images}}},
  author = {Wei, Jerry and Suriawinata, Arief and Vaickus, Louis and Ren, Bing and Liu, Xiaoying and Wei, Jason and Hassanpour, Saeed},
  year = {2019},
  pages = {17},
  abstract = {We present an image translation approach to generate augmented data for mitigating data imbalances in a dataset of histopathology images of colorectal polyps, adenomatous tumors that can lead to colorectal cancer if left untreated. By applying cycle-consistent generative adversarial networks (CycleGANs) to a source domain of normal colonic mucosa images, we generate synthetic colorectal polyp images that belong to diagnostically less common polyp classes. Generated images maintain the general structure of their source image but exhibit adenomatous features that can be enhanced with our proposed filtration module, called Path-Rank-Filter. We evaluate the quality of generated images through Turing tests with four gastrointestinal pathologists, finding that at least two of the four pathologists could not identify generated images at a statistically significant level. Finally, we demonstrate that using CycleGAN-generated images to augment training data improves the AUC of a convolutional neural network for detecting sessile serrated adenomas by over 10\%, suggesting that our approach might warrant further research for other histopathology image classification tasks.},
  file = {/home/henriklg/Zotero/storage/7WSJKNCF/Wei et al. - Generative Image Translation for Data Augmentation.pdf},
  language = {en}
}

@article{GradientDescent19,
  title = {Gradient {{Descent}} Based {{Optimization Algorithms}} for {{Deep Learning Models Training}}},
  author = {Zhang, Jiawei},
  year = {2019},
  month = mar,
  abstract = {In this paper, we aim at providing an introduction to the gradient descent based optimization algorithms for learning deep neural network models. Deep learning models involving multiple nonlinear projection layers are very challenging to train. Nowadays, most of the deep learning model training still relies on the back propagation algorithm actually. In back propagation, the model variables will be updated iteratively until convergence with gradient descent based optimization algorithms. Besides the conventional vanilla gradient descent algorithm, many gradient descent variants have also been proposed in recent years to improve the learning performance, including Momentum, Adagrad, Adam, Gadam, etc., which will all be introduced in this paper respectively.},
  archivePrefix = {arXiv},
  eprint = {1903.03614},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/K275R8RR/Zhang - 2019 - Gradient Descent based Optimization Algorithms for.pdf;/home/henriklg/Zotero/storage/LHI2CFXN/1903.html},
  journal = {arXiv:1903.03614 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{HereditaryFamilial10,
  title = {Hereditary and {{Familial Colon Cancer}}},
  author = {Jasperson, Kory W. and Tuohy, Th{\'e}r{\`e}se M. and Neklason, Deborah W. and Burt, Randall W.},
  year = {2010},
  month = may,
  volume = {138},
  pages = {2044--2058},
  issn = {00165085},
  doi = {10.1053/j.gastro.2010.01.054},
  file = {/home/henriklg/Zotero/storage/D6FU3Y83/Jasperson et al. - 2010 - Hereditary and Familial Colon Cancer.pdf},
  journal = {Gastroenterology},
  language = {en},
  number = {6}
}

@article{HowMuch06,
  title = {How Much Can Current Interventions Reduce Colorectal Cancer Mortality in the {{U}}.{{S}}.?},
  author = {Vogelaar, Iris and van Ballegooijen, Marjolein and Schrag, Deborah and Boer, Rob and Winawer, Sidney J. and Habbema, J. Dik F. and Zauber, Ann G.},
  year = {2006},
  volume = {107},
  pages = {1624--1633},
  issn = {1097-0142},
  doi = {10.1002/cncr.22115},
  abstract = {BACKGROUND. Although colorectal cancer (CRC) is the second leading cause of cancer death in the U.S., available interventions to reduce CRC mortality are disseminated only partially throughout the population. This study assessed the potential reduction in CRC mortality that may be achieved through further dissemination of current interventions for risk-factor modification, screening, and treatment. METHODS. The MISCAN-COLON microsimulation model was used to simulate the 2000 U.S. population with respect to CRC risk-factor prevalence, screening use, and treatment use. The model was used to project age-standardized CRC mortality from 2000 to 2020 for 3 intervention scenarios. RESULTS. Without changes in risk-factor prevalence, screening use, and treatment use after 2000, CRC mortality would decrease by 17\% by the Year 2020. If the 1995 to 2000 trends continue, then the projected reduction in mortality would be 36\%. However, if trends in the prevalence of risk-factors could be improved above continued trends, if screening use increased to 70\% of the target population, and if the use of chemotherapy increased among all age groups, then a 49\% reduction would be possible. Screening drove most (23\%) of the projected mortality reduction with these optimistic trends; however, decreasing risk-factors (16\%) and increasing use of chemotherapy (10\%) also contributed substantially. The contribution of risk-factors may have been overestimated, because effect estimates could not be obtained from randomized controlled trials. CONCLUSIONS. Currently available interventions for risk-factor modification, screening, and treatment have the potential to reduce CRC mortality by almost 50\% by the Year 2020. However, without action now to further increase the uptake of current effective interventions, the reduction in CRC mortality may be only 17\%. Cancer 2006. \textcopyright{} American Cancer Society.},
  copyright = {Copyright \textcopyright{} 2006 American Cancer Society},
  file = {/home/henriklg/Zotero/storage/WEZPDFLY/Vogelaar et al. - 2006 - How much can current interventions reduce colorect.pdf;/home/henriklg/Zotero/storage/2A2AMDJI/cncr.html},
  journal = {Cancer},
  keywords = {colorectal neoplasms,computer simulation,forecasting,mortality,prevention and control},
  language = {en},
  number = {7}
}

@techreport{HyperKvasirComprehensive19,
  title = {Hyper-{{Kvasir}}: {{A Comprehensive Multi}}-{{Class Image}} and {{Video Dataset}} for {{Gastrointestinal Endoscopy}}},
  shorttitle = {Hyper-{{Kvasir}}},
  author = {Borgli, Hanna and Thambawita, Vajira and Smedsrud, Pia H. and Hicks, Steven and Jha, Debesh and Eskeland, Sigrun L. and Randel, Kristin Ranheim and Pogorelov, Konstantin and Lux, Mathias and Nguyen, Duc Tien Dang and Johansen, Dag and Griwodz, Carsten and Stensland, Haakon K. and Ceja, Enrique Garcia and Schmidt, Peter T. and Hammer, Hugo L. and Riegler, Michael and Halvorsen, Paal and {de Lange}, Thomas},
  year = {2019},
  month = dec,
  institution = {{Open Science Framework}},
  doi = {10.31219/osf.io/mkzcq},
  abstract = {Artificial intelligence is currently a hot topic in medicine. The fact that medical data is often sparse and hard to obtain due to legal restrictions and lack of medical personnel to perform the cumbersome and tedious labeling of the data leads to limitations for what would be possible to achieve with automatic analysis. In this respect, this article presents Hyper-Kvasir which is the largest image and video dataset of the gastrointestinal tract available today. The data is collected during real gastro- and colonoscopy examinations at B\ae rum Hospital in Norway and partly labeled by experienced gastrointestinal endoscopists. The dataset contains 110,079 images and 373 videos and represents anatomical landmarks and pathological and normal findings. The total number of images and video frames together is around 1,17 million. Initial experiments demonstrate the potential benefits of artificial intelligence-based computer assisted diagnosis systems. The Hyper-Kvasir dataset can play an important role in developing better algorithms and computer assisted examination systems not just for gastro- and colonoscopy, butpossible also other fields in medicine.},
  file = {/home/henriklg/Zotero/storage/4AQ3SMMA/Borgli et al. - 2019 - Hyper-Kvasir A Comprehensive Multi-Class Image an.pdf},
  type = {Preprint}
}

@phdthesis{HyperparameterOptimization18,
  title = {Hyperparameter Optimization Using {{Bayesian}} Optimization on Transfer Learning for Medical Image Classification},
  author = {Borgli, Rune Johan},
  year = {2018},
  abstract = {The field of medicine has a history of adopting new technology. Video equipment and sensors are used to visualize areas of interest in the human allowing for doctors to make diagnoses based on imagery observations. However, the detection rate of the doctors towards diseases and abnormalities is heavily dependent on the experience and state of mind of the doctor doing the examination. Computer-aided detection systems are systems designed to aid the doctor in improving the detection rate, and they are using or experimenting with machine learning. Deep convolutional neural networks, a type of machine learning, are shown to be highly efficient at image detection, classification, and analysis. However, these networks require large datasets to train properly. Transfer learning is a training technique where we use a pre-trained machine learning model and transfer some of the attained knowledge from other application domains over to a new model. This way, we can use small datasets and train a model in much shorter time. In this respect, transfer learning works fine but has many configurations called hyperparameters which are often not optimized. Our work aims to address the lack of automatic hyperparameter optimization for transfer learning by experiments utilizing a known hyperparameter optimization method and creating a system for running those experiments. First, we decided to focus on the field of gastroenterology by utilizing two publicly available datasets showing images from the gastrointestinal tract. Next, we used a specific transfer learning method and chose hyperparameters suitable for automatic optimization. The optimization method we chose was Bayesian optimization because of its reputation for being one of the best methods for hyperparameter optimization. However, Bayesian optimization has hyperparameters of its own, and there are also different versions of Bayesian optimization. We chose to limit the thesis, so we use standard Bayesian optimization with standard parameters. We created a system for running automatic experiments of three different hyperparameter optimization strategies. With the system, we ran a set of experiments for each dataset. Between the strategies, one was successful in achieving a high validation accuracy, while the others were considered failures. Compared to baselines, our best models was around 10\% better. With these experiments, we demonstrated that automatic hyperparameter optimization is an effective strategy for increasing performance in transfer learning and that the best hyperparameters are nontrivial to select manually.},
  file = {/home/henriklg/Zotero/storage/F3CTSEMU/Borgli - 2018 - Hyperparameter optimization using Bayesian optimiz.pdf;/home/henriklg/Zotero/storage/DGPRG2IR/64146.html},
  language = {eng}
}

@inproceedings{IdentificationUlcers09,
  title = {Identification of Ulcers in {{Wireless Capsule Endoscopy}} Videos},
  booktitle = {Proceedings of the 2009 {{IEEE International Symposium}} on {{Biomedical Imaging}}: {{From Nano}} to {{Macro}}},
  author = {Karargyris, A. and Bourbakis, N.},
  year = {2009},
  month = jun,
  pages = {554--557},
  doi = {10.1109/ISBI.2009.5193107},
  abstract = {Wireless Capsule Endoscopy (WCE) is a non invasive procedure which is used to view the lower gastrointestinal tract. Physicians can detect diseases such as bleeding, Crohn's disease, peptic ulcers, and colon cancer. In this paper a methodology is presented to identify peptic ulcers in the small intestine automatically. It first performs color transformation into the HSV color space; it utilizes log Gabor filters to find meaningful regions. A segmentation scheme is used to extract color information of these meaningful regions in the original RGB color space. Additionally, texture information is extracted and together with color values are fed into an artificial neural network for classification. Illustrative results from the methodology are also given in this paper.},
  file = {/home/henriklg/Zotero/storage/6VBSP4UY/Karargyris and Bourbakis - 2009 - Identification of ulcers in Wireless Capsule Endos.pdf;/home/henriklg/Zotero/storage/BW92FMMW/5193107.html},
  keywords = {artificial neural network,biological organs,bleeding,Cancer detection,Colon,colon cancer,color information extraction,Crohn's disease,Data mining,diseases,Diseases,endoscopes,Endoscopes,feature extraction,fuzzy least squares support vector machines,fuzzy region segmentation,Gabor filters,gastrointestinal tract,Gastrointestinal tract,Hemorrhaging,HSV color space,image classification,image segmentation,Intestines,log Gabor filters,log-Gabor filters,medical image processing,neural nets,noninvasive procedure,peptic ulcers,RGB color space,segmentation scheme,small intestine,texture,texture information,ulcers,video signal processing,Videos,Wireless capsule Endoscopy Imaging,wireless capsule endoscopy videos}
}

@article{IdentifyingAttacking14,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  month = jun,
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
  archivePrefix = {arXiv},
  eprint = {1406.2572},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/S5YFCDW3/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf;/home/henriklg/Zotero/storage/C5Q8JABB/1406.html},
  journal = {arXiv:1406.2572 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@incollection{ImageNetClassification12,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/henriklg/Zotero/storage/8C5IIC5A/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf;/home/henriklg/Zotero/storage/ID4BRJ6C/4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}

@article{ImprovedRegularization17,
  title = {Improved {{Regularization}} of {{Convolutional Neural Networks}} with {{Cutout}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  year = {2017},
  month = nov,
  abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
  archivePrefix = {arXiv},
  eprint = {1708.04552},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/8ML3XBAQ/DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf;/home/henriklg/Zotero/storage/GXS6A9YB/1708.html},
  journal = {arXiv:1708.04552 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{ImprovingDeep17,
  title = {Improving {{Deep Learning}} Using {{Generic Data Augmentation}}},
  author = {Taylor, Luke and Nitschke, Geoff},
  year = {2017},
  month = aug,
  abstract = {Deep artificial neural networks require a large corpus of training data in order to effectively learn, where collection of such training data is often expensive and laborious. Data augmentation overcomes this issue by artificially inflating the training set with label preserving transformations. Recently there has been extensive use of generic data augmentation to improve Convolutional Neural Network (CNN) task performance. This study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate for their data sets. Various geometric and photometric schemes are evaluated on a coarse-grained data set using a relatively simple CNN. Experimental results, run using 4-fold cross-validation and reported in terms of Top-1 and Top-5 accuracy, indicate that cropping in geometric augmentation significantly increases CNN task performance.},
  archivePrefix = {arXiv},
  eprint = {1708.06020},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/DLPCPWWD/Taylor and Nitschke - 2017 - Improving Deep Learning using Generic Data Augment.pdf;/home/henriklg/Zotero/storage/HZYC4H2C/1708.html},
  journal = {arXiv:1708.06020 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{Inceptionv4InceptionResNet17,
  title = {Inception-v4, {{Inception}}-{{ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  booktitle = {Thirty-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  year = {2017},
  month = feb,
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {/home/henriklg/Zotero/storage/77HQJ9QB/Szegedy et al. - 2017 - Inception-v4, Inception-ResNet and the Impact of R.pdf;/home/henriklg/Zotero/storage/PTK4TJCC/14806.html},
  language = {en}
}

@inproceedings{InfluenceResampling15,
  title = {Influence of Resampling on Accuracy of Imbalanced Classification},
  booktitle = {Eighth {{International Conference}} on {{Machine Vision}} ({{ICMV}} 2015)},
  author = {Burnaev, E. and Erofeev, P. and Papanov, A.},
  year = {2015},
  month = dec,
  volume = {9875},
  pages = {987521},
  publisher = {{International Society for Optics and Photonics}},
  doi = {10.1117/12.2228523},
  abstract = {In many real-world binary classification tasks (e.g. detection of certain objects from images), an available dataset is imbalanced, i.e., it has much less representatives of a one class (a \emph{minor class}), than of another. Generally, accurate prediction of the minor class is crucial but it's hard to achieve since there is not much information about the minor class. One approach to deal with this problem is to preliminarily \emph{resample} the dataset, i.e., add new elements to the dataset or remove existing ones. Resampling can be done in various ways which raises the problem of choosing the most appropriate one. In this paper we experimentally investigate impact of resampling on classification accuracy, compare resampling methods and highlight key points and difficulties of resampling.},
  file = {/home/henriklg/Zotero/storage/C58GRGEK/Burnaev et al. - 2015 - Influence of resampling on accuracy of imbalanced .pdf;/home/henriklg/Zotero/storage/UGWBQV3B/12.2228523.html},
  keywords = {Imbalanced data}
}

@article{JPEGStill92,
  title = {The {{JPEG}} Still Picture Compression Standard},
  author = {Wallace, G.K.},
  year = {1992},
  month = feb,
  volume = {38},
  pages = {xviii-xxxiv},
  issn = {1558-4127},
  doi = {10.1109/30.125072},
  abstract = {A joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG's proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT (discrete cosine transform)-based method is specified for 'lossy' compression, and a predictive method for 'lossless' compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. The author provides an overview of the JPEG standard, and focuses in detail on the Baseline method.{$<>$}},
  journal = {IEEE Transactions on Consumer Electronics},
  keywords = {Baseline method,CCITT,coding,color,continuous-tone still images,Costs,data compression,DCT,Digital images,discrete cosine transform,Displays,Facsimile,Gray-scale,grayscale,Image coding,Image storage,international compression standard,ISO,ISO standards,Joint Photographic Experts Group,JPEG,lossless compression,lossy compression,picture processing,predictive method,Standards development,still picture compression standard,television standards,Transform coding,transforms,TV standard},
  number = {1}
}

@article{KIDProject17,
  title = {{{KID Project}}: An Internet-Based Digital Video Atlas of Capsule Endoscopy for Research Purposes},
  shorttitle = {{{KID Project}}},
  author = {Koulaouzidis, Anastasios and Iakovidis, Dimitris K. and Yung, Diana E. and Rondonotti, Emanuele and Kopylov, Uri and Plevris, John N. and Toth, Ervin and Eliakim, Abraham and Johansson, Gabrielle Wurm and Marlicz, Wojciech and Mavrogenis, Georgios and Nemeth, Artur and Thorlacius, Henrik and Tontini, Gian Eugenio},
  year = {2017},
  month = jun,
  volume = {05},
  pages = {E477-E483},
  publisher = {{\textcopyright{} Georg Thieme Verlag KG}},
  issn = {2364-3722, 2196-9736},
  doi = {10.1055/s-0043-105488},
  abstract = {Background and aims Capsule endoscopy (CE) has revolutionized small-bowel (SB) investigation. Computational methods can enhance diagnostic yield (DY); however, incorporating machine learning algorithms (MLAs) into CE reading is difficult as large amounts of image annotations are required for training. Current databases lack graphic annotations of pathologies and cannot be used. A novel database, KID, aims to provide a reference for research and development of medical decision support systems (MDSS) for CE.

  Methods Open-source software was used for the KID database. Clinicians contribute anonymized, annotated CE images and videos. Graphic annotations are supported by an open-access annotation tool (Ratsnake). We detail an experiment based on the KID database, examining differences in SB lesion measurement between human readers and a MLA. The Jaccard Index (JI) was used to evaluate similarity between annotations by the MLA and human readers.

  Results The MLA performed best in measuring lymphangiectasias with a JI of 81 {$\pm$} 6 \%. The other lesion types were: angioectasias (JI 64 {$\pm$} 11 \%), aphthae (JI 64 {$\pm$} 8 \%), chylous cysts (JI 70 {$\pm$} 14 \%), polypoid lesions (JI 75 {$\pm$} 21 \%), and ulcers (JI 56 {$\pm$} 9 \%).

  Conclusion MLA can perform as well as human readers in the measurement of SB angioectasias in white light (WL). Automated lesion measurement is therefore feasible. KID is currently the only open-source CE database developed specifically to aid development of MDSS. Our experiment demonstrates this potential.},
  copyright = {\textcopyright{} Georg Thieme Verlag KG Stuttgart {$\cdot$} New York},
  file = {/home/henriklg/Zotero/storage/35WBJB3F/Koulaouzidis et al. - 2017 - KID Project an internet-based digital video atlas.pdf;/home/henriklg/Zotero/storage/5QXNPB3I/s-0043-105488.html},
  journal = {Endoscopy International Open},
  keywords = {Dataset,Original article},
  language = {en},
  number = {6}
}

@inproceedings{KVASIRMultiClass17,
  title = {{{KVASIR}}: {{A Multi}}-{{Class Image Dataset}} for {{Computer Aided Gastrointestinal Disease Detection}}},
  shorttitle = {{{KVASIR}}},
  booktitle = {Proceedings of the 8th {{ACM}} on {{Multimedia Systems Conference}} - {{MMSys}}'17},
  author = {Pogorelov, Konstantin and Schmidt, Peter Thelin and Riegler, Michael and Halvorsen, P\aa l and Randel, Kristin Ranheim and Griwodz, Carsten and Eskeland, Sigrun Losada and {de Lange}, Thomas and Johansen, Dag and Spampinato, Concetto and {Dang-Nguyen}, Duc-Tien and Lux, Mathias},
  year = {2017},
  pages = {164--169},
  publisher = {{ACM Press}},
  address = {{Taipei, Taiwan}},
  doi = {10.1145/3083187.3083212},
  abstract = {In this paper, we present Kvasir, a dataset containing images from inside the gastrointestinal (GI) tract. 
The collection of images are classified into three important anatomical landmarks and three clinically significant findings. In addition, it contains two categories of images related to endoscopic polyp removal. Sorting and annotation of the dataset is performed by medical doctors (experienced endoscopists).
In this respect, Kvasir is important for research on both single- and multi-disease computer aided detection. By providing it, we invite and enable multimedia researcher into the medical domain of detection and retrieval.},
  file = {/home/henriklg/Zotero/storage/M9K8TM7M/Pogorelov et al. - 2017 - KVASIR A Multi-Class Image Dataset for Computer A.pdf},
  isbn = {978-1-4503-5002-0},
  language = {en}
}

@article{LearningImbalanced00,
  title = {Learning from {{Imbalanced Data Sets}}: {{A Comparison}} of {{Various Strategies}}},
  author = {Japkowicz, Nathalie},
  pages = {6},
  abstract = {Althoughthe majority of concept-learning systems previously designedusually assumethat their training sets are well-balanced, this assumptionis not necessarily correct. Indeed, there exists manydomainsfor which one class is represented by a large numberof examples while the other is represented by only a few. The purpose of this paper is 1) to demonstrateexperimentally that, at least in the case of connectionistsystems, class imbalances hinder the performance of standard classifiers and 2) to comparethe performanceof several approaches previously proposed to deal with the problem.},
  file = {/home/henriklg/Zotero/storage/AU6RJECM/Japkowicz - Learning from Imbalanced Data Sets A Comparison o.pdf},
  language = {en}
}

@inproceedings{LearningRate92,
  title = {Learning Rate Schedules for Faster Stochastic Gradient Search},
  booktitle = {Neural {{Networks}} for {{Signal Processing II Proceedings}} of the 1992 {{IEEE Workshop}}},
  author = {Darken, C. and Chang, J. and Moody, J.},
  year = {1992},
  month = aug,
  pages = {3--12},
  doi = {10.1109/NNSP.1992.253713},
  abstract = {The authors propose a new methodology for creating the first automatically adapting learning rates that achieve the optimal rate of convergence for stochastic gradient descent. Empirical tests agree with theoretical expectations that drift can be used to determine whether the crucial parameter c is large enough. Using this statistic, it will be possible to produce the first adaptive learning rates which converge at optimal speed.{$<>$}},
  file = {/home/henriklg/Zotero/storage/8PZQHPRQ/253713.html},
  keywords = {automatically adapting learning rates,Backpropagation algorithms,Computer science,convergence,Convergence,Displays,drift,Fluctuations,learning (artificial intelligence),learning rate schedules,Least squares approximation,optimal rate of convergence,Processor scheduling,Random variables,search problems,statistic,statistics,Statistics,stochastic gradient descent,stochastic gradient search,Stochastic processes}
}

@inproceedings{LesionDetection15,
  title = {Lesion Detection of Endoscopy Images Based on Convolutional Neural Network Features},
  booktitle = {Proceedings of the 2015 8th {{International Congress}} on {{Image}} and {{Signal Processing}} ({{CISP}})},
  author = {Zhu, R. and Zhang, R. and Xue, D.},
  year = {2015},
  month = oct,
  pages = {372--376},
  doi = {10.1109/CISP.2015.7407907},
  abstract = {Since gastroscopy is able to observe the interior of gastrointestinal tract directly, it has been widely used for gastrointestinal examination. But it is hard for clinicians to accurately detect gastrointestinal disease due to its great dependence on doctors experiences. Therefore, a computer-aided lesion detection system can offer great help for clinicians. In this paper, we propose a new scheme for endoscopy image lesion detection. A trainable feature extractor based on convolutional neural network (CNN) is utilized to get more generic features for endoscopy images. And features are fed to support vector machine (SVM) to enhance the generalization ability. Experiments show that the proposed scheme outperforms the previous conventional methods based on color and texture features.},
  file = {/home/henriklg/Zotero/storage/9ILDKALR/Zhu et al. - 2015 - Lesion detection of endoscopy images based on conv.pdf;/home/henriklg/Zotero/storage/XMXK62TU/7407907.html},
  keywords = {cancer,CNN,color features,computer-aided lesion detection system,convolutional neural network features,detect gastrointestinal disease,endoscopes,Endoscopes,endoscopy image lesion detection,feature extraction,Feature extraction,gastrointestinal examination,Gastrointestinal tract,gastrointestinal tract directly,Histograms,Image color analysis,image colour analysis,image texture,Lesions,medical image processing,neural nets,support vector machine,support vector machines,Support vector machines,SVM,texture features,trainable feature extractor}
}

@misc{MachineLearning18,
  title = {Machine {{Learning}} Is {{Fun}}!},
  author = {Geitgey, Adam},
  year = {2018},
  month = nov,
  abstract = {The world's easiest introduction to Machine Learning},
  file = {/home/henriklg/Zotero/storage/J4ZYW6CC/machine-learning-is-fun-80ea3ec3c471.html},
  howpublished = {https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471},
  journal = {Medium},
  language = {en}
}

@phdthesis{MachineLearning19,
  title = {A {{Machine Learning Approach To Improve Consistency In User}}-{{Driven Medical Image Analysis}}},
  author = {Eriksen, Edvarda},
  year = {2019},
  abstract = {The work we present in this thesis stems out from the need of standardisation of training image analysts. We will particularly focus on the training
of image analysis using a growingly popular medical imaging technique,
T1 mapping MRI. Its usefulness relies on the ability to detect abnormalities
in the cardiac tissue myocardial structure due to a range of pathologies in a
non invasive and mostly contrast-agent free manner. T1 mapping is not yet
a routinely used clinical imaging modality, but as more evidence of its potential is published, it is foreseen by experts to soon become a fundamental
clinical tool.},
  school = {University of Oslo}
}

@phdthesis{MedicalMultimedia17,
  title = {A {{Medical Multimedia Real}}-{{Time Polyp Detection System}} Using {{Low Computational Resources}}},
  author = {Khan, Asif Qayyum},
  year = {2017},
  abstract = {In this research, the focal point has been on real-time polyp detection on computers
with low computational resources with the help of open source libraries such as LIRE
Lucene, OpenCV.},
  file = {/home/henriklg/Zotero/storage/5RYV6QNB/Khan - 2017 - A Medical Multimedia Real-Time Polyp Detection Sys.pdf},
  school = {University of Oslo},
  type = {Master's {{Thesis}}}
}

@misc{MedicalPhysics00,
  title = {Medical {{Physics}} - {{Endoscopes}}},
  file = {/home/henriklg/Zotero/storage/YTY6DUKB/index.html},
  howpublished = {http://www.genesis.net.au/\textasciitilde ajs/projects/medical\_physics/endoscopes/index.html}
}

@article{MedicoMultimedia18,
  title = {Medico Multimedia Task at {{MediaEval}} 2018},
  author = {Pogorelov, Konstantin and Riegler, Michael and Halvorsen, P\aa l and Hicks, Steven and Randel, Kristin Ranheim and Dang Nguyen, Duc Tien and Lux, Mathias and Ostroukhova, Olga and {de Lange}, Thomas},
  year = {2018},
  month = dec,
  publisher = {{CEUR Workshop Proceedings}},
  abstract = {The Medico: Multimedia for Medicine Task, running for the second time as part of MediaEval 2018, focuses on detecting abnormalities, diseases, anatomical landmarks and other findings in images captured by medical devices in the gastrointestinal tract. The task is described, including the use case and its challenges, the dataset with ground truth, the required participant runs and the evaluation metrics.},
  copyright = {Copyright 2018 The Authors.},
  file = {/home/henriklg/Zotero/storage/WIVTSDI6/Pogorelov et al. - 2018 - Medico multimedia task at MediaEval 2018.pdf;/home/henriklg/Zotero/storage/PR7D2QFY/20930.html},
  keywords = {Dataset},
  language = {eng}
}

@article{MethodUnconstrained83,
  title = {A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence o(1/K\^2)},
  author = {NESTEROV, Y.},
  year = {1983},
  volume = {269},
  pages = {543--547},
  file = {/home/henriklg/Zotero/storage/DEN3DBVL/20001173129.html},
  journal = {Doklady AN USSR}
}

@phdthesis{MimirAutomatic18,
  title = {{Mimir: An Automatic Reporting and Reasoning System for Screening of the Gastrointestinal Tract Using Deep Neural Networks}},
  shorttitle = {{Mimir}},
  author = {Hicks, Steven Alexander},
  year = {2018},
  abstract = {Data is arguably one of the most valuable resources available today. More
than ever, data is collected on such a large scale that we do not have
the capacity to process it efficiently. In healthcare alone, there is an
estimated 162 exabyte of data throughout the world, which is growing at
the speed of approximately 2.5 exabytes per year [18]. Medical data in
and of itself can be used for many things, such as patient follow-ups or
recommendations. Nevertheless, to enable the use of this information to
its fullest potential, we need sophisticated data analysis methods such as
statistics or machine learning. Machine learning is a field where machines
learn from data without explicitly being programmed. This process is
often applied through supervised learning (machines learning from labeled
data), unsupervised learning (machines learning from unlabeled data), or
semi-supervised (machines learning from a combination of labeled and
unlabeled data). Over the past few years, this field has been dominated
by a growing class of algorithms known as deep learning. Inspired by
the neurological connections in the animal brain, deep learning has made
immense strides in the production of state-of-the-art results within many
areas of data analytics [4]. Nowadays, deep learning based methods have
become a popular topic within the medical field as well [7]. This has
brought up some specific challenges which may make the application of
these methods difficult, such as the lack of data or poor understanding
of their internal workings. The latter issue, namely that deep learning is
something of a ``black box'', is one of the biggest hurdles since it hinders
the application of deep learning from being used in hospitals due to lack
of trust and understanding. For this reason, we developed a medical
reporting system, which focuses on transparency and understanding of
its internal processes. In this thesis, we present this system and show
how it may aid us in the development and understanding of deep neural
networks.},
  file = {/home/henriklg/Zotero/storage/M6ILEV7V/Hicks - 2018 - Mimir An Automatic Reporting and Reasoning System.pdf;/home/henriklg/Zotero/storage/69JSKP4S/65179.html},
  language = {nob}
}

@article{MomentumTerm99,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author = {Qian, Ning},
  year = {1999},
  month = jan,
  volume = {12},
  pages = {145--151},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00116-6},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
  file = {/home/henriklg/Zotero/storage/DB2BP9PA/Qian - 1999 - On the momentum term in gradient descent learning .pdf;/home/henriklg/Zotero/storage/VWJRZ8PD/S0893608098001166.html},
  journal = {Neural Networks},
  keywords = {Critical damping,Damped harmonic oscillator,Gradient descent learning algorithm,Learning rate,Momentum,Speed of convergence},
  language = {en},
  number = {1}
}

@inproceedings{NerthusBowel17,
  title = {Nerthus: {{A Bowel Preparation Quality Video Dataset}}},
  shorttitle = {Nerthus},
  booktitle = {Proceedings of the 8th {{ACM}} on {{Multimedia Systems Conference}}},
  author = {Pogorelov, Konstantin and Randel, Kristin Ranheim and {de Lange}, Thomas and Eskeland, Sigrun Losada and Griwodz, Carsten and Johansen, Dag and Spampinato, Concetto and Taschwer, Mario and Lux, Mathias and Schmidt, Peter Thelin and Riegler, Michael and Halvorsen, P\aa l},
  year = {2017},
  month = jun,
  pages = {170--174},
  publisher = {{Association for Computing Machinery}},
  address = {{Taipei, Taiwan}},
  doi = {10.1145/3083187.3083216},
  abstract = {Bowel preparation (cleansing) is considered to be a key precondition for successful colonoscopy (endoscopic examination of the bowel). The degree of bowel cleansing directly affects the possibility to detect diseases and may influence decisions on screening and follow-up examination intervals. An accurate assessment of bowel preparation quality is therefore important. Despite the use of reliable and validated bowel preparation scales, the grading may vary from one doctor to another. An objective and automated assessment of bowel cleansing would contribute to reduce such inequalities and optimize use of medical resources. This would also be a valuable feature for automatic endoscopy reporting in the future. In this paper, we present Nerthus, a dataset containing videos from inside the gastrointestinal (GI) tract, showing different degrees of bowel cleansing. By providing this dataset, we invite multimedia researchers to contribute in the medical field by making systems automatically evaluate the quality of bowel cleansing for colonoscopy. Such innovations would probably contribute to improve the medical field of GI endoscopy.},
  file = {/home/henriklg/Zotero/storage/WFME6HC9/Pogorelov et al. - 2017 - Nerthus A Bowel Preparation Quality Video Dataset.pdf},
  isbn = {978-1-4503-5002-0},
  keywords = {Dataset},
  series = {{{MMSys}}'17}
}

@article{NewMethod54,
  title = {A {{New Method}} of Transporting {{Optical Images}} without {{Aberrations}}},
  author = {Van Heel, A. C. S.},
  year = {1954},
  month = jan,
  volume = {173},
  pages = {39--39},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/173039a0},
  file = {/home/henriklg/Zotero/storage/R3TXS4M8/Van Heel - 1954 - A New Method of transporting Optical Images withou.pdf},
  journal = {Nature},
  language = {en},
  number = {4392}
}

@article{NotesBackpropagation16,
  title = {Notes on Backpropagation},
  author = {Sadowski, Peter},
  year = {2016},
  file = {/home/henriklg/Zotero/storage/SC592RJV/Sadowski - 2016 - Notes on backpropagation.pdf}
}

@article{ObjectTracking15,
  title = {Object {{Tracking Benchmark}}},
  author = {Wu, Y. and Lim, J. and Yang, M.},
  year = {2015},
  month = sep,
  volume = {37},
  pages = {1834--1848},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2014.2388226},
  abstract = {Object tracking has been one of the most important and active research areas in the field of computer vision. A large number of tracking algorithms have been proposed in recent years with demonstrated success. However, the set of sequences used for evaluation is often not sufficient or is sometimes biased for certain types of algorithms. Many datasets do not have common ground-truth object positions or extents, and this makes comparisons among the reported quantitative results difficult. In addition, the initial conditions or parameters of the evaluated tracking algorithms are not the same, and thus, the quantitative results reported in literature are incomparable or sometimes contradictory. To address these issues, we carry out an extensive evaluation of the state-of-the-art online object-tracking algorithms with various evaluation criteria to understand how these methods perform within the same framework. In this work, we first construct a large dataset with ground-truth object positions and extents for tracking and introduce the sequence attributes for the performance analysis. Second, we integrate most of the publicly available trackers into one code library with uniform input and output formats to facilitate large-scale performance evaluation. Third, we extensively evaluate the performance of 31 algorithms on 100 sequences with different initialization settings. By analyzing the quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.},
  file = {/home/henriklg/Zotero/storage/QI7DVTQK/Wu et al. - 2015 - Object Tracking Benchmark.pdf;/home/henriklg/Zotero/storage/SA4AYB9C/7001050.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Algorithm design and analysis,benchmark dataset,Histograms,object tracking,Object tracking,performance evaluation,Performance evaluation,Robustness,Target tracking},
  number = {9}
}

@article{OverviewTechnical15,
  title = {Overview of Technical Solutions and Assessment of Clinical Usefulness of Capsule Endoscopy},
  author = {Koprowski, Robert},
  year = {2015},
  month = dec,
  volume = {14},
  issn = {1475-925X},
  doi = {10.1186/s12938-015-0108-3},
  abstract = {The paper presents an overview of endoscopic capsules with particular emphasis on technical aspects. It indicates common problems in capsule endoscopy such as: (1) limited wireless communication (2) the use of capsule endoscopy in the case of partial patency of the gastrointestinal tract, (3) limited imaging area, (4) external capsule control limitations. It also presents the prospects of capsule endoscopy, the most recent technical solutions for biopsy and the mobility of the capsule in the gastrointestinal tract. The paper shows the possibilities of increasing clinical usefulness of capsule endoscopy resulting from technological limitations. Attention has also been paid to the current role of capsule endoscopy in screening tests and the limitations of its effectiveness. The paper includes the author's recommendations concerning the direction of further research and the possibility of enhancing the scope of capsule endoscopy.},
  file = {/home/henriklg/Zotero/storage/P9RZBLK7/Koprowski - 2015 - Overview of technical solutions and assessment of .pdf},
  journal = {BioMedical Engineering OnLine},
  pmcid = {PMC4665909},
  pmid = {26626725}
}

@article{PatientPerspectives19,
  title = {Patient {{Perspectives About Decisions}} to {{Share Medical Data}} and {{Biospecimens}} for {{Research}}},
  author = {Kim, Jihoon and Kim, Hyeoneui and Bell, Elizabeth and Bath, Tyler and Paul, Paulina and Pham, Anh and Jiang, Xiaoqian and Zheng, Kai and {Ohno-Machado}, Lucila},
  year = {2019},
  month = aug,
  volume = {2},
  pages = {e199550-e199550},
  publisher = {{American Medical Association}},
  doi = {10.1001/jamanetworkopen.2019.9550},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}Patients increasingly demand transparency in and control of how their medical records and biospecimens are shared for research. How much they are willing to share and what factors influence their sharing preferences remain understudied in real settings.{$<$}/p{$><$}h3{$>$}Objectives{$<$}/h3{$><$}p{$>$}To examine whether and how various presentations of consent forms are associated with differences in electronic health record and biospecimen sharing rates and whether these rates vary according to user interface design, data recipients, data and biospecimen items, and patient characteristics.{$<$}/p{$><$}h3{$>$}Design, Setting, and Participants{$<$}/h3{$><$}p{$>$}For this survey study, a data and biospecimen sharing preference survey was conducted at 2 academic hospitals from May 1, 2017, to September 31, 2018, after simple randomization of patients to 1 of 4 options with different layout and formats of indicating sharing preferences: opt-in simple, opt-in detailed, opt-out simple, and opt-out detailed.{$<$}/p{$><$}h3{$>$}Interventions{$<$}/h3{$><$}p{$>$}All participants were presented with a list of data and biospecimen items that could be shared for research within the same health care organization or with other nonprofit or for-profit institutions. Participating patients were randomly asked to select the items that they would share (opt-in) or were asked to select items they would not share (opt-out). Patients in these 2 groups were further randomized to select only among 18 categories vs 59 detailed items (simple vs detailed form layout).{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}The primary end points were the percentages of patients willing to share data and biospecimen categories or items.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}Among 1800 eligible participants, 1246 (69.2\%) who completed their data sharing survey were included in the analysis, and 850 of these patients (mean [SD] age, 51.1 [16.7] years; 507 [59.6\%] female; 677 [79.6\%] white) responded to the satisfaction survey. A total of 46 participants (3.7\%) declined sharing with the home institution, 352 (28.3\%) with nonprofit institutions, and 590 (47.4\%) with for-profit institutions. A total of 836 (67.1\%) indicated that they would share all items with researchers from the home institution. When comparing opt-out with opt-in interfaces, all 59 sharing choice variables (100\%) were associated with the sharing decision. When comparing simple with detailed forms, only 14 variables (23.7\%) were associated with the sharing decision.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}The findings suggest that most patients are willing to share their data and biospecimens for research. Allowing patients to decide with whom they want to share certain types of data may affect research that involves secondary use of electronic health records and/or biosamples for research.{$<$}/p{$>$}},
  file = {/home/henriklg/Zotero/storage/ST9SILWT/Kim et al. - 2019 - Patient Perspectives About Decisions to Share Medi.pdf;/home/henriklg/Zotero/storage/WQ2X24Q8/2748592.html},
  journal = {JAMA Network Open},
  language = {en},
  number = {8}
}

@misc{PillCamCamera00,
  title = {{{PillCam}} - {{A Camera Pill System}} for {{Automatic Screening}} of the {{Digestive System}} - {{Institutt}} for Informatikk.},
  abstract = {In this project, we aim to design and develop a system for analysing video from a camera pill. The pill is~swallowed and records video of the~digestive system - the goal is to be able to automatically detect cancer in the colon.},
  file = {/home/henriklg/Zotero/storage/JAQ2XTAL/SRL-media-pill-cam.html},
  howpublished = {https://www.mn.uio.no/ifi/studier/masteroppgaver/nd/SRL-media-pill-cam.html}
}

@phdthesis{PolypDetection17,
  title = {Polyp {{Detection}} Using {{Neural Networks}} - {{Data Enhancement}} and {{Training Optimization}}},
  author = {Jensen, Rune},
  year = {2017},
  abstract = {Colorectal cancer is the third most common type of cancer diagnosed for men and the second most for women. Today's main methods of examination are expensive, time consuming and intrusive for the patient. Recent technologies, such as CAD and ACD, aims to increase automation in the screening and examination processes. CAD could aid medical professionals during examinations by providing a second opinion, while ACD could be used to screen entire populations, and thus relieving pressure on the health care system. In recent years, neural networks have gained traction among researchers in topics regarding recognition, and we believe it can be utilized in these automated systems. In this thesis, we examine the performance of neural networks for polyp detection. We also explore how data enhancement affect the training and evaluation of the networks, and if it can be used to increase the polyp detection rate. Finally, we experiment with how various training techniques can be used to increase performance. We conclude that neural networks are suitable for polyp detection. We show how data enhancement and training optimization can be used to increase different aspects of the performance. We discuss what aspects are suitable for different scenarios. At the end, we also discuss how our system can be used to detect polyps per frame, per sequence and per polyp, and what the results of our system look like using the different metrics. Detection per frame can be considered a computer science viewpoint, while detection per sequence or per polyp is more of a medical field viewpoint.},
  file = {/home/henriklg/Zotero/storage/DG89EWW9/Jensen - 2017 - Polyp Detection using Neural Networks - Data Enhan.pdf},
  keywords = {Artificial neural network,Biological Neural Networks,Classification,Computer Assisted Diagnosis,Computer-aided design,Endoscopy of stomach,Greater,Machine learning,Mathematical optimization,Neoplasms,Neural Network Simulation,Object detection,Open-source software,polyps,Population,TensorFlow,Tract (literature),Traction TeamPage,Urinary tract infection,Weight}
}

@phdthesis{PolypDetection17a,
  title = {Polyp {{Detection}}: {{Effect}} of {{Early}} and {{Late Feature Fusion}}},
  shorttitle = {Polyp {{Detection}}},
  author = {Asskali, Salman},
  year = {2017},
  abstract = {In this thesis, we look at a specific component of these learning
methods and how they affect performance in aiding medical systems. This
component, called feature fusion, has two widely adopted variations: early
fusion and late fusion. We seek to compare the performance of early and
late fusion for medical diagnosis problems through image datasets, and
provide some insight to data scientists on how our results can help their
decision when building a practical system.},
  file = {/home/henriklg/Zotero/storage/TAPV4X24/Asskali - 2017 - Polyp Detection Effect of Early and Late Feature .pdf},
  school = {University of Oslo},
  type = {Master's {{Thesis}}}
}

@article{PracticalRecommendations12,
  title = {Practical Recommendations for Gradient-Based Training of Deep Architectures},
  author = {Bengio, Yoshua},
  year = {2012},
  month = sep,
  abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
  archivePrefix = {arXiv},
  eprint = {1206.5533},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/CIBDG36L/Bengio - 2012 - Practical recommendations for gradient-based train.pdf;/home/henriklg/Zotero/storage/QVCVIMB5/1206.html},
  journal = {arXiv:1206.5533 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{PricingSurgeries12,
  title = {Pricing of Surgeries for Colon Cancer},
  author = {Dor, Avi and Koroukian, Siran and Xu, Fang and Stulberg, Jonah and Delaney, Conor and Cooper, Gregory},
  year = {2012},
  volume = {118},
  pages = {5741--5748},
  issn = {1097-0142},
  doi = {10.1002/cncr.27573},
  abstract = {BACKGROUND: This study examined effects of health maintenance organization (HMO) penetration, hospital competition, and patient severity on the uptake of laparoscopic colectomy and its price relative to open surgery for colon cancer. METHODS: The MarketScan Database (data from 2002-2007) was used to identify admissions for privately insured colorectal cancer patients undergoing laparoscopic or open partial colectomy (n = 1035 and n = 6389, respectively). Patient and health plan characteristics were retrieved from these data; HMO market penetration rates and an index of hospital market concentration, the Herfindahl-Hirschman index (HHI), were derived from national databases. Logistic and logarithmic regressions were used to examine the odds of having laparoscopic colectomy, effect of covariates on colectomy prices, and the differential price of laparoscopy. RESULTS: Adoption of laparoscopy was highly sensitive to market forces, with a 10\% increase in HMO penetration leading to a 10.9\% increase in the likelihood of undergoing laparoscopic colectomy (adjusted odds ratio = 1.109; 95\% confidence interval [CI] = 1.062, 1.158) and a 10\% increase in HHI resulting in 6.6\% lower likelihood (adjusted odds ratio = 0.936; 95\% CI = 0.880, 0.996). Price models indicated that the price of laparoscopy was 7.6\% lower than that of open surgery (transformed coefficient = 0.927; 95\% CI = 0.895, 0.960). A 10\% increase in HMO penetration was associated with 1.6\% lower price (transformed coefficient = 0.985; 95\% CI = 0.977, 0.992), whereas a 10\% increase in HHI was associated with 1.6\% higher price (transformed coefficient = 1.016; 95\% CI = 1.006, 1.027; P {$<$} .001 for all comparisons). CONCLUSIONS: Laparoscopy was significantly associated with lower hospital prices. Moreover, laparoscopic surgery may result in cost savings, while market pressures contribute to its adoption. Cancer 2012. \textcopyright{} 2012 American Cancer Society.},
  file = {/home/henriklg/Zotero/storage/B5XYNSIT/Dor et al. - 2012 - Pricing of surgeries for colon cancer.pdf;/home/henriklg/Zotero/storage/IJYE4VB6/cncr.html},
  journal = {Cancer},
  keywords = {colon cancer,laparoscopy,medical prices,pricing,surgery},
  language = {en},
  number = {23}
}

@article{ProgressChallenges10,
  title = {Progress and {{Challenges}} in {{Colorectal Cancer Screening}} and {{Surveillance}}},
  author = {Lieberman, David},
  year = {2010},
  month = may,
  volume = {138},
  pages = {2115--2126},
  issn = {00165085},
  doi = {10.1053/j.gastro.2010.02.006},
  file = {/home/henriklg/Zotero/storage/5ZJHN63L/Lieberman - 2010 - Progress and Challenges in Colorectal Cancer Scree.pdf},
  journal = {Gastroenterology},
  language = {en},
  number = {6}
}

@article{RandAugmentPractical19,
  title = {{{RandAugment}}: {{Practical}} Automated Data Augmentation with a Reduced Search Space},
  shorttitle = {{{RandAugment}}},
  author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0\% accuracy, a 0.6\% increase over the previous state-of-the-art and 1.0\% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3\% improvement over baseline augmentation, and is within 0.3\% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.},
  archivePrefix = {arXiv},
  eprint = {1909.13719},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/3FKTR8I8/Cubuk et al. - 2019 - RandAugment Practical automated data augmentation.pdf;/home/henriklg/Zotero/storage/XR7CTB6E/1909.html},
  journal = {arXiv:1909.13719 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@incollection{RealData17,
  title = {Real {{Data Augmentation}} for {{Medical Image Classification}}},
  booktitle = {Intravascular {{Imaging}} and {{Computer Assisted Stenting}}, and {{Large}}-{{Scale Annotation}} of {{Biomedical Data}} and {{Expert Label Synthesis}}},
  author = {Zhang, Chuanhai and Tavanapong, Wallapak and Wong, Johnny and {de Groen}, Piet C. and Oh, JungHwan},
  editor = {Cardoso, M. Jorge and Arbel, Tal and Lee, Su-Lin and Cheplygina, Veronika and Balocco, Simone and Mateus, Diana and Zahnd, Guillaume and {Maier-Hein}, Lena and Demirci, Stefanie and Granger, Eric and Duong, Luc and Carbonneau, Marc-Andr{\'e} and Albarqouni, Shadi and Carneiro, Gustavo},
  year = {2017},
  volume = {10552},
  pages = {67--76},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-67534-3_8},
  abstract = {Many medical image classification tasks share a common unbalanced data problem. That is images of the target classes, e.g., certain types of diseases, only appear in a very small portion of the entire dataset. Nowadays, large collections of medical images are readily available. However, it is costly and may not even be feasible for medical experts to manually comb through a huge unlabeled dataset to obtain enough representative examples of the rare classes. In this paper, we propose a new method called Unified LF\&SM to recommend most similar images for each class from a large unlabeled dataset for verification by medical experts and inclusion in the seed labeled dataset. Our real data augmentation significantly reduces expensive manual labeling time. In our experiments, Unified LF\&SM performed best, selecting a high percentage of relevant images in its recommendation and achieving the best classification accuracy. It is easily extendable to other medical image classification problems.},
  file = {/home/henriklg/Zotero/storage/8RVN78RU/Zhang et al. - 2017 - Real Data Augmentation for Medical Image Classific.pdf},
  isbn = {978-3-319-67533-6 978-3-319-67534-3},
  language = {en}
}

@article{ResnetResnet16,
  title = {Resnet in {{Resnet}}: {{Generalizing Residual Architectures}}},
  shorttitle = {Resnet in {{Resnet}}},
  author = {Targ, Sasha and Almeida, Diogo and Lyman, Kevin},
  year = {2016},
  month = mar,
  abstract = {Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.},
  archivePrefix = {arXiv},
  eprint = {1603.08029},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/QVYQJAZT/Targ et al. - 2016 - Resnet in Resnet Generalizing Residual Architectu.pdf;/home/henriklg/Zotero/storage/YWN2LB44/1603.html},
  journal = {arXiv:1603.08029 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{RevisitingSmall18,
  title = {Revisiting {{Small Batch Training}} for {{Deep Neural Networks}}},
  author = {Masters, Dominic and Luschi, Carlo},
  year = {2018},
  month = apr,
  abstract = {Modern deep neural network training is typically based on mini-batch stochastic gradient optimization. While the use of large mini-batches increases the available computational parallelism, small batch training has been shown to provide improved generalization performance and allows a significantly smaller memory footprint, which might also be exploited to improve machine throughput. In this paper, we review common assumptions on learning rate scaling and training duration, as a basis for an experimental comparison of test performance for different mini-batch sizes. We adopt a learning rate that corresponds to a constant average weight update per gradient calculation (i.e., per unit cost of computation), and point out that this results in a variance of the weight updates that increases linearly with the mini-batch size \$m\$. The collected experimental results for the CIFAR-10, CIFAR-100 and ImageNet datasets show that increasing the mini-batch size progressively reduces the range of learning rates that provide stable convergence and acceptable test performance. On the other hand, small mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training. The best performance has been consistently obtained for mini-batch sizes between \$m = 2\$ and \$m = 32\$, which contrasts with recent work advocating the use of mini-batch sizes in the thousands.},
  archivePrefix = {arXiv},
  eprint = {1804.07612},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/9JXG2XII/Masters and Luschi - 2018 - Revisiting Small Batch Training for Deep Neural Ne.pdf;/home/henriklg/Zotero/storage/C6RWWXDK/1804.html},
  journal = {arXiv:1804.07612 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{SelftrainingNoisy20,
  title = {Self-Training with {{Noisy Student}} Improves {{ImageNet}} Classification},
  author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
  year = {2020},
  month = jan,
  abstract = {We present a simple self-training method that achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.},
  archivePrefix = {arXiv},
  eprint = {1911.04252},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/3T9DEQAR/Xie et al. - 2020 - Self-training with Noisy Student improves ImageNet.pdf;/home/henriklg/Zotero/storage/TNMWI2MA/1911.html},
  journal = {arXiv:1911.04252 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ShapeShading94,
  title = {Shape from Shading Using Linear Approximation},
  author = {{Ping-Sing}, Tsai and Shah, Mubarak},
  year = {1994},
  month = oct,
  volume = {12},
  pages = {487--498},
  issn = {0262-8856},
  doi = {10.1016/0262-8856(94)90002-7},
  abstract = {In this paper, we present an extremely simple algorithm for shape from shading, which can be implemented in 25 lines of C code{${_\ast}{_\ast}$}C code and some images can be obtained by anonymous ftp from under the directory /pub/shading.. The algorithm is very fast, taking 0.2 seconds on a Sun SparcStation-1 for a 128 \texttimes{} 128 image, and is purely local and highly parallelizable (parallel implementation included). In our approach, we employ a linear approximation of the reflectance function, as used by others. However, the main difference is that we first use the discrete approximations for surface normal, p and q, using finite differences, and then linearize the reflectance function in depth, Z(x, y), instead of p and q. The algorithm has been tested on several synthetic and real images of both Lambertian and specular surfaces, and good results have been obtained.},
  file = {/home/henriklg/Zotero/storage/RP6KRHXZ/0262885694900027.html},
  journal = {Image and Vision Computing},
  keywords = {3D shape,physics-based vision,shape from shading},
  number = {8}
}

@article{StackedCapsule19,
  title = {Stacked {{Capsule Autoencoders}}},
  author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
  year = {2019},
  month = jun,
  abstract = {An object can be seen as a geometrically organized set of interrelated parts. A system that makes explicit use of these geometric relationships to recognize objects should be naturally robust to changes in viewpoint, because the intrinsic geometric relationships are viewpoint-invariant. We describe an unsupervised version of capsule networks, in which a neural encoder, which looks at all of the parts, is used to infer the presence and poses of object capsules. The encoder is trained by backpropagating through a decoder, which predicts the pose of each already discovered part using a mixture of pose predictions. The parts are discovered directly from an image, in a similar manner, by using a neural encoder, which infers parts and their affine transformations. The corresponding decoder models each image pixel as a mixture of predictions made by affine-transformed parts. We learn object- and their part-capsules on unlabeled data, and then cluster the vectors of presences of object capsules. When told the names of these clusters, we achieve state-of-the-art results for unsupervised classification on SVHN (55\%) and near state-of-the-art on MNIST (98.5\%).},
  archivePrefix = {arXiv},
  eprint = {1906.06818},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/TL56H4EG/Kosiorek et al. - 2019 - Stacked Capsule Autoencoders.pdf;/home/henriklg/Zotero/storage/ANLX6ZT3/1906.html},
  journal = {arXiv:1906.06818 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{StackedHourglass16,
  title = {Stacked {{Hourglass Networks}} for {{Human Pose Estimation}}},
  author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
  year = {2016},
  month = jul,
  abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a "stacked hourglass" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
  archivePrefix = {arXiv},
  eprint = {1603.06937},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/Q8M2CKRY/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf;/home/henriklg/Zotero/storage/L27TVEVT/1603.html},
  journal = {arXiv:1603.06937 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@misc{StreetView00,
  title = {The {{Street View House Numbers}} ({{SVHN}}) {{Dataset}}},
  file = {/home/henriklg/Zotero/storage/4EP6T55T/housenumbers.html},
  howpublished = {http://ufldl.stanford.edu/housenumbers/}
}

@article{SurfaceArea14,
  title = {Surface Area of the Digestive Tract \textendash{} Revisited},
  author = {Helander, Herbert F. and F{\"a}ndriks, Lars},
  year = {2014},
  month = jun,
  volume = {49},
  pages = {681--689},
  issn = {0036-5521},
  doi = {10.3109/00365521.2014.898326},
  abstract = {Background. According to textbooks, the human gut mucosa measures 260\textendash 300 m2, that is, in the order of a tennis court. However, the quantitative data are incomplete and sometimes conflicting. Objectives. To review the literature regarding the mucosal surface area of the human digestive tract; to collect morphometric data from the parts of the gut where such data are missing; and to recalculate the mucosal surface area of the intestine in man. Methods. With focus on the intestine, we carried out morphometry by light and electron microscopy on biopsies from healthy adult volunteers or patients with endoscopically normal mucosae. Results. Literature review of intubation or radiological methods indicates an oroanal length of {$\sim$}5 m, two-third of which refers to the small intestine. However, there is a considerable variation between individuals. The inner diameter of the small intestine averages 2.5 cm and that of the large intestine averages 4.8 cm. The mucosa of the small intestine is enlarged {$\sim$}1.6 times by the plicae circulares. Morphometric data obtained by light and electron microscopy of biopsies demonstrate that villi and microvilli together amplify the small intestinal surface area by 60\textendash 120 times. Surface amplification due to microvilli in the colon is {$\sim$}6.5 times. The mean total mucosal surface of the digestive tract interior averages {$\sim$}32 m2, of which about 2 m2 refers to the large intestine. Conclusion. The total area of the human adult gut mucosa is not in the order of tennis lawn, rather is that of half a badminton court.},
  file = {/home/henriklg/Zotero/storage/855K2GWQ/Helander and Fändriks - 2014 - Surface area of the digestive tract – revisited.pdf;/home/henriklg/Zotero/storage/PAJ5TWQL/00365521.2014.html},
  journal = {Scandinavian Journal of Gastroenterology},
  keywords = {adult,diameter,digestive tract,human,length,microvilli,surface area,vertical sections,villi},
  number = {6},
  pmid = {24694282}
}

@article{SurveyTransfer10,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  month = oct,
  volume = {22},
  pages = {1345--1359},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.191},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
  file = {/home/henriklg/Zotero/storage/8RCRSP7B/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  language = {en},
  number = {10}
}

@article{Temporal3D17,
  title = {Temporal {{3D ConvNets}}: {{New Architecture}} and {{Transfer Learning}} for {{Video Classification}}},
  shorttitle = {Temporal {{3D ConvNets}}},
  author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Karami, Amir Hossein and Arzani, Mohammad Mahdi and Yousefzadeh, Rahman and Van Gool, Luc},
  year = {2017},
  month = nov,
  abstract = {The work in this paper is driven by the question how to exploit the temporal cues available in videos for their accurate classification, and for human action recognition in particular? Thus far, the vision community has focused on spatio-temporal approaches with fixed temporal convolution kernel depths. We introduce a new temporal layer that models variable temporal convolution kernel depths. We embed this new temporal layer in our proposed 3D CNN. We extend the DenseNet architecture - which normally is 2D - with 3D filters and pooling kernels. We name our proposed video convolutional network `Temporal 3D ConvNet'\textasciitilde (T3D) and its new temporal layer `Temporal Transition Layer'\textasciitilde (TTL). Our experiments show that T3D outperforms the current state-of-the-art methods on the HMDB51, UCF101 and Kinetics datasets. The other issue in training 3D ConvNets is about training them from scratch with a huge labeled dataset to get a reasonable performance. So the knowledge learned in 2D ConvNets is completely ignored. Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D CNN for a stable weight initialization. This allows us to significantly reduce the number of training samples for 3D CNNs. Thus, by finetuning this network, we beat the performance of generic and recent methods in 3D CNNs, which were trained on large video datasets, e.g. Sports-1M, and finetuned on the target datasets, e.g. HMDB51/UCF101. The T3D codes will be released},
  archivePrefix = {arXiv},
  eprint = {1711.08200},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/LDFKECWH/Diba et al. - 2017 - Temporal 3D ConvNets New Architecture and Transfe.pdf;/home/henriklg/Zotero/storage/EG8XUIVI/1711.html},
  journal = {arXiv:1711.08200 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{TensorFlowLargeScale16,
  title = {{{TensorFlow}}: {{Large}}-{{Scale Machine Learning}} on {{Heterogeneous Distributed Systems}}},
  shorttitle = {{{TensorFlow}}},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2016},
  month = mar,
  abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
  archivePrefix = {arXiv},
  eprint = {1603.04467},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/WGJZZ683/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Hetero.pdf;/home/henriklg/Zotero/storage/BUZLA3V7/1603.html},
  journal = {arXiv:1603.04467 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@misc{TfKeras19,
  title = {Tf.Keras and {{TensorFlow}}: {{Batch Normalization}} to Train Deep Neural Networks Faster},
  shorttitle = {Tf.Keras and {{TensorFlow}}},
  author = {Rawles, Chris},
  year = {2019},
  month = nov,
  abstract = {Training deep neural networks can be time consuming. In particular, training can be significantly impeded by vanishing gradients, which\ldots},
  file = {/home/henriklg/Zotero/storage/VJ36ZSMI/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faste.html},
  howpublished = {https://towardsdatascience.com/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73},
  journal = {Medium},
  language = {en}
}

@incollection{TrainingVery15,
  title = {Training {{Very Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Srivastava, Rupesh K and Greff, Klaus and Schmidhuber, J{\"u}rgen},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {2377--2385},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/henriklg/Zotero/storage/5YE6HR5H/Srivastava et al. - 2015 - Training Very Deep Networks.pdf;/home/henriklg/Zotero/storage/2C865SHW/5850-training-very-deep-networks.html}
}

@misc{UnderstandingConvolutional19,
  title = {Understanding {{Convolutional Neural Networks}} through {{Visualizations}} in {{PyTorch}}},
  author = {Kurama, Vihar},
  year = {2019},
  month = jan,
  abstract = {Getting down to the nitty-gritty of CNNs},
  file = {/home/henriklg/Zotero/storage/KJXQZXET/understanding-convolutional-neural-networks-through-visualizations-in-pytorch-b5444de08b91.html},
  howpublished = {https://towardsdatascience.com/understanding-convolutional-neural-networks-through-visualizations-in-pytorch-b5444de08b91},
  journal = {Towards Data Science}
}

@inproceedings{UNetConvolutional15,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Proceedings of the {{Medical Image Computing}} and {{Computer}}-{{Assisted Intervention}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  file = {/home/henriklg/Zotero/storage/HVJEMHFB/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf},
  isbn = {978-3-319-24574-4},
  keywords = {Betina,Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{UNetNested18,
  title = {{{UNet}}++: {{A Nested U}}-{{Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  year = {2018},
  month = jul,
  abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  archivePrefix = {arXiv},
  eprint = {1807.10165},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/E5WHVWBK/Zhou et al. - 2018 - UNet++ A Nested U-Net Architecture for Medical Im.pdf;/home/henriklg/Zotero/storage/Z59EZZFN/1807.html},
  journal = {arXiv:1807.10165 [cs, eess, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{UnsupervisedData19,
  title = {Unsupervised {{Data Augmentation}} for {{Consistency Training}}},
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  year = {2019},
  month = sep,
  abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 2.7\% with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
  archivePrefix = {arXiv},
  eprint = {1904.12848},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/DTWSJPUH/Xie et al. - 2019 - Unsupervised Data Augmentation for Consistency Tra.pdf;/home/henriklg/Zotero/storage/DWMT42HS/1904.html},
  journal = {arXiv:1904.12848 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{UnsupervisedObject19,
  title = {Unsupervised {{Object Discovery}} via {{Capsule Decoders}}},
  author = {Kosiorek, Adam Roman and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey},
  year = {2019},
  file = {/home/henriklg/Zotero/storage/8JTECSMB/Kosiorek et al. - 2019 - Unsupervised Object Discovery via Capsule Decoders.pdf}
}

@phdthesis{UnsupervisedPreprocessing19,
  title = {Unsupervised Preprocessing of Medical Imaging Data with Generative Adversarial Networks},
  author = {Kirker\o d, Mathias},
  year = {2019},
  abstract = {As an attempt to address the challenge of improving the field of computer-aided
diagnosis, our work explores ways to help existing models to increase their accuracy
when it comes to finding anomalies in medical images. In this thesis, we tackle the
problems associated with the misclassification of data based on overlays and other
artefacts in the medical image data.
We will look at how we can use machine learning to develop a system to increase
the classification accuracy of existing models, as well as going in-depth into the topic
of preprocessing to see if it has a place in modern classification models based on deep
learning.},
  file = {/home/henriklg/Zotero/storage/VACL4IVN/Kirkerød - 2019 - Unsupervised preprocessing of medical imaging data.pdf},
  school = {University of Oslo},
  type = {Master's {{Thesis}}}
}

@article{VeryDeep17,
  title = {Very {{Deep Convolutional Networks}} for {{Text Classification}}},
  author = {Conneau, Alexis and Schwenk, Holger and Barrault, Lo{\"i}c and Lecun, Yann},
  year = {2017},
  month = jan,
  abstract = {The dominant approach for many NLP tasks are recurrent neural networks, in particular LSTMs, and convolutional neural networks. However, these architectures are rather shallow in comparison to the deep convolutional networks which have pushed the state-of-the-art in computer vision. We present a new architecture (VDCNN) for text processing which operates directly at the character level and uses only small convolutions and pooling operations. We are able to show that the performance of this model increases with depth: using up to 29 convolutional layers, we report improvements over the state-of-the-art on several public text classification tasks. To the best of our knowledge, this is the first time that very deep convolutional nets have been applied to text processing.},
  archivePrefix = {arXiv},
  eprint = {1606.01781},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/Y4YJDE7N/Conneau et al. - 2017 - Very Deep Convolutional Networks for Text Classifi.pdf;/home/henriklg/Zotero/storage/IRKG88MR/1606.html},
  journal = {arXiv:1606.01781 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{VideoPixel16,
  title = {Video {{Pixel Networks}}},
  author = {Kalchbrenner, Nal and van den Oord, Aaron and Simonyan, Karen and Danihelka, Ivo and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = oct,
  pages = {16},
  abstract = {We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.},
  archivePrefix = {arXiv},
  eprint = {1610.00527},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/WNL632ME/Kalchbrenner et al. - 2016 - Video Pixel Networks.pdf;/home/henriklg/Zotero/storage/DVCSEY8H/1610.html},
  journal = {arXiv:1610.00527 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{VisualizingLoss18,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year = {2018},
  month = nov,
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archivePrefix = {arXiv},
  eprint = {1712.09913},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/JL77X86Q/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf;/home/henriklg/Zotero/storage/RDQHV9TX/1712.html},
  journal = {arXiv:1712.09913 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{WeightedMachine18,
  title = {Weighted {{Machine Learning}}},
  author = {Hashemi, Mahdi and Karimi, Hassan A.},
  year = {2018},
  month = nov,
  volume = {6},
  pages = {497--525},
  issn = {2310-5070, 2311-004X},
  doi = {10.19139/soic.v6i4.479},
  abstract = {Sometimes not all training samples are equal in supervised machine learning. This might happen in different applications because some training samples are measured by more accurate devices, training samples come from different sources with different reliabilities, there is more confidence on some training samples than others, some training samples are more relevant than others, or for any other reason the user wants to put more emphasis on some training samples. In non-weighted machine learning techniques which are designed for equally important training samples: (a) the cost of misclassification is equal for training samples in parametric classification techniques, (b) residuals are equally important in parametric regression models, and (c) when voting in non-parametric classification and regression models, training samples either have equal weights or their weights are determined internally by kernels in the feature space, thus no external weights. The weighted least squares model is an example of a weighted machine learning technique which takes the training samples' weights into account. In this work, we develop the weighted versions of Bayesian predictor, perceptron, multilayer perceptron, SVM, and decision tree and show how their results would be different from their non-weighted versions.},
  file = {/home/henriklg/Zotero/storage/6HXYJAE5/Hashemi and Karimi - 2018 - Weighted Machine Learning.pdf},
  journal = {Statistics, Optimization \& Information Computing},
  language = {en},
  number = {4}
}

@article{WhyShould16,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archivePrefix = {arXiv},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/AQEMGTQ3/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf;/home/henriklg/Zotero/storage/PWMHJ2TI/1602.html},
  journal = {arXiv:1602.04938 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{WideResidual17,
  title = {Wide {{Residual Networks}}},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2017},
  month = jun,
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
  archivePrefix = {arXiv},
  eprint = {1605.07146},
  eprinttype = {arxiv},
  file = {/home/henriklg/Zotero/storage/SGXFMSV8/Zagoruyko and Komodakis - 2017 - Wide Residual Networks.pdf;/home/henriklg/Zotero/storage/XH732W2A/1605.html},
  journal = {arXiv:1605.07146 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{WirelessCapsule00,
  title = {Wireless Capsule Endoscopy},
  author = {Iddan, Gavriel and Meron, Gavriel and Glukhovsky, Arkady and Swain, Paul},
  year = {2000},
  month = may,
  volume = {405},
  pages = {417},
  issn = {1476-4687},
  doi = {10.1038/35013140},
  abstract = {The discomfort of internal gastrointestinal examination may soon be a thing of the past.},
  copyright = {2000 Macmillan Magazines Ltd.},
  file = {/home/henriklg/Zotero/storage/U6JJ6MIK/Iddan et al. - 2000 - Wireless capsule endoscopy.pdf;/home/henriklg/Zotero/storage/HS7TCR72/35013140.html},
  journal = {Nature},
  language = {En},
  number = {6785}
}

@article{WirelessCapsule03,
  title = {Wireless Capsule Endoscopy: A Comparison with Push Enteroscopy in Patients with Gastroscopy and Colonoscopy Negative Gastrointestinal Bleeding},
  shorttitle = {Wireless Capsule Endoscopy},
  author = {Mylonaki, M and {Fritscher-Ravens}, A and Swain, P},
  year = {2003},
  month = aug,
  volume = {52},
  pages = {1122--1126},
  issn = {0017-5749},
  abstract = {Background: The development of wireless capsule endoscopy allows painless imaging of the small intestine. Its clinical use is not yet defined. The aim of this study was to compare the clinical efficacy and technical performance of capsule endoscopy and push enteroscopy in a series of 50 patients with colonoscopy and gastroscopy negative gastrointestinal bleeding., Methods: A wireless capsule endoscope was used containing a CMOS colour video imager, transmitter, and batteries. Approximately 50 000 transmitted images are received by eight abdominal aerials and stored on a portable solid state recorder, which is carried on a belt. Push enteroscopy was performed using a 240 cm Olympus video enteroscope., Results: Studies in 14 healthy volunteers gave information on normal anatomical appearances and preparation. In 50 patients with gastrointestinal bleeding and negative colonoscopy and gastroscopy, push enteroscopy was compared with capsule endoscopy. A bleeding source was discovered in the small intestine in 34 of 50 patients (68\%). These included angiodysplasia (16), focal fresh bleeding (eight), apthous ulceration suggestive of Crohn's disease (three), tumour (two), Meckel's diverticulum (two), ileal ulcer (one), jejunitis (one), and ulcer due to intussusception (one). One additional intestinal diagnosis was made by enteroscopy. The yield of push enteroscopy in evaluating obscure bleeding was 32\% (16/50). The capsule identified significantly more small intestinal bleeding sources than push enteroscopy (p{$<$}0.05). Patients preferred capsule endoscopy to push enteroscopy (p{$<$}0.001)., Conclusions: In this study capsule endoscopy was superior to push enteroscopy in the diagnosis of recurrent bleeding in patients who had a negative gastroscopy and colonoscopy. It was safe and well tolerated.},
  file = {/home/henriklg/Zotero/storage/NZAYR8DN/Mylonaki et al. - 2003 - Wireless capsule endoscopy a comparison with push.pdf},
  journal = {Gut},
  number = {8},
  pmcid = {PMC1773749},
  pmid = {12865269}
}


