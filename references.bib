
@book{ArtificialIntelligence16,
  title = {Artificial {{Intelligence}} : {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  abstract = {Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; and everything from microelectronic devices to robotic planetary explorers. The book is also big because we go into some depth.},
  language = {en},
  publisher = {{Malaysia; Pearson Education Limited,}},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2016},
  file = {/home/henrik/Zotero/storage/G5Z53ZUJ/Russell and Norvig - 2016 - Artificial Intelligence  A Modern Approach.html}
}

@misc{UnderstandingConvolutional19,
  title = {Understanding {{Convolutional Neural Networks}} through {{Visualizations}} in {{PyTorch}}},
  abstract = {Getting down to the nitty-gritty of CNNs},
  journal = {Towards Data Science},
  howpublished = {https://towardsdatascience.com/understanding-convolutional-neural-networks-through-visualizations-in-pytorch-b5444de08b91},
  author = {Kurama, Vihar},
  month = jan,
  year = {2019},
  file = {/home/henrik/Zotero/storage/KJXQZXET/understanding-convolutional-neural-networks-through-visualizations-in-pytorch-b5444de08b91.html}
}

@misc{PillCamCamera,
  title = {{{PillCam}} - {{A Camera Pill System}} for {{Automatic Screening}} of the {{Digestive System}} - {{Institutt}} for Informatikk.},
  abstract = {In this project, we aim to design and develop a system for analysing video from a camera pill. The pill is~swallowed and records video of the~digestive system - the goal is to be able to automatically detect cancer in the colon.},
  howpublished = {https://www.mn.uio.no/ifi/studier/masteroppgaver/nd/SRL-media-pill-cam.html},
  file = {/home/henrik/Zotero/storage/JAQ2XTAL/SRL-media-pill-cam.html}
}

@article{CancerStatistics10,
  title = {Cancer {{Statistics}}},
  volume = {60},
  issn = {1542-4863},
  abstract = {Each year, the American Cancer Society estimates the number of new cancer cases and deaths expected in the United States in the current year and compiles the most recent data regarding cancer incidence, mortality, and survival based on incidence data from the National Cancer Institute, the Centers for Disease Control and Prevention, and the North American Association of Central Cancer Registries and mortality data from the National Center for Health Statistics. Incidence and death rates are age-standardized to the 2000 US standard million population. A total of 1,529,560 new cancer cases and 569,490 deaths from cancer are projected to occur in the United States in 2010. Overall cancer incidence rates decreased in the most recent time period in both men (1.3\% per year from 2000 to 2006) and women (0.5\% per year from 1998 to 2006), largely due to decreases in the 3 major cancer sites in men (lung, prostate, and colon and rectum [colorectum]) and 2 major cancer sites in women (breast and colorectum). This decrease occurred in all racial/ethnic groups in both men and women with the exception of American Indian/Alaska Native women, in whom rates were stable. Among men, death rates for all races combined decreased by 21.0\% between 1990 and 2006, with decreases in lung, prostate, and colorectal cancer rates accounting for nearly 80\% of the total decrease. Among women, overall cancer death rates between 1991 and 2006 decreased by 12.3\%, with decreases in breast and colorectal cancer rates accounting for 60\% of the total decrease. The reduction in the overall cancer death rates translates to the avoidance of approximately 767,000 deaths from cancer over the 16-year period. This report also examines cancer incidence, mortality, and survival by site, sex, race/ethnicity, geographic area, and calendar year. Although progress has been made in reducing incidence and mortality rates and improving survival, cancer still accounts for more deaths than heart disease in persons younger than 85 years. Further progress can be accelerated by applying existing cancer control knowledge across all segments of the population and by supporting new discoveries in cancer prevention, early detection, and treatment. CA Cancer J Clin 2010. \textcopyright{} 2010 American Cancer Society, Inc.},
  language = {en},
  number = {5},
  journal = {CA: A Cancer Journal for Clinicians},
  doi = {10.3322/caac.20073},
  author = {Jemal, Ahmedin and Siegel, Rebecca and Xu, Jiaquan and Ward, Elizabeth},
  year = {2010},
  keywords = {cancer statistics},
  pages = {277-300},
  file = {/home/henrik/Zotero/storage/UDTTBEGD/Jemal et al. - 2010 - Cancer Statistics, 2010.pdf;/home/henrik/Zotero/storage/D9BH79GY/caac.html}
}

@article{PricingSurgeries12,
  title = {Pricing of Surgeries for Colon Cancer},
  volume = {118},
  issn = {1097-0142},
  abstract = {BACKGROUND: This study examined effects of health maintenance organization (HMO) penetration, hospital competition, and patient severity on the uptake of laparoscopic colectomy and its price relative to open surgery for colon cancer. METHODS: The MarketScan Database (data from 2002-2007) was used to identify admissions for privately insured colorectal cancer patients undergoing laparoscopic or open partial colectomy (n = 1035 and n = 6389, respectively). Patient and health plan characteristics were retrieved from these data; HMO market penetration rates and an index of hospital market concentration, the Herfindahl-Hirschman index (HHI), were derived from national databases. Logistic and logarithmic regressions were used to examine the odds of having laparoscopic colectomy, effect of covariates on colectomy prices, and the differential price of laparoscopy. RESULTS: Adoption of laparoscopy was highly sensitive to market forces, with a 10\% increase in HMO penetration leading to a 10.9\% increase in the likelihood of undergoing laparoscopic colectomy (adjusted odds ratio = 1.109; 95\% confidence interval [CI] = 1.062, 1.158) and a 10\% increase in HHI resulting in 6.6\% lower likelihood (adjusted odds ratio = 0.936; 95\% CI = 0.880, 0.996). Price models indicated that the price of laparoscopy was 7.6\% lower than that of open surgery (transformed coefficient = 0.927; 95\% CI = 0.895, 0.960). A 10\% increase in HMO penetration was associated with 1.6\% lower price (transformed coefficient = 0.985; 95\% CI = 0.977, 0.992), whereas a 10\% increase in HHI was associated with 1.6\% higher price (transformed coefficient = 1.016; 95\% CI = 1.006, 1.027; P {$<$} .001 for all comparisons). CONCLUSIONS: Laparoscopy was significantly associated with lower hospital prices. Moreover, laparoscopic surgery may result in cost savings, while market pressures contribute to its adoption. Cancer 2012. \textcopyright{} 2012 American Cancer Society.},
  language = {en},
  number = {23},
  journal = {Cancer},
  doi = {10.1002/cncr.27573},
  author = {Dor, Avi and Koroukian, Siran and Xu, Fang and Stulberg, Jonah and Delaney, Conor and Cooper, Gregory},
  year = {2012},
  keywords = {colon cancer,laparoscopy,medical prices,surgery,pricing},
  pages = {5741-5748},
  file = {/home/henrik/Zotero/storage/B5XYNSIT/Dor et al. - 2012 - Pricing of surgeries for colon cancer.pdf;/home/henrik/Zotero/storage/IJYE4VB6/cncr.html}
}

@article{HereditaryFamilial10,
  title = {Hereditary and {{Familial Colon Cancer}}},
  volume = {138},
  issn = {00165085},
  language = {en},
  number = {6},
  journal = {Gastroenterology},
  doi = {10.1053/j.gastro.2010.01.054},
  author = {Jasperson, Kory W. and Tuohy, Th{\'e}r{\`e}se M. and Neklason, Deborah W. and Burt, Randall W.},
  month = may,
  year = {2010},
  pages = {2044-2058},
  file = {/home/henrik/Zotero/storage/D6FU3Y83/Jasperson et al. - 2010 - Hereditary and Familial Colon Cancer.pdf}
}

@article{ColorectalCancer10,
  title = {Colorectal {{Cancer}}: {{National}} and {{International Perspective}} on the {{Burden}} of {{Disease}} and {{Public Health Impact}}},
  volume = {138},
  issn = {00165085},
  shorttitle = {Colorectal {{Cancer}}},
  language = {en},
  number = {6},
  journal = {Gastroenterology},
  doi = {10.1053/j.gastro.2010.01.056},
  author = {Gellad, Ziad F. and Provenzale, Dawn},
  month = may,
  year = {2010},
  pages = {2177-2190},
  file = {/home/henrik/Zotero/storage/SR6Z7LRC/Gellad and Provenzale - 2010 - Colorectal Cancer National and International Pers.pdf}
}

@article{ProgressChallenges10,
  title = {Progress and {{Challenges}} in {{Colorectal Cancer Screening}} and {{Surveillance}}},
  volume = {138},
  issn = {00165085},
  language = {en},
  number = {6},
  journal = {Gastroenterology},
  doi = {10.1053/j.gastro.2010.02.006},
  author = {Lieberman, David},
  month = may,
  year = {2010},
  pages = {2115-2126},
  file = {/home/henrik/Zotero/storage/5ZJHN63L/Lieberman - 2010 - Progress and Challenges in Colorectal Cancer Scree.pdf}
}

@misc{ComputerAidedScreening15,
  title = {Computer-{{Aided Screening}} of {{Capsule Endoscopy Videos}}},
  abstract = {Colon cancer accounts for almost 10\% of all cancer cases worldwide. It is also the fourth most common cause of death from cancer globally. However, many cases of colon cancer could be prevented by early screening and removal of colon polyps - a common precursor of colon cancer. In this respect, capsule endoscopy is a non-invasive screening method with the potential to significantly reduce the cost of screening as well as the discomfort caused for the patient using traditional endoscopy examination. The financial cost of evaluating the recorded video footage, as well as the availability of specialists, currently prevents the deployment of capsule endoscopy for mass screening. With this work, we research solutions for automating the evaluation of capsule endoscopy video sequences using machine learning, image recognition and extraction of global image features. Rather than focusing on a single approach, we build tools that can be used for conducting further experiments with different methods and algorithms. We present the prototype of an integrated software solution that can be used for collecting videos from hospitals, annotating videos, tracking objects in video sequences, build- ing training and testing datasets, training classifiers and eventually, testing and evaluating the generated classifiers. We evaluate our software by training classifiers that are based on three different image recognition approaches. We also test the generated classifiers with different datasets and thereby evaluate the different approaches for their feasibility of being used to recognize colon polyps. Our main conclusion is that state of the art image recognition methods, such as the use of Haar- features or Histogram of oriented Gradients based detectors, are not suitable for detecting lesions in the intestine because of the enormous variety of possible appearances and orientations of such lesions. Global image features such as Joint Composite Descriptor on the other hand, lead to very promising results. Performing leave-one-out-cross-validation with all 20 videos of the ASU-Mayo Clinic polyp database, our system achieves a weighted average precision of 93.9\% and a weighted average recall of 98.5\%.},
  language = {eng},
  author = {Albisser, Zeno},
  year = {2015},
  keywords = {cancer,capsule,colon,colonoscopy,endoscopy,polyp}
}

@article{CompetitiveNeural13,
  title = {A {{Competitive Neural Network}} for {{Multiple Object Tracking}} in {{Video Sequence Analysis}}},
  volume = {37},
  issn = {1370-4621, 1573-773X},
  abstract = {Tracking of moving objects in real situation is a challenging research issue, due to dynamic changes in objects or background appearance, illumination, shape and occlusions. In this paper, we deal with these difficulties by incorporating an adaptive feature weighting mechanism to the proposed growing competitive neural network for multiple objects tracking. The neural network takes advantage of the most relevant object features (information provided by the proposed adaptive feature weighting mechanism) in order to estimate the trajectories of the moving objects. The feature selection mechanism is based on a genetic algorithm, and the tracking algorithm is based on a growing competitive neural network where each unit is associated to each object in the scene. The proposed methods (object tracking and feature selection mechanism) are applied to detect the trajectories of moving vehicles in roads. Experimental results show the performance of the proposed system compared to the standard Kalman filter.},
  language = {en},
  number = {1},
  journal = {Neural Processing Letters},
  doi = {10.1007/s11063-012-9268-3},
  author = {{Luque-Baena}, Rafael M. and {Ortiz-de-Lazcano-Lobato}, Juan M. and {L{\'o}pez-Rubio}, Ezequiel and Dom{\'i}nguez, Enrique and J. Palomo, Esteban},
  month = feb,
  year = {2013},
  keywords = {object tracking},
  pages = {47-67},
  file = {/home/henrik/Zotero/storage/HC7K3L48/Luque-Baena et al. - 2013 - A Competitive Neural Network for Multiple Object T.pdf}
}

@article{ObjectTracking15,
  title = {Object {{Tracking Benchmark}}},
  volume = {37},
  issn = {0162-8828},
  abstract = {Object tracking has been one of the most important and active research areas in the field of computer vision. A large number of tracking algorithms have been proposed in recent years with demonstrated success. However, the set of sequences used for evaluation is often not sufficient or is sometimes biased for certain types of algorithms. Many datasets do not have common ground-truth object positions or extents, and this makes comparisons among the reported quantitative results difficult. In addition, the initial conditions or parameters of the evaluated tracking algorithms are not the same, and thus, the quantitative results reported in literature are incomparable or sometimes contradictory. To address these issues, we carry out an extensive evaluation of the state-of-the-art online object-tracking algorithms with various evaluation criteria to understand how these methods perform within the same framework. In this work, we first construct a large dataset with ground-truth object positions and extents for tracking and introduce the sequence attributes for the performance analysis. Second, we integrate most of the publicly available trackers into one code library with uniform input and output formats to facilitate large-scale performance evaluation. Third, we extensively evaluate the performance of 31 algorithms on 100 sequences with different initialization settings. By analyzing the quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.},
  number = {9},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  doi = {10.1109/TPAMI.2014.2388226},
  author = {Wu, Y. and Lim, J. and Yang, M.},
  month = sep,
  year = {2015},
  keywords = {Algorithm design and analysis,benchmark dataset,Histograms,object tracking,Object tracking,performance evaluation,Performance evaluation,Robustness,Target tracking},
  pages = {1834-1848},
  file = {/home/henrik/Zotero/storage/QI7DVTQK/Wu et al. - 2015 - Object Tracking Benchmark.pdf;/home/henrik/Zotero/storage/SA4AYB9C/7001050.html}
}

@misc{GOTURNDeep,
  title = {{{GOTURN}} : {{Deep Learning}} Based {{Object Tracking}} | {{Learn OpenCV}}},
  howpublished = {https://www.learnopencv.com/goturn-deep-learning-based-object-tracking/}
}

@article{WirelessCapsule00,
  title = {Wireless Capsule Endoscopy},
  volume = {405},
  copyright = {2000 Macmillan Magazines Ltd.},
  issn = {1476-4687},
  abstract = {The discomfort of internal gastrointestinal examination may soon be a thing of the past.},
  language = {En},
  number = {6785},
  journal = {Nature},
  doi = {10.1038/35013140},
  author = {Iddan, Gavriel and Meron, Gavriel and Glukhovsky, Arkady and Swain, Paul},
  month = may,
  year = {2000},
  pages = {417},
  file = {/home/henrik/Zotero/storage/U6JJ6MIK/Iddan et al. - 2000 - Wireless capsule endoscopy.pdf;/home/henrik/Zotero/storage/HS7TCR72/35013140.html}
}

@misc{CapsuleEndoscopyAlanCrawford56a11c235f9b58b7d0bbcd15Jpg,
  title = {Capsule-{{Endoscopy}}-{{Alan}}-{{Crawford}}-56a11c235f9b58b7d0bbcd15.Jpg (2800\texttimes{}2100)},
  howpublished = {https://www.verywellhealth.com/thmb/IVrSo77yi8FT4dc0laOqgVbSIDg=/2800x2100/filters:fill(87E3EF,1)/Capsule-Endoscopy-Alan-Crawford-56a11c235f9b58b7d0bbcd15.jpg},
  file = {/home/henrik/Zotero/storage/QQEAC6K6/Capsule-Endoscopy-Alan-Crawford-56a11c235f9b58b7d0bbcd15.html}
}

@misc{MedicalPhysics,
  title = {Medical {{Physics}} - {{Endoscopes}}},
  howpublished = {http://www.genesis.net.au/\textasciitilde{}ajs/projects/medical\_physics/endoscopes/index.html},
  file = {/home/henrik/Zotero/storage/YTY6DUKB/index.html}
}

@article{NewMethod54,
  title = {A {{New Method}} of Transporting {{Optical Images}} without {{Aberrations}}},
  volume = {173},
  issn = {0028-0836, 1476-4687},
  language = {en},
  number = {4392},
  journal = {Nature},
  doi = {10.1038/173039a0},
  author = {Van Heel, A. C. S.},
  month = jan,
  year = {1954},
  pages = {39-39},
  file = {/home/henrik/Zotero/storage/R3TXS4M8/Van Heel - 1954 - A New Method of transporting Optical Images withou.pdf}
}

@inproceedings{ExpertDriven15,
  address = {{Portland, Oregon}},
  title = {Expert Driven Semi-Supervised Elucidation Tool for Medical Endoscopic Videos},
  isbn = {978-1-4503-3351-1},
  abstract = {In this paper, we present a novel application for elucidating all kind of videos that require expert knowledge, e.g., sport videos, medical videos etc., focusing on endoscopic surgery and video capsule endoscopy. In the medical domain, the knowledge of experts for tagging and interpretation of videos is of high value. As a result of the stressful working environment of medical doctors, they often simply do not have time for extensive annotations. We therefore present a semisupervised method to gather the annotations in a very easy and time saving way for the experts and we show how this information can be used later on.},
  booktitle = {Proceedings of the 6th {{ACM Multimedia Systems Conference}}},
  publisher = {{ACM Press}},
  doi = {10.1145/2713168.2713184},
  author = {Albisser, Zeno and Riegler, Michael and Halvorsen, P{\aa}l and Zhou, Jiang and Griwodz, Carsten and Balasingham, Ilangko and Gurrin, Cathal},
  year = {2015},
  pages = {73-76},
  file = {/home/henrik/Zotero/storage/3V2GE6II/Albisser et al. - 2015 - Expert driven semi-supervised elucidation tool for.pdf}
}

@misc{ImageColon13,
  title = {Image of Colon},
  shorttitle = {Deutsch},
  author = {Dr.HH.Krause},
  month = jun,
  year = {2013},
  file = {/home/henrik/Zotero/storage/TSYSJMVF/FileNormales_Colon.html}
}

@misc{SmallIntestine13,
  title = {Small Intestine},
  shorttitle = {Deutsch},
  author = {{Dr.HH.Krause}},
  month = jun,
  year = {2013},
  file = {/home/henrik/Zotero/storage/9VVYEPIQ/FileDünndarm.html}
}

@article{DigestiveSystem,
  title = {Digestive System Diagram},
  copyright = {Creative Commons Attribution-ShareAlike License},
  shorttitle = {Digestive {{System}}},
  language = {en},
  journal = {Wikipedia},
  file = {/home/henrik/Zotero/storage/CAT5AHNQ/FileDigestive_system_diagram_edit.html}
}

@inproceedings{DeepConvolutional16,
  title = {A Deep Convolutional Neural Network for Bleeding Detection in {{Wireless Capsule Endoscopy}} Images},
  abstract = {Wireless Capsule Endoscopy (WCE) is a standard non-invasive modality for small bowel examination. Recently, the development of computer-aided diagnosis (CAD) systems for gastrointestinal (GI) bleeding detection in WCE image videos has become an active research area with the goal of relieving the workload of physicians. Existing methods based primarily on handcrafted features usually give insufficient accuracy for bleeding detection, due to their limited capability of feature representation. In this paper, we present a new automatic bleeding detection strategy based on a deep convolutional neural network and evaluate our method on an expanded dataset of 10,000 WCE images. Experimental results with an increase of around 2 percentage points in the Fi score demonstrate that our method outperforms the state-of-the-art approaches in WCE bleeding detection. The achieved Fi score is of up to 0.9955.},
  booktitle = {Proceedings of the 2016 38th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  doi = {10.1109/EMBC.2016.7590783},
  author = {Jia, X. and Meng, M. Q.-},
  month = aug,
  year = {2016},
  keywords = {wireless capsule endoscopy,automatic bleeding detection strategy,biomedical optical imaging,CAD,Capsule Endoscopy,computer-aided diagnosis systems,deep convolutional neural network,Diagnosis; Computer-Assisted,endoscopes,Endoscopes,Feature extraction,feature representation,Fi score,gastrointestinal bleeding detection,Gastrointestinal Hemorrhage,Hemorrhaging,Humans,medical disorders,medical image processing,neural nets,Neural Networks (Computer),small bowel examination,standard noninvasive modality,Support vector machines,Training,Videos,WCE bleeding detection,WCE image videos,Wireless communication},
  pages = {639-642},
  file = {/home/henrik/Zotero/storage/AL7M2D2G/Jia and Meng - 2016 - A deep convolutional neural network for bleeding d.pdf;/home/henrik/Zotero/storage/GK7TEZ73/7590783.html}
}

@inproceedings{ClassifyingDigestive15,
  title = {Classifying Digestive Organs in Wireless Capsule Endoscopy Images Based on Deep Convolutional Neural Network},
  abstract = {This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95\% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.},
  booktitle = {Proceedings of the 2015 {{IEEE International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  doi = {10.1109/ICDSP.2015.7252086},
  author = {Zou, Y. and Li, L. and Wang, Y. and Yu, J. and Li, Y. and Deng, W. J.},
  month = jul,
  year = {2015},
  keywords = {wireless capsule endoscopy,biomedical optical imaging,deep convolutional neural network,endoscopes,Endoscopes,Feature extraction,medical image processing,Training,Wireless communication,Accuracy,biological organs,brightness,complex digestive tract circumstance,Convolution,DCNN-WCE-CS,deep CNN-based WCE classification system,digestive organ classification problem,digestive organs classification,feature extraction,feedforward neural nets,human biological visual systems,image classification,Intestines,layer-wise hierarchy models,learning (artificial intelligence),luminance change,object recognition,parameter selection,semantic image feature recognition,training data,wireless capsule endoscopy images,detecting organs},
  pages = {1274-1278},
  file = {/home/henrik/Zotero/storage/7N333XQA/Zou et al. - 2015 - Classifying digestive organs in wireless capsule e.pdf;/home/henrik/Zotero/storage/X9LW93CT/7252086.html}
}

@article{NotesBackpropagation16,
  title = {Notes on Backpropagation},
  author = {Sadowski, Peter},
  year = {2016},
  file = {/home/henrik/Zotero/storage/SC592RJV/Sadowski - 2016 - Notes on backpropagation.pdf}
}

@inproceedings{LesionDetection15,
  title = {Lesion Detection of Endoscopy Images Based on Convolutional Neural Network Features},
  abstract = {Since gastroscopy is able to observe the interior of gastrointestinal tract directly, it has been widely used for gastrointestinal examination. But it is hard for clinicians to accurately detect gastrointestinal disease due to its great dependence on doctors experiences. Therefore, a computer-aided lesion detection system can offer great help for clinicians. In this paper, we propose a new scheme for endoscopy image lesion detection. A trainable feature extractor based on convolutional neural network (CNN) is utilized to get more generic features for endoscopy images. And features are fed to support vector machine (SVM) to enhance the generalization ability. Experiments show that the proposed scheme outperforms the previous conventional methods based on color and texture features.},
  booktitle = {Proceedings of the 2015 8th {{International Congress}} on {{Image}} and {{Signal Processing}} ({{CISP}})},
  doi = {10.1109/CISP.2015.7407907},
  author = {Zhu, R. and Zhang, R. and Xue, D.},
  month = oct,
  year = {2015},
  keywords = {cancer,Histograms,endoscopes,Endoscopes,Feature extraction,medical image processing,neural nets,Support vector machines,feature extraction,CNN,color features,computer-aided lesion detection system,convolutional neural network features,detect gastrointestinal disease,endoscopy image lesion detection,gastrointestinal examination,Gastrointestinal tract,gastrointestinal tract directly,Image color analysis,image colour analysis,image texture,Lesions,support vector machine,support vector machines,SVM,texture features,trainable feature extractor},
  pages = {372-376},
  file = {/home/henrik/Zotero/storage/9ILDKALR/Zhu et al. - 2015 - Lesion detection of endoscopy images based on conv.pdf;/home/henrik/Zotero/storage/XMXK62TU/7407907.html}
}

@article{DeepEndoVO18,
  title = {Deep {{EndoVO}}: {{A}} Recurrent Convolutional Neural Network ({{RCNN}}) Based Visual Odometry Approach for Endoscopic Capsule Robots},
  volume = {275},
  issn = {0925-2312},
  shorttitle = {Deep {{EndoVO}}},
  abstract = {Ingestible wireless capsule endoscopy is an emerging minimally invasive diagnostic technology for inspection of the GI tract and diagnosis of a wide range of diseases and pathologies. Medical device companies and many research groups have recently made substantial progresses in converting passive capsule endoscopes to active capsule robots, enabling more accurate, precise, and intuitive detection of the location and size of the diseased areas. Since a reliable real time pose estimation functionality is crucial for actively controlled endoscopic capsule robots, in this study, we propose a monocular visual odometry (VO) method for endoscopic capsule robot operations. Our method lies on the application of the deep recurrent convolutional neural networks (RCNNs) for the visual odometry task, where convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are used for the feature extraction and inference of dynamics across the frames, respectively. Detailed analyses and evaluations made on a real pig stomach dataset proves that our system achieves high translational and rotational accuracies for different types of endoscopic capsule robot trajectories.},
  journal = {Neurocomputing},
  doi = {10.1016/j.neucom.2017.10.014},
  author = {Turan, Mehmet and Almalioglu, Yasin and Araujo, Helder and Konukoglu, Ender and Sitti, Metin},
  month = jan,
  year = {2018},
  keywords = {CNN,Endoscopic capsule robot,Localization,LSTM,RCNN,Sequential deep learning,Visual odometry},
  pages = {1861-1870},
  file = {/home/henrik/Zotero/storage/IW7GIJDZ/Turan et al. - 2018 - Deep EndoVO A recurrent convolutional neural netw.pdf;/home/henrik/Zotero/storage/HV6QI5IW/S092523121731665X.html}
}

@article{DeepLearning17,
  title = {Deep Learning for Polyp Recognition in Wireless Capsule Endoscopy Images},
  volume = {44},
  issn = {00942405},
  abstract = {Purpose: Wireless capsule endoscopy (WCE) enables physicians to examine the digestive tract without any surgical operations, at the cost of a large volume of images to be analyzed. In the computer-aided diagnosis of WCE images, the main challenge arises from the difficulty of robust characterization of images. This study aims to provide discriminative description of WCE images and assist physicians to recognize polyp images automatically.
Methods: We propose a novel deep feature learning method, named stacked sparse autoencoder with image manifold constraint (SSAEIM), to recognize polyps in the WCE images. Our SSAEIM differs from the traditional sparse autoencoder (SAE) by introducing an image manifold constraint, which is constructed by a nearest neighbor graph and represents intrinsic structures of images. The image manifold constraint enforces that images within the same category share similar learned features and images in different categories should be kept far away. Thus, the learned features preserve large intervariances and small intravariances among images.
Results: The average overall recognition accuracy (ORA) of our method for WCE images is 98.00\%. The accuracies for polyps, bubbles, turbid images, and clear images are 98.00\%, 99.50\%, 99.00\%, and 95.50\%, respectively. Moreover, the comparison results show that our SSAEIM outperforms existing polyp recognition methods with relative higher ORA.
Conclusion: The comprehensive results have demonstrated that the proposed SSAEIM can provide descriptive characterization for WCE images and recognize polyps in a WCE video accurately. This method could be further utilized in the clinical trials to help physicians from the tedious image reading work. \textcopyright{} 2017 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.12147]},
  language = {en},
  number = {4},
  journal = {Medical Physics},
  doi = {10.1002/mp.12147},
  author = {Yuan, Yixuan and Meng, Max Q.-H.},
  month = apr,
  year = {2017},
  pages = {1379-1389},
  file = {/home/henrik/Zotero/storage/SAXC9CNE/Yuan and Meng - 2017 - Deep learning for polyp recognition in wireless ca.pdf}
}

@inproceedings{IdentificationUlcers09,
  title = {Identification of Ulcers in {{Wireless Capsule Endoscopy}} Videos},
  abstract = {Wireless Capsule Endoscopy (WCE) is a non invasive procedure which is used to view the lower gastrointestinal tract. Physicians can detect diseases such as bleeding, Crohn's disease, peptic ulcers, and colon cancer. In this paper a methodology is presented to identify peptic ulcers in the small intestine automatically. It first performs color transformation into the HSV color space; it utilizes log Gabor filters to find meaningful regions. A segmentation scheme is used to extract color information of these meaningful regions in the original RGB color space. Additionally, texture information is extracted and together with color values are fed into an artificial neural network for classification. Illustrative results from the methodology are also given in this paper.},
  booktitle = {Proceedings of the 2009 {{IEEE International Symposium}} on {{Biomedical Imaging}}: {{From Nano}} to {{Macro}}},
  doi = {10.1109/ISBI.2009.5193107},
  author = {Karargyris, A. and Bourbakis, N.},
  month = jun,
  year = {2009},
  keywords = {colon cancer,endoscopes,Endoscopes,Hemorrhaging,medical image processing,neural nets,Videos,biological organs,feature extraction,image classification,Intestines,Gastrointestinal tract,artificial neural network,bleeding,Cancer detection,Colon,color information extraction,Crohn's disease,Data mining,diseases,Diseases,fuzzy least squares support vector machines,fuzzy region segmentation,Gabor filters,gastrointestinal tract,HSV color space,image segmentation,log Gabor filters,log-Gabor filters,noninvasive procedure,peptic ulcers,RGB color space,segmentation scheme,small intestine,texture,texture information,ulcers,video signal processing,Wireless capsule Endoscopy Imaging,wireless capsule endoscopy videos},
  pages = {554-557},
  file = {/home/henrik/Zotero/storage/6VBSP4UY/Karargyris and Bourbakis - 2009 - Identification of ulcers in Wireless Capsule Endos.pdf;/home/henrik/Zotero/storage/BW92FMMW/5193107.html}
}

@article{DeepConvolutional13,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1312.6034},
  primaryClass = {cs},
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  journal = {arXiv:1312.6034 [cs]},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  month = dec,
  year = {2013},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/henrik/Zotero/storage/DSPZ2LDD/Simonyan et al. - 2013 - Deep Inside Convolutional Networks Visualising Im.pdf;/home/henrik/Zotero/storage/628HFNB7/1312.html}
}

@inproceedings{IdentificationUlcers09a,
  title = {Identification of Ulcers in {{Wireless Capsule Endoscopy}} Videos},
  abstract = {Wireless Capsule Endoscopy (WCE) is a non invasive procedure which is used to view the lower gastrointestinal tract. Physicians can detect diseases such as bleeding, Crohn's disease, peptic ulcers, and colon cancer. In this paper a methodology is presented to identify peptic ulcers in the small intestine automatically. It first performs color transformation into the HSV color space; it utilizes log Gabor filters to find meaningful regions. A segmentation scheme is used to extract color information of these meaningful regions in the original RGB color space. Additionally, texture information is extracted and together with color values are fed into an artificial neural network for classification. Illustrative results from the methodology are also given in this paper.},
  booktitle = {Proceedings of the 2009 {{IEEE International Symposium}} on {{Biomedical Imaging}}: {{From Nano}} to {{Macro}}},
  doi = {10.1109/ISBI.2009.5193107},
  author = {Karargyris, A. and Bourbakis, N.},
  month = jun,
  year = {2009},
  keywords = {colon cancer,endoscopes,Endoscopes,Hemorrhaging,medical image processing,neural nets,Videos,biological organs,feature extraction,image classification,Intestines,Gastrointestinal tract,artificial neural network,bleeding,Cancer detection,Colon,color information extraction,Crohn's disease,Data mining,diseases,Diseases,fuzzy least squares support vector machines,fuzzy region segmentation,Gabor filters,gastrointestinal tract,HSV color space,image segmentation,log Gabor filters,log-Gabor filters,noninvasive procedure,peptic ulcers,RGB color space,segmentation scheme,small intestine,texture,texture information,ulcers,video signal processing,Wireless capsule Endoscopy Imaging,wireless capsule endoscopy videos},
  pages = {554-557},
  file = {/home/henrik/Zotero/storage/667XME3L/Karargyris and Bourbakis - 2009 - Identification of ulcers in Wireless Capsule Endos.pdf;/home/henrik/Zotero/storage/U3HA6ZRY/5193107.html}
}

@article{CNNbasedSegmentation17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.03056},
  primaryClass = {cs},
  title = {{{CNN}}-Based {{Segmentation}} of {{Medical Imaging Data}}},
  abstract = {Convolutional neural networks have been applied to a wide variety of computer vision tasks. Recent advances in semantic segmentation have enabled their application to medical image segmentation. While most CNNs use two-dimensional kernels, recent CNN-based publications on medical image segmentation featured three-dimensional kernels, allowing full access to the three-dimensional structure of medical images. Though closely related to semantic segmentation, medical image segmentation includes specific challenges that need to be addressed, such as the scarcity of labelled data, the high class imbalance found in the ground truth and the high memory demand of three-dimensional images. In this work, a CNN-based method with three-dimensional filters is demonstrated and applied to hand and brain MRI. Two modifications to an existing CNN architecture are discussed, along with methods on addressing the aforementioned challenges. While most of the existing literature on medical image segmentation focuses on soft tissue and the major organs, this work is validated on data both from the central nervous system as well as the bones of the hand.},
  journal = {arXiv:1701.03056 [cs]},
  author = {Kayalibay, Baris and Jensen, Grady and {van der Smagt}, Patrick},
  month = jan,
  year = {2017},
  keywords = {Betina,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/henrik/Zotero/storage/TPRA2E2Y/Kayalibay et al. - 2017 - CNN-based Segmentation of Medical Imaging Data.pdf;/home/henrik/Zotero/storage/X8T859BV/1701.html}
}

@inproceedings{UNetConvolutional15,
  series = {Lecture {{Notes}} in {{Computer Science}}},
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  isbn = {978-3-319-24574-4},
  shorttitle = {U-{{Net}}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  language = {en},
  booktitle = {Proceedings of the {{Medical Image Computing}} and {{Computer}}-{{Assisted Intervention}}},
  publisher = {{Springer International Publishing}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  keywords = {Betina,Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  pages = {234-241},
  file = {/home/henrik/Zotero/storage/HVJEMHFB/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf}
}

@incollection{ImageNetClassification12,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  publisher = {{Curran Associates, Inc.}},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  pages = {1097--1105},
  file = {/home/henrik/Zotero/storage/8C5IIC5A/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf;/home/henrik/Zotero/storage/ID4BRJ6C/4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}

@article{DeepReinforcement17,
  archivePrefix = {arXiv},
  eprinttype = {arxiv},
  eprint = {1701.08936},
  primaryClass = {cs},
  title = {Deep {{Reinforcement Learning}} for {{Visual Object Tracking}} in {{Videos}}},
  abstract = {In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than real-time. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.},
  journal = {arXiv:1701.08936 [cs]},
  author = {Zhang, Da and Maei, Hamid and Wang, Xin and Wang, Yuan-Fang},
  month = jan,
  year = {2017},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/henrik/Zotero/storage/7VJDK8ZD/Zhang et al. - 2017 - Deep Reinforcement Learning for Visual Object Trac.pdf;/home/henrik/Zotero/storage/NGVQ2SIY/1701.html}
}

@inproceedings{FullyConvolutional15,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431-3440},
  file = {/home/henrik/Zotero/storage/DEGCVEFG/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf;/home/henrik/Zotero/storage/DIC8VB4W/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html}
}

@article{ShapeShading94,
  title = {Shape from Shading Using Linear Approximation},
  volume = {12},
  issn = {0262-8856},
  abstract = {In this paper, we present an extremely simple algorithm for shape from shading, which can be implemented in 25 lines of C code{${_\ast}{_\ast}$}C code and some images can be obtained by anonymous ftp from under the directory /pub/shading.. The algorithm is very fast, taking 0.2 seconds on a Sun SparcStation-1 for a 128 \texttimes{} 128 image, and is purely local and highly parallelizable (parallel implementation included). In our approach, we employ a linear approximation of the reflectance function, as used by others. However, the main difference is that we first use the discrete approximations for surface normal, p and q, using finite differences, and then linearize the reflectance function in depth, Z(x, y), instead of p and q. The algorithm has been tested on several synthetic and real images of both Lambertian and specular surfaces, and good results have been obtained.},
  number = {8},
  journal = {Image and Vision Computing},
  doi = {10.1016/0262-8856(94)90002-7},
  author = {{Ping-Sing}, Tsai and Shah, Mubarak},
  month = oct,
  year = {1994},
  keywords = {3D shape,physics-based vision,shape from shading},
  pages = {487-498},
  file = {/home/henrik/Zotero/storage/RP6KRHXZ/0262885694900027.html}
}


