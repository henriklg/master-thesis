
@article{3DCNNLSTM19,
  title = {A {{3D}}-{{CNN}} and {{LSTM Based Multi}}-{{Task Learning Architecture}} for {{Action Recognition}}},
  author = {Ouyang, Xi and Xu, Shuangjie and Zhang, Chaoyun and Zhou, Pan and Yang, Yang and Liu, Guanghui and Li, Xuelong},
  year = {2019},
  volume = {7},
  pages = {40757--40770},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2906654},
  abstract = {Multi-task learning (MTL) is a machine learning method to share knowledge for multiple related machine learning tasks via learning those tasks jointly. It has been shown to be capable of effectively improving the generalization capability of each single task (learning just one task at a time). In this paper, we propose a novel MTL architecture that first combines 3D convolutional neural networks (3D CNN) plus the long short-term memory (LSTM) networks together with the MTL mechanism, tailored to information sharing of video inputs. We split each video into several clips and apply the hybrid deep model of 3D CNN and LSTM to extract the sequential features of those video clips. Therefore, our MTL model can share visual knowledge based on those video-clip features among different categories more efficiently. We evaluate our method on three popular public action recognition video datasets. The experimental results show that our novel MTL method can efficiently share detailed information in video clips among multiple action categories and outperforms other multi-task methods.},
  file = {/home/henrik/Zotero/storage/3FWWG5BC/Ouyang et al. - 2019 - A 3D-CNN and LSTM Based Multi-Task Learning Archit.pdf;/home/henrik/Zotero/storage/MF8PRAAB/8677269.html},
  journal = {IEEE Access},
  keywords = {3D CNN,3D convolutional neural networks,3D-CNN,Action recognition,Computer vision,convolutional neural nets,Deep learning,feature extraction,Feature extraction,generalization capability,hybrid deep model,information sharing,learning (artificial intelligence),long short-term memory,LSTM,LSTM based multitask learning architecture,Matrix decomposition,MTL architecture,multi-task learning,multiple action categories,multiple related machine learning tasks,Pattern recognition,public action recognition video datasets,sequential features,single task,Task analysis,Three-dimensional displays,video inputs,video signal processing,video-clip features}
}

@article{3DConvolutional13,
  title = {{{3D Convolutional Neural Networks}} for {{Human Action Recognition}}},
  author = {Ji, Shuiwang and Xu, Wei and Yang, Ming and Yu, Kai},
  year = {2013},
  month = jan,
  volume = {35},
  pages = {221--231},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2012.59},
  abstract = {We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods.},
  file = {/home/henrik/Zotero/storage/3I6IMF55/Ji et al. - 2013 - 3D Convolutional Neural Networks for Human Action .pdf;/home/henrik/Zotero/storage/NQULXMQK/6165309.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {3D CNN model,3D convolution,3D convolutional neural networks,action recognition,airport surveillance videos,Algorithms,automated human action recognition,baseline methods,complex handcrafted features,Computational modeling,Computer architecture,convolutional neural networks,Decision Support Techniques,Deep learning,deep model,feature extraction,Feature extraction,feature representation,gesture recognition,high-level features,image classification,Image Interpretation; Computer-Assisted,image motion analysis,image representation,Imaging; Three-Dimensional,Kernel,model combination,motion information encoding,Movement,neural nets,Neural Networks (Computer),Pattern Recognition; Automated,Solid modeling,spatial dimensions,spatiotemporal phenomena,Subtraction Technique,temporal dimensions,Three dimensional displays,video surveillance,Videos},
  number = {1}
}

@article{ADADELTAAdaptive12,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  author = {Zeiler, Matthew D.},
  year = {2012},
  month = dec,
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  archivePrefix = {arXiv},
  eprint = {1212.5701},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/PTLY3CGP/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf;/home/henrik/Zotero/storage/ZYNDRCSC/1212.html},
  journal = {arXiv:1212.5701 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@book{ArtificialIntelligence16,
  title = {Artificial {{Intelligence}} : {{A Modern Approach}}},
  shorttitle = {Artificial {{Intelligence}}},
  author = {Russell, Stuart J. and Norvig, Peter},
  year = {2016},
  publisher = {{Malaysia; Pearson Education Limited,}},
  abstract = {Artificial Intelligence (AI) is a big field, and this is a big book. We have tried to explore the full breadth of the field, which encompasses logic, probability, and continuous mathematics; perception, reasoning, learning, and action; and everything from microelectronic devices to robotic planetary explorers. The book is also big because we go into some depth.},
  file = {/home/henrik/Zotero/storage/G5Z53ZUJ/Russell and Norvig - 2016 - Artificial Intelligence  A Modern Approach.html},
  language = {en}
}

@phdthesis{AutomaticAnalysis17,
  title = {Automatic {{Analysis}} of {{Endoscopic Videos}}},
  author = {H{\o}iland, Torbj{\o}rn Nesb{\o}},
  year = {2017},
  abstract = {The human digestive system can be affected by many types of diseases. For example, three of the six most common cancer types (esophagus, stomach and colorectal) are located in the gastrointestinal tract. Colorectal cancer (CRC) is the third most common cancer in men and the second most common cancer in women worldwide, and Norway has one of the highest incidences of this cancer. Early detection is vital for the prognosis, level of treatment and survival. EIR is a multimedia system with the main objective of supporting doctors in gastrointestinal tract disease detection, both as a live examination system and an offline system for VCE. However, the detection and automatic analysis subsystem within EIR today consists of two parts; the detection subsystem and the localisation subsystem. Recent advances in machine learning, particularly deep learning, have provided excellent object detection models. This thesis explores the possibility of using a deep neural network at the base of the detection and automatic analysis subsystem in EIR, specifically by using You only look once (YOLO). YOLO is a state-of-the-art, real-time object detection system that was used together with the ASU Mayo Clinic polyp database to detect CRC precursors called polyps. The YOLO system reaches a satisfactory detection accuracy, while still being able to process videos in real-time. The proposed system and EIR is compared using the standard metrics of recall, precision and F1-score. When compared, it is clear that the system still has room for improvement in regard to its precision.},
  file = {/home/henrik/Zotero/storage/SUFDRZJD/HÃ¸iland - 2017 - Automatic Analysis of Endoscopic Videos.pdf;/home/henrik/Zotero/storage/EKD2IYKW/56885.html},
  language = {eng},
  school = {University of Oslo}
}

@article{BleedingDetection19,
  title = {Bleeding Detection in Wireless Capsule Endoscopy Videos \textemdash{} {{Color}} versus Texture Features},
  author = {Pogorelov, Konstantin and Suman, Shipra and Azmadi Hussin, Fawnizu and Saeed Malik, Aamir and Ostroukhova, Olga and Riegler, Michael and Halvorsen, P{\aa}l and Hooi Ho, Shiaw and Goh, Khean-Lee},
  year = {2019},
  month = aug,
  volume = {20},
  pages = {141--154},
  issn = {1526-9914},
  doi = {10.1002/acm2.12662},
  abstract = {Abstract Wireless capsule endoscopy (WCE) is an effective technology that can be used to make a gastrointestinal (GI) tract diagnosis of various lesions and abnormalities. Due to a long time required to pass through the GI tract, the resulting WCE data stream contains a large number of frames which leads to a tedious job for clinical experts to perform a visual check of each and every frame of a complete patient?s video footage. In this paper, an automated technique for bleeding detection based on color and texture features is proposed. The approach combines the color information which is an essential feature for initial detection of frame with bleeding. Additionally, it uses the texture which plays an important role to extract more information from the lesion captured in the frames and allows the system to distinguish finely between borderline cases. The detection algorithm utilizes machine-learning-based classification methods, and it can efficiently distinguish between bleeding and nonbleeding frames and perform pixel-level segmentation of bleeding areas in WCE frames. The performed experimental studies demonstrate the performance of the proposed bleeding detection method in terms of detection accuracy, where we are at least as good as the state-of-the-art approaches. In this research, we have conducted a broad comparison of a number of different state-of-the-art features and classification methods that allows building an efficient and flexible WCE video processing system.},
  file = {/home/henrik/Zotero/storage/YABA4NNF/Pogorelov et al. - 2019 - Bleeding detection in wireless capsule endoscopy v.pdf;/home/henrik/Zotero/storage/IRVPWGI2/acm2.html},
  journal = {Journal of Applied Clinical Medical Physics},
  keywords = {bleeding detection,color feature,machine learning,texture feature,wireless capsule endoscopy},
  number = {8}
}

@article{BleedingDetection19a,
  title = {Bleeding Detection in Wireless Capsule Endoscopy Videos \textemdash{} {{Color}} versus Texture Features},
  author = {Pogorelov, Konstantin and Suman, Shipra and Hussin, Fawnizu Azmadi and Malik, Aamir Saeed and Ostroukhova, Olga and Riegler, Michael and Halvorsen, P{\aa}l and Ho, Shiaw Hooi and Goh, Khean-Lee},
  year = {2019},
  volume = {20},
  pages = {141--154},
  issn = {1526-9914},
  doi = {10.1002/acm2.12662},
  abstract = {Wireless capsule endoscopy (WCE) is an effective technology that can be used to make a gastrointestinal (GI) tract diagnosis of various lesions and abnormalities. Due to a long time required to pass through the GI tract, the resulting WCE data stream contains a large number of frames which leads to a tedious job for clinical experts to perform a visual check of each and every frame of a complete patient's video footage. In this paper, an automated technique for bleeding detection based on color and texture features is proposed. The approach combines the color information which is an essential feature for initial detection of frame with bleeding. Additionally, it uses the texture which plays an important role to extract more information from the lesion captured in the frames and allows the system to distinguish finely between borderline cases. The detection algorithm utilizes machine-learning-based classification methods, and it can efficiently distinguish between bleeding and nonbleeding frames and perform pixel-level segmentation of bleeding areas in WCE frames. The performed experimental studies demonstrate the performance of the proposed bleeding detection method in terms of detection accuracy, where we are at least as good as the state-of-the-art approaches. In this research, we have conducted a broad comparison of a number of different state-of-the-art features and classification methods that allows building an efficient and flexible WCE video processing system.},
  file = {/home/henrik/Zotero/storage/I59VMQ4B/Pogorelov et al. - 2019 - Bleeding detection in wireless capsule endoscopy v.pdf;/home/henrik/Zotero/storage/7F258ICU/acm2.html},
  journal = {Journal of Applied Clinical Medical Physics},
  keywords = {bleeding detection,color feature,machine learning,texture feature,wireless capsule endoscopy},
  language = {en},
  number = {8}
}

@article{CancerStatistics10,
  title = {Cancer {{Statistics}}},
  author = {Jemal, Ahmedin and Siegel, Rebecca and Xu, Jiaquan and Ward, Elizabeth},
  year = {2010},
  volume = {60},
  pages = {277--300},
  issn = {1542-4863},
  doi = {10.3322/caac.20073},
  abstract = {Each year, the American Cancer Society estimates the number of new cancer cases and deaths expected in the United States in the current year and compiles the most recent data regarding cancer incidence, mortality, and survival based on incidence data from the National Cancer Institute, the Centers for Disease Control and Prevention, and the North American Association of Central Cancer Registries and mortality data from the National Center for Health Statistics. Incidence and death rates are age-standardized to the 2000 US standard million population. A total of 1,529,560 new cancer cases and 569,490 deaths from cancer are projected to occur in the United States in 2010. Overall cancer incidence rates decreased in the most recent time period in both men (1.3\% per year from 2000 to 2006) and women (0.5\% per year from 1998 to 2006), largely due to decreases in the 3 major cancer sites in men (lung, prostate, and colon and rectum [colorectum]) and 2 major cancer sites in women (breast and colorectum). This decrease occurred in all racial/ethnic groups in both men and women with the exception of American Indian/Alaska Native women, in whom rates were stable. Among men, death rates for all races combined decreased by 21.0\% between 1990 and 2006, with decreases in lung, prostate, and colorectal cancer rates accounting for nearly 80\% of the total decrease. Among women, overall cancer death rates between 1991 and 2006 decreased by 12.3\%, with decreases in breast and colorectal cancer rates accounting for 60\% of the total decrease. The reduction in the overall cancer death rates translates to the avoidance of approximately 767,000 deaths from cancer over the 16-year period. This report also examines cancer incidence, mortality, and survival by site, sex, race/ethnicity, geographic area, and calendar year. Although progress has been made in reducing incidence and mortality rates and improving survival, cancer still accounts for more deaths than heart disease in persons younger than 85 years. Further progress can be accelerated by applying existing cancer control knowledge across all segments of the population and by supporting new discoveries in cancer prevention, early detection, and treatment. CA Cancer J Clin 2010. \textcopyright{} 2010 American Cancer Society, Inc.},
  file = {/home/henrik/Zotero/storage/UDTTBEGD/Jemal et al. - 2010 - Cancer Statistics, 2010.pdf;/home/henrik/Zotero/storage/D9BH79GY/caac.html},
  journal = {CA: A Cancer Journal for Clinicians},
  keywords = {cancer statistics},
  language = {en},
  number = {5}
}

@misc{CapsuleEndoscopyAlanCrawford56a11c235f9b58b7d0bbcd15Jpg,
  title = {Capsule-{{Endoscopy}}-{{Alan}}-{{Crawford}}-56a11c235f9b58b7d0bbcd15.Jpg (2800\texttimes{}2100)},
  file = {/home/henrik/Zotero/storage/QQEAC6K6/Capsule-Endoscopy-Alan-Crawford-56a11c235f9b58b7d0bbcd15.html},
  howpublished = {https://www.verywellhealth.com/thmb/IVrSo77yi8FT4dc0laOqgVbSIDg=/2800x2100/filters:fill(87E3EF,1)/Capsule-Endoscopy-Alan-Crawford-56a11c235f9b58b7d0bbcd15.jpg}
}

@inproceedings{ClassifyingDigestive15,
  title = {Classifying Digestive Organs in Wireless Capsule Endoscopy Images Based on Deep Convolutional Neural Network},
  booktitle = {Proceedings of the 2015 {{IEEE International Conference}} on {{Digital Signal Processing}} ({{DSP}})},
  author = {Zou, Y. and Li, L. and Wang, Y. and Yu, J. and Li, Y. and Deng, W. J.},
  year = {2015},
  month = jul,
  pages = {1274--1278},
  doi = {10.1109/ICDSP.2015.7252086},
  abstract = {This paper studies the classification problem of the digestive organs in wireless capsule endoscopy (WCE) images based on deep convolutional neural network (DCNN) framework. Essentially, DCNN proves having powerful ability to learn layer-wise hierarchy models with huge training data, which works similar to human biological visual systems. Classifying digestive organs in WCE images intuitively means to recognize higher semantic image features. To achieve this, an effective deep CNN-based WCE classification system has been constructed (DCNN-WCE-CS). With about 1 million real WCE images, intensive experiments are conducted to evaluate its performance by setting different network parameters. Results illustrate its superior performance compared to traditional classification methods, where about 95\% classification accuracy can be achieved in average. Moreover, it is observed that the DCNN-WCE-CS is robust to the large variations of the WCE images due to the individuals and complex digestive tract circumstance, including the rotation, the luminance change of the WCE images.},
  file = {/home/henrik/Zotero/storage/7N333XQA/Zou et al. - 2015 - Classifying digestive organs in wireless capsule e.pdf;/home/henrik/Zotero/storage/X9LW93CT/7252086.html},
  keywords = {Accuracy,biological organs,biomedical optical imaging,brightness,complex digestive tract circumstance,Convolution,DCNN-WCE-CS,deep CNN-based WCE classification system,deep convolutional neural network,detecting organs,digestive organ classification problem,digestive organs classification,endoscopes,Endoscopes,feature extraction,Feature extraction,feedforward neural nets,human biological visual systems,image classification,Intestines,layer-wise hierarchy models,learning (artificial intelligence),luminance change,medical image processing,object recognition,parameter selection,semantic image feature recognition,Training,training data,wireless capsule endoscopy,wireless capsule endoscopy images,Wireless communication}
}

@article{CNNbasedSegmentation17,
  title = {{{CNN}}-Based {{Segmentation}} of {{Medical Imaging Data}}},
  author = {Kayalibay, Baris and Jensen, Grady and {van der Smagt}, Patrick},
  year = {2017},
  month = jan,
  abstract = {Convolutional neural networks have been applied to a wide variety of computer vision tasks. Recent advances in semantic segmentation have enabled their application to medical image segmentation. While most CNNs use two-dimensional kernels, recent CNN-based publications on medical image segmentation featured three-dimensional kernels, allowing full access to the three-dimensional structure of medical images. Though closely related to semantic segmentation, medical image segmentation includes specific challenges that need to be addressed, such as the scarcity of labelled data, the high class imbalance found in the ground truth and the high memory demand of three-dimensional images. In this work, a CNN-based method with three-dimensional filters is demonstrated and applied to hand and brain MRI. Two modifications to an existing CNN architecture are discussed, along with methods on addressing the aforementioned challenges. While most of the existing literature on medical image segmentation focuses on soft tissue and the major organs, this work is validated on data both from the central nervous system as well as the bones of the hand.},
  archivePrefix = {arXiv},
  eprint = {1701.03056},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/TPRA2E2Y/Kayalibay et al. - 2017 - CNN-based Segmentation of Medical Imaging Data.pdf;/home/henrik/Zotero/storage/X8T859BV/1701.html},
  journal = {arXiv:1701.03056 [cs]},
  keywords = {Betina,Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{ColorectalCancer10,
  title = {Colorectal {{Cancer}}: {{National}} and {{International Perspective}} on the {{Burden}} of {{Disease}} and {{Public Health Impact}}},
  shorttitle = {Colorectal {{Cancer}}},
  author = {Gellad, Ziad F. and Provenzale, Dawn},
  year = {2010},
  month = may,
  volume = {138},
  pages = {2177--2190},
  issn = {00165085},
  doi = {10.1053/j.gastro.2010.01.056},
  file = {/home/henrik/Zotero/storage/SR6Z7LRC/Gellad and Provenzale - 2010 - Colorectal Cancer National and International Pers.pdf},
  journal = {Gastroenterology},
  language = {en},
  number = {6}
}

@article{CompetitiveNeural13,
  title = {A {{Competitive Neural Network}} for {{Multiple Object Tracking}} in {{Video Sequence Analysis}}},
  author = {{Luque-Baena}, Rafael M. and {Ortiz-de-Lazcano-Lobato}, Juan M. and {L{\'o}pez-Rubio}, Ezequiel and Dom{\'i}nguez, Enrique and J. Palomo, Esteban},
  year = {2013},
  month = feb,
  volume = {37},
  pages = {47--67},
  issn = {1370-4621, 1573-773X},
  doi = {10.1007/s11063-012-9268-3},
  abstract = {Tracking of moving objects in real situation is a challenging research issue, due to dynamic changes in objects or background appearance, illumination, shape and occlusions. In this paper, we deal with these difficulties by incorporating an adaptive feature weighting mechanism to the proposed growing competitive neural network for multiple objects tracking. The neural network takes advantage of the most relevant object features (information provided by the proposed adaptive feature weighting mechanism) in order to estimate the trajectories of the moving objects. The feature selection mechanism is based on a genetic algorithm, and the tracking algorithm is based on a growing competitive neural network where each unit is associated to each object in the scene. The proposed methods (object tracking and feature selection mechanism) are applied to detect the trajectories of moving vehicles in roads. Experimental results show the performance of the proposed system compared to the standard Kalman filter.},
  file = {/home/henrik/Zotero/storage/HC7K3L48/Luque-Baena et al. - 2013 - A Competitive Neural Network for Multiple Object T.pdf},
  journal = {Neural Processing Letters},
  keywords = {object tracking},
  language = {en},
  number = {1}
}

@phdthesis{ComputerAidedScreening15,
  title = {Computer-{{Aided Screening}} of {{Capsule Endoscopy Videos}}},
  author = {Albisser, Zeno},
  year = {2015},
  abstract = {Colon cancer accounts for almost 10\% of all cancer cases worldwide. It is also the fourth most common cause of death from cancer globally. However, many cases of colon cancer could be prevented by early screening and removal of colon polyps - a common precursor of colon cancer. In this respect, capsule endoscopy is a non-invasive screening method with the potential to significantly reduce the cost of screening as well as the discomfort caused for the patient using traditional endoscopy examination. The financial cost of evaluating the recorded video footage, as well as the availability of specialists, currently prevents the deployment of capsule endoscopy for mass screening. With this work, we research solutions for automating the evaluation of capsule endoscopy video sequences using machine learning, image recognition and extraction of global image features. Rather than focusing on a single approach, we build tools that can be used for conducting further experiments with different methods and algorithms. We present the prototype of an integrated software solution that can be used for collecting videos from hospitals, annotating videos, tracking objects in video sequences, build- ing training and testing datasets, training classifiers and eventually, testing and evaluating the generated classifiers. We evaluate our software by training classifiers that are based on three different image recognition approaches. We also test the generated classifiers with different datasets and thereby evaluate the different approaches for their feasibility of being used to recognize colon polyps. Our main conclusion is that state of the art image recognition methods, such as the use of Haar- features or Histogram of oriented Gradients based detectors, are not suitable for detecting lesions in the intestine because of the enormous variety of possible appearances and orientations of such lesions. Global image features such as Joint Composite Descriptor on the other hand, lead to very promising results. Performing leave-one-out-cross-validation with all 20 videos of the ASU-Mayo Clinic polyp database, our system achieves a weighted average precision of 93.9\% and a weighted average recall of 98.5\%.},
  file = {/home/henrik/Zotero/storage/9W6IJWWT/Albisser - 2015 - Computer-Aided Screening of Capsule Endoscopy Vide.pdf;/home/henrik/Zotero/storage/2MKQ2Z9T/47642.html},
  language = {eng},
  school = {University of Oslo}
}

@article{CornerNetDetecting19,
  title = {{{CornerNet}}: {{Detecting Objects}} as {{Paired Keypoints}}},
  shorttitle = {{{CornerNet}}},
  author = {Law, Hei and Deng, Jia},
  year = {2019},
  month = mar,
  abstract = {We propose CornerNet, a new approach to object detection where we detect an object bounding box as a pair of keypoints, the top-left corner and the bottom-right corner, using a single convolution neural network. By detecting objects as paired keypoints, we eliminate the need for designing a set of anchor boxes commonly used in prior single-stage detectors. In addition to our novel formulation, we introduce corner pooling, a new type of pooling layer that helps the network better localize corners. Experiments show that CornerNet achieves a 42.2\% AP on MS COCO, outperforming all existing one-stage detectors.},
  archivePrefix = {arXiv},
  eprint = {1808.01244},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/F53IPUZ6/Law and Deng - 2019 - CornerNet Detecting Objects as Paired Keypoints.pdf;/home/henrik/Zotero/storage/8KNWZKHW/1808.html},
  journal = {arXiv:1808.01244 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{DeepConvolutional13,
  title = {Deep {{Inside Convolutional Networks}}: {{Visualising Image Classification Models}} and {{Saliency Maps}}},
  shorttitle = {Deep {{Inside Convolutional Networks}}},
  author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2013},
  month = dec,
  abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
  archivePrefix = {arXiv},
  eprint = {1312.6034},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/DSPZ2LDD/Simonyan et al. - 2013 - Deep Inside Convolutional Networks Visualising Im.pdf;/home/henrik/Zotero/storage/628HFNB7/1312.html},
  journal = {arXiv:1312.6034 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{DeepConvolutional16,
  title = {A Deep Convolutional Neural Network for Bleeding Detection in {{Wireless Capsule Endoscopy}} Images},
  booktitle = {Proceedings of the 2016 38th {{Annual International Conference}} of the {{IEEE Engineering}} in {{Medicine}} and {{Biology Society}} ({{EMBC}})},
  author = {Jia, X. and Meng, M. Q.-},
  year = {2016},
  month = aug,
  pages = {639--642},
  doi = {10.1109/EMBC.2016.7590783},
  abstract = {Wireless Capsule Endoscopy (WCE) is a standard non-invasive modality for small bowel examination. Recently, the development of computer-aided diagnosis (CAD) systems for gastrointestinal (GI) bleeding detection in WCE image videos has become an active research area with the goal of relieving the workload of physicians. Existing methods based primarily on handcrafted features usually give insufficient accuracy for bleeding detection, due to their limited capability of feature representation. In this paper, we present a new automatic bleeding detection strategy based on a deep convolutional neural network and evaluate our method on an expanded dataset of 10,000 WCE images. Experimental results with an increase of around 2 percentage points in the Fi score demonstrate that our method outperforms the state-of-the-art approaches in WCE bleeding detection. The achieved Fi score is of up to 0.9955.},
  file = {/home/henrik/Zotero/storage/AL7M2D2G/Jia and Meng - 2016 - A deep convolutional neural network for bleeding d.pdf;/home/henrik/Zotero/storage/GK7TEZ73/7590783.html},
  keywords = {automatic bleeding detection strategy,biomedical optical imaging,CAD,Capsule Endoscopy,computer-aided diagnosis systems,deep convolutional neural network,Diagnosis; Computer-Assisted,endoscopes,Endoscopes,Feature extraction,feature representation,Fi score,gastrointestinal bleeding detection,Gastrointestinal Hemorrhage,Hemorrhaging,Humans,medical disorders,medical image processing,neural nets,Neural Networks (Computer),small bowel examination,standard noninvasive modality,Support vector machines,Training,Videos,WCE bleeding detection,WCE image videos,wireless capsule endoscopy,Wireless communication}
}

@article{DeepEndoVO18,
  title = {Deep {{EndoVO}}: {{A}} Recurrent Convolutional Neural Network ({{RCNN}}) Based Visual Odometry Approach for Endoscopic Capsule Robots},
  shorttitle = {Deep {{EndoVO}}},
  author = {Turan, Mehmet and Almalioglu, Yasin and Araujo, Helder and Konukoglu, Ender and Sitti, Metin},
  year = {2018},
  month = jan,
  volume = {275},
  pages = {1861--1870},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2017.10.014},
  abstract = {Ingestible wireless capsule endoscopy is an emerging minimally invasive diagnostic technology for inspection of the GI tract and diagnosis of a wide range of diseases and pathologies. Medical device companies and many research groups have recently made substantial progresses in converting passive capsule endoscopes to active capsule robots, enabling more accurate, precise, and intuitive detection of the location and size of the diseased areas. Since a reliable real time pose estimation functionality is crucial for actively controlled endoscopic capsule robots, in this study, we propose a monocular visual odometry (VO) method for endoscopic capsule robot operations. Our method lies on the application of the deep recurrent convolutional neural networks (RCNNs) for the visual odometry task, where convolutional neural networks (CNNs) and recurrent neural networks (RNNs) are used for the feature extraction and inference of dynamics across the frames, respectively. Detailed analyses and evaluations made on a real pig stomach dataset proves that our system achieves high translational and rotational accuracies for different types of endoscopic capsule robot trajectories.},
  file = {/home/henrik/Zotero/storage/IW7GIJDZ/Turan et al. - 2018 - Deep EndoVO A recurrent convolutional neural netw.pdf;/home/henrik/Zotero/storage/HV6QI5IW/S092523121731665X.html},
  journal = {Neurocomputing},
  keywords = {CNN,Endoscopic capsule robot,Localization,LSTM,RCNN,Sequential deep learning,Visual odometry}
}

@article{DeepLearning17,
  title = {Deep Learning for Polyp Recognition in Wireless Capsule Endoscopy Images},
  author = {Yuan, Yixuan and Meng, Max Q.-H.},
  year = {2017},
  month = apr,
  volume = {44},
  pages = {1379--1389},
  issn = {00942405},
  doi = {10.1002/mp.12147},
  abstract = {Purpose: Wireless capsule endoscopy (WCE) enables physicians to examine the digestive tract without any surgical operations, at the cost of a large volume of images to be analyzed. In the computer-aided diagnosis of WCE images, the main challenge arises from the difficulty of robust characterization of images. This study aims to provide discriminative description of WCE images and assist physicians to recognize polyp images automatically.
Methods: We propose a novel deep feature learning method, named stacked sparse autoencoder with image manifold constraint (SSAEIM), to recognize polyps in the WCE images. Our SSAEIM differs from the traditional sparse autoencoder (SAE) by introducing an image manifold constraint, which is constructed by a nearest neighbor graph and represents intrinsic structures of images. The image manifold constraint enforces that images within the same category share similar learned features and images in different categories should be kept far away. Thus, the learned features preserve large intervariances and small intravariances among images.
Results: The average overall recognition accuracy (ORA) of our method for WCE images is 98.00\%. The accuracies for polyps, bubbles, turbid images, and clear images are 98.00\%, 99.50\%, 99.00\%, and 95.50\%, respectively. Moreover, the comparison results show that our SSAEIM outperforms existing polyp recognition methods with relative higher ORA.
Conclusion: The comprehensive results have demonstrated that the proposed SSAEIM can provide descriptive characterization for WCE images and recognize polyps in a WCE video accurately. This method could be further utilized in the clinical trials to help physicians from the tedious image reading work. \textcopyright{} 2017 American Association of Physicists in Medicine [https://doi.org/10.1002/mp.12147]},
  file = {/home/henrik/Zotero/storage/SAXC9CNE/Yuan and Meng - 2017 - Deep learning for polyp recognition in wireless ca.pdf},
  journal = {Medical Physics},
  language = {en},
  number = {4}
}

@article{DeepLearning19,
  title = {Deep {{Learning}} for {{Fall Detection}}: {{Three}}-{{Dimensional CNN Combined With LSTM}} on {{Video Kinematic Data}}},
  shorttitle = {Deep {{Learning}} for {{Fall Detection}}},
  author = {Lu, Na and Wu, Yidan and Feng, Li and Song, Jinbo},
  year = {2019},
  month = jan,
  volume = {23},
  pages = {314--323},
  issn = {2168-2194, 2168-2208},
  doi = {10.1109/JBHI.2018.2808281},
  abstract = {Fall detection is an important public healthcare problem. Timely detection could enable instant delivery of medical service to the injured. A popular nonintrusive solution for fall detection is based on videos obtained through ambient camera, and the corresponding methods usually require a large dataset to train a classifier and are inclined to be influenced by the image quality. However, it is hard to collect fall data and instead simulated falls are recorded to construct the training dataset, which is restricted to limited quantity. To address these problems, a three-dimensional convolutional neural network (3-D CNN) based method for fall detection is developed, which only uses video kinematic data to train an automatic feature extractor and could circumvent the requirement for large fall dataset of deep learning solution. 2-D CNN could only encode spatial information, and the employed 3-D convolution could extract motion feature from temporal sequence, which is important for fall detection. To further locate the region of interest in each frame, a long short-term memory (LSTM) based spatial visual attention scheme is incorporated. Sports dataset Sports-1 M with no fall examples is employed to train the 3-D CNN, which is then combined with LSTM to train a classifier with fall dataset. Experiments have verified the proposed scheme on fall detection benchmark with high accuracy as 100\%. Superior performance has also been obtained on other activity databases.},
  file = {/home/henrik/Zotero/storage/QBM7SJRA/Lu et al. - 2019 - Deep Learning for Fall Detection Three-Dimensiona.pdf;/home/henrik/Zotero/storage/I2F5YGT5/8295206.html},
  journal = {IEEE Journal of Biomedical and Health Informatics},
  keywords = {2D CNN,3D CNN based method,3D convolution,Activity recognition,ambient camera,automatic feature extractor,Convolution,convolutional neural network,Convolutional neural networks,Data mining,deep learning,fall data collection,fall dataset,fall detection,fall detection benchmark,feature extraction,Feature extraction,health care,image classification,image motion analysis,image quality,image representation,image sequences,learning (artificial intelligence),long short-term memory based spatial visual attention scheme,LSTM,Machine learning,medical image processing,medical service,motion feature extraction,object detection,public healthcare problem,recurrent neural nets,sport dataset,Sports-1 M,temporal sequence,three-dimensional CNN,three-dimensional convolutional neural network,Three-dimensional displays,Two dimensional displays,video cameras,video kinematic data,video signal processing,visual attention},
  number = {1}
}

@article{DeepReinforcement17,
  title = {Deep {{Reinforcement Learning}} for {{Visual Object Tracking}} in {{Videos}}},
  author = {Zhang, Da and Maei, Hamid and Wang, Xin and Wang, Yuan-Fang},
  year = {2017},
  month = jan,
  abstract = {In this paper we introduce a fully end-to-end approach for visual tracking in videos that learns to predict the bounding box locations of a target object at every frame. An important insight is that the tracking problem can be considered as a sequential decision-making process and historical semantics encode highly relevant information for future decisions. Based on this intuition, we formulate our model as a recurrent convolutional neural network agent that interacts with a video overtime, and our model can be trained with reinforcement learning (RL) algorithms to learn good tracking policies that pay attention to continuous, inter-frame correlation and maximize tracking performance in the long run. The proposed tracking algorithm achieves state-of-the-art performance in an existing tracking benchmark and operates at frame-rates faster than real-time. To the best of our knowledge, our tracker is the first neural-network tracker that combines convolutional and recurrent networks with RL algorithms.},
  archivePrefix = {arXiv},
  eprint = {1701.08936},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/7VJDK8ZD/Zhang et al. - 2017 - Deep Reinforcement Learning for Visual Object Trac.pdf;/home/henrik/Zotero/storage/NGVQ2SIY/1701.html},
  journal = {arXiv:1701.08936 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{DeepResidual15,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/X2BBHVY2/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/henrik/Zotero/storage/3HIXBC68/1512.html},
  journal = {arXiv:1512.03385 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@misc{DemystifyingConvolutional18,
  title = {Demystifying {{Convolutional Neural Networks}}},
  author = {Zerium, Aegeus},
  year = {2018},
  month = sep,
  abstract = {An Intuitive Explanation of Convolutional Neural Networks.},
  file = {/home/henrik/Zotero/storage/BFDGM3IN/demystifying-convolutional-neural-networks-ca17bdc75559.html},
  howpublished = {https://medium.com/@eternalzer0dayx/demystifying-convolutional-neural-networks-ca17bdc75559},
  journal = {Medium},
  language = {en}
}

@article{DistillingKnowledge15,
  title = {Distilling the {{Knowledge}} in a {{Neural Network}}},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  month = mar,
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archivePrefix = {arXiv},
  eprint = {1503.02531},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/Y5IYLRJ5/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf;/home/henrik/Zotero/storage/RI7Y29HZ/1503.html},
  journal = {arXiv:1503.02531 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{DynamicRouting17,
  title = {Dynamic {{Routing Between Capsules}}},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
  year = {2017},
  month = oct,
  abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
  archivePrefix = {arXiv},
  eprint = {1710.09829},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/VAK5X6YX/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf;/home/henrik/Zotero/storage/756F7S8K/1710.html},
  journal = {arXiv:1710.09829 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{EfficientNetRethinking19,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves state-of-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet. Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flowers (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters. Source code is at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  archivePrefix = {arXiv},
  eprint = {1905.11946},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/L46UWD3M/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf;/home/henrik/Zotero/storage/SPSEGYK4/1905.html},
  journal = {arXiv:1905.11946 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{ExpertDriven15,
  title = {Expert Driven Semi-Supervised Elucidation Tool for Medical Endoscopic Videos},
  booktitle = {Proceedings of the 6th {{ACM Multimedia Systems Conference}}},
  author = {Albisser, Zeno and Riegler, Michael and Halvorsen, P{\aa}l and Zhou, Jiang and Griwodz, Carsten and Balasingham, Ilangko and Gurrin, Cathal},
  year = {2015},
  pages = {73--76},
  publisher = {{ACM Press}},
  address = {{Portland, Oregon}},
  doi = {10.1145/2713168.2713184},
  abstract = {In this paper, we present a novel application for elucidating all kind of videos that require expert knowledge, e.g., sport videos, medical videos etc., focusing on endoscopic surgery and video capsule endoscopy. In the medical domain, the knowledge of experts for tagging and interpretation of videos is of high value. As a result of the stressful working environment of medical doctors, they often simply do not have time for extensive annotations. We therefore present a semisupervised method to gather the annotations in a very easy and time saving way for the experts and we show how this information can be used later on.},
  file = {/home/henrik/Zotero/storage/3V2GE6II/Albisser et al. - 2015 - Expert driven semi-supervised elucidation tool for.pdf},
  isbn = {978-1-4503-3351-1}
}

@inproceedings{FullyConvolutional15,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431--3440},
  file = {/home/henrik/Zotero/storage/DEGCVEFG/Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf;/home/henrik/Zotero/storage/DIC8VB4W/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html}
}

@article{GenerativeImage19,
  title = {Generative {{Image Translation}} for {{Data Augmentation}} in {{Colorectal Histopathology Images}}},
  author = {Wei, Jerry and Suriawinata, Arief and Vaickus, Louis and Ren, Bing and Liu, Xiaoying and Wei, Jason and Hassanpour, Saeed},
  year = {2019},
  pages = {17},
  abstract = {We present an image translation approach to generate augmented data for mitigating data imbalances in a dataset of histopathology images of colorectal polyps, adenomatous tumors that can lead to colorectal cancer if left untreated. By applying cycle-consistent generative adversarial networks (CycleGANs) to a source domain of normal colonic mucosa images, we generate synthetic colorectal polyp images that belong to diagnostically less common polyp classes. Generated images maintain the general structure of their source image but exhibit adenomatous features that can be enhanced with our proposed filtration module, called Path-Rank-Filter. We evaluate the quality of generated images through Turing tests with four gastrointestinal pathologists, finding that at least two of the four pathologists could not identify generated images at a statistically significant level. Finally, we demonstrate that using CycleGAN-generated images to augment training data improves the AUC of a convolutional neural network for detecting sessile serrated adenomas by over 10\%, suggesting that our approach might warrant further research for other histopathology image classification tasks.},
  file = {/home/henrik/Zotero/storage/7WSJKNCF/Wei et al. - Generative Image Translation for Data Augmentation.pdf},
  language = {en}
}

@misc{GettingStarted19,
  title = {Getting Started with {{TensorFlow}} 2.0},
  author = {Rawlani, Himanshu},
  year = {2019},
  month = oct,
  abstract = {A practitioner's guide to building and deploying an image classifier in TensorFlow 2.0},
  file = {/home/henrik/Zotero/storage/PY2UANVH/getting-started-with-tensorflow-2-0-faf5428febae.html},
  howpublished = {https://medium.com/@himanshurawlani/getting-started-with-tensorflow-2-0-faf5428febae},
  journal = {Medium},
  language = {en}
}

@article{HereditaryFamilial10,
  title = {Hereditary and {{Familial Colon Cancer}}},
  author = {Jasperson, Kory W. and Tuohy, Th{\'e}r{\`e}se M. and Neklason, Deborah W. and Burt, Randall W.},
  year = {2010},
  month = may,
  volume = {138},
  pages = {2044--2058},
  issn = {00165085},
  doi = {10.1053/j.gastro.2010.01.054},
  file = {/home/henrik/Zotero/storage/D6FU3Y83/Jasperson et al. - 2010 - Hereditary and Familial Colon Cancer.pdf},
  journal = {Gastroenterology},
  language = {en},
  number = {6}
}

@techreport{HyperKvasirComprehensive19,
  title = {Hyper-{{Kvasir}}: {{A Comprehensive Multi}}-{{Class Image}} and {{Video Dataset}} for {{Gastrointestinal Endoscopy}}},
  shorttitle = {Hyper-{{Kvasir}}},
  author = {Borgli, Hanna and Thambawita, Vajira and Smedsrud, Pia H. and Hicks, Steven and Jha, Debesh and Eskeland, Sigrun L. and Randel, Kristin Ranheim and Pogorelov, Konstantin and Lux, Mathias and Nguyen, Duc Tien Dang and Johansen, Dag and Griwodz, Carsten and Stensland, Haakon K. and Ceja, Enrique Garcia and Schmidt, Peter T. and Hammer, Hugo L. and Riegler, Michael and Halvorsen, Paal and {de Lange}, Thomas},
  year = {2019},
  month = dec,
  institution = {{Open Science Framework}},
  doi = {10.31219/osf.io/mkzcq},
  abstract = {Artificial intelligence is currently a hot topic in medicine. The fact that medical data is often sparse and hard to obtain due to legal restrictions and lack of medical personnel to perform the cumbersome and tedious labeling of the data leads to limitations for what would be possible to achieve with automatic analysis. In this respect, this article presents Hyper-Kvasir which is the largest image and video dataset of the gastrointestinal tract available today. The data is collected during real gastro- and colonoscopy examinations at B{\ae}rum Hospital in Norway and partly labeled by experienced gastrointestinal endoscopists. The dataset contains 110,079 images and 373 videos and represents anatomical landmarks and pathological and normal findings. The total number of images and video frames together is around 1,17 million. Initial experiments demonstrate the potential benefits of artificial intelligence-based computer assisted diagnosis systems. The Hyper-Kvasir dataset can play an important role in developing better algorithms and computer assisted examination systems not just for gastro- and colonoscopy, butpossible also other fields in medicine.},
  type = {Preprint}
}

@phdthesis{HyperparameterOptimization18,
  title = {Hyperparameter Optimization Using {{Bayesian}} Optimization on Transfer Learning for Medical Image Classification},
  author = {Borgli, Rune Johan},
  year = {2018},
  abstract = {The field of medicine has a history of adopting new technology. Video equipment and sensors are used to visualize areas of interest in the human allowing for doctors to make diagnoses based on imagery observations. However, the detection rate of the doctors towards diseases and abnormalities is heavily dependent on the experience and state of mind of the doctor doing the examination. Computer-aided detection systems are systems designed to aid the doctor in improving the detection rate, and they are using or experimenting with machine learning. Deep convolutional neural networks, a type of machine learning, are shown to be highly efficient at image detection, classification, and analysis. However, these networks require large datasets to train properly. Transfer learning is a training technique where we use a pre-trained machine learning model and transfer some of the attained knowledge from other application domains over to a new model. This way, we can use small datasets and train a model in much shorter time. In this respect, transfer learning works fine but has many configurations called hyperparameters which are often not optimized. Our work aims to address the lack of automatic hyperparameter optimization for transfer learning by experiments utilizing a known hyperparameter optimization method and creating a system for running those experiments. First, we decided to focus on the field of gastroenterology by utilizing two publicly available datasets showing images from the gastrointestinal tract. Next, we used a specific transfer learning method and chose hyperparameters suitable for automatic optimization. The optimization method we chose was Bayesian optimization because of its reputation for being one of the best methods for hyperparameter optimization. However, Bayesian optimization has hyperparameters of its own, and there are also different versions of Bayesian optimization. We chose to limit the thesis, so we use standard Bayesian optimization with standard parameters. We created a system for running automatic experiments of three different hyperparameter optimization strategies. With the system, we ran a set of experiments for each dataset. Between the strategies, one was successful in achieving a high validation accuracy, while the others were considered failures. Compared to baselines, our best models was around 10\% better. With these experiments, we demonstrated that automatic hyperparameter optimization is an effective strategy for increasing performance in transfer learning and that the best hyperparameters are nontrivial to select manually.},
  file = {/home/henrik/Zotero/storage/F3CTSEMU/Borgli - 2018 - Hyperparameter optimization using Bayesian optimiz.pdf;/home/henrik/Zotero/storage/DGPRG2IR/64146.html},
  language = {eng}
}

@inproceedings{IdentificationUlcers09,
  title = {Identification of Ulcers in {{Wireless Capsule Endoscopy}} Videos},
  booktitle = {Proceedings of the 2009 {{IEEE International Symposium}} on {{Biomedical Imaging}}: {{From Nano}} to {{Macro}}},
  author = {Karargyris, A. and Bourbakis, N.},
  year = {2009},
  month = jun,
  pages = {554--557},
  doi = {10.1109/ISBI.2009.5193107},
  abstract = {Wireless Capsule Endoscopy (WCE) is a non invasive procedure which is used to view the lower gastrointestinal tract. Physicians can detect diseases such as bleeding, Crohn's disease, peptic ulcers, and colon cancer. In this paper a methodology is presented to identify peptic ulcers in the small intestine automatically. It first performs color transformation into the HSV color space; it utilizes log Gabor filters to find meaningful regions. A segmentation scheme is used to extract color information of these meaningful regions in the original RGB color space. Additionally, texture information is extracted and together with color values are fed into an artificial neural network for classification. Illustrative results from the methodology are also given in this paper.},
  file = {/home/henrik/Zotero/storage/6VBSP4UY/Karargyris and Bourbakis - 2009 - Identification of ulcers in Wireless Capsule Endos.pdf;/home/henrik/Zotero/storage/BW92FMMW/5193107.html},
  keywords = {artificial neural network,biological organs,bleeding,Cancer detection,Colon,colon cancer,color information extraction,Crohn's disease,Data mining,diseases,Diseases,endoscopes,Endoscopes,feature extraction,fuzzy least squares support vector machines,fuzzy region segmentation,Gabor filters,gastrointestinal tract,Gastrointestinal tract,Hemorrhaging,HSV color space,image classification,image segmentation,Intestines,log Gabor filters,log-Gabor filters,medical image processing,neural nets,noninvasive procedure,peptic ulcers,RGB color space,segmentation scheme,small intestine,texture,texture information,ulcers,video signal processing,Videos,Wireless capsule Endoscopy Imaging,wireless capsule endoscopy videos}
}

@incollection{ImageNetClassification12,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/henrik/Zotero/storage/8C5IIC5A/Krizhevsky et al. - 2012 - ImageNet Classification with Deep Convolutional Ne.pdf;/home/henrik/Zotero/storage/ID4BRJ6C/4824-imagenet-classification-with-deep-convolutional-neural-networks.html}
}

@article{ImprovedRegularization17,
  title = {Improved {{Regularization}} of {{Convolutional Neural Networks}} with {{Cutout}}},
  author = {DeVries, Terrance and Taylor, Graham W.},
  year = {2017},
  month = nov,
  abstract = {Convolutional neural networks are capable of learning powerful representational spaces, which are necessary for tackling complex learning tasks. However, due to the model capacity required to capture such representations, they are often susceptible to overfitting and therefore require proper regularization in order to generalize well. In this paper, we show that the simple regularization technique of randomly masking out square regions of input during training, which we call cutout, can be used to improve the robustness and overall performance of convolutional neural networks. Not only is this method extremely easy to implement, but we also demonstrate that it can be used in conjunction with existing forms of data augmentation and other regularizers to further improve model performance. We evaluate this method by applying it to current state-of-the-art architectures on the CIFAR-10, CIFAR-100, and SVHN datasets, yielding new state-of-the-art results of 2.56\%, 15.20\%, and 1.30\% test error respectively. Code is available at https://github.com/uoguelph-mlrg/Cutout},
  archivePrefix = {arXiv},
  eprint = {1708.04552},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/8ML3XBAQ/DeVries and Taylor - 2017 - Improved Regularization of Convolutional Neural Ne.pdf;/home/henrik/Zotero/storage/GXS6A9YB/1708.html},
  journal = {arXiv:1708.04552 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@inproceedings{Inceptionv4InceptionResNet17,
  title = {Inception-v4, {{Inception}}-{{ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  booktitle = {Thirty-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  year = {2017},
  month = feb,
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {/home/henrik/Zotero/storage/77HQJ9QB/Szegedy et al. - 2017 - Inception-v4, Inception-ResNet and the Impact of R.pdf;/home/henrik/Zotero/storage/PTK4TJCC/14806.html},
  language = {en}
}

@article{JPEGStill92,
  title = {The {{JPEG}} Still Picture Compression Standard},
  author = {Wallace, G.K.},
  year = {1992},
  month = feb,
  volume = {38},
  pages = {xviii-xxxiv},
  issn = {1558-4127},
  doi = {10.1109/30.125072},
  abstract = {A joint ISO/CCITT committee known as JPEG (Joint Photographic Experts Group) has been working to establish the first international compression standard for continuous-tone still images, both grayscale and color. JPEG's proposed standard aims to be generic, to support a wide variety of applications for continuous-tone images. To meet the differing needs of many applications, the JPEG standard includes two basic compression methods, each with various modes of operation. A DCT (discrete cosine transform)-based method is specified for 'lossy' compression, and a predictive method for 'lossless' compression. JPEG features a simple lossy technique known as the Baseline method, a subset of the other DCT-based modes of operation. The Baseline method has been by far the most widely implemented JPEG method to date, and is sufficient in its own right for a large number of applications. The author provides an overview of the JPEG standard, and focuses in detail on the Baseline method.{$<>$}},
  journal = {IEEE Transactions on Consumer Electronics},
  keywords = {Baseline method,CCITT,coding,color,continuous-tone still images,Costs,data compression,DCT,Digital images,discrete cosine transform,Displays,Facsimile,Gray-scale,grayscale,Image coding,Image storage,international compression standard,ISO,ISO standards,Joint Photographic Experts Group,JPEG,lossless compression,lossy compression,picture processing,predictive method,Standards development,still picture compression standard,television standards,Transform coding,transforms,TV standard},
  number = {1}
}

@inproceedings{KVASIRMultiClass17,
  title = {{{KVASIR}}: {{A Multi}}-{{Class Image Dataset}} for {{Computer Aided Gastrointestinal Disease Detection}}},
  shorttitle = {{{KVASIR}}},
  booktitle = {Proceedings of the 8th {{ACM}} on {{Multimedia Systems Conference}} - {{MMSys}}'17},
  author = {Pogorelov, Konstantin and Schmidt, Peter Thelin and Riegler, Michael and Halvorsen, P{\aa}l and Randel, Kristin Ranheim and Griwodz, Carsten and Eskeland, Sigrun Losada and {de Lange}, Thomas and Johansen, Dag and Spampinato, Concetto and {Dang-Nguyen}, Duc-Tien and Lux, Mathias},
  year = {2017},
  pages = {164--169},
  publisher = {{ACM Press}},
  address = {{Taipei, Taiwan}},
  doi = {10.1145/3083187.3083212},
  abstract = {In this paper, we present Kvasir, a dataset containing images from inside the gastrointestinal (GI) tract. 
The collection of images are classified into three important anatomical landmarks and three clinically significant findings. In addition, it contains two categories of images related to endoscopic polyp removal. Sorting and annotation of the dataset is performed by medical doctors (experienced endoscopists).
In this respect, Kvasir is important for research on both single- and multi-disease computer aided detection. By providing it, we invite and enable multimedia researcher into the medical domain of detection and retrieval.},
  file = {/home/henrik/Zotero/storage/M9K8TM7M/Pogorelov et al. - 2017 - KVASIR A Multi-Class Image Dataset for Computer A.pdf},
  isbn = {978-1-4503-5002-0},
  language = {en}
}

@inproceedings{LesionDetection15,
  title = {Lesion Detection of Endoscopy Images Based on Convolutional Neural Network Features},
  booktitle = {Proceedings of the 2015 8th {{International Congress}} on {{Image}} and {{Signal Processing}} ({{CISP}})},
  author = {Zhu, R. and Zhang, R. and Xue, D.},
  year = {2015},
  month = oct,
  pages = {372--376},
  doi = {10.1109/CISP.2015.7407907},
  abstract = {Since gastroscopy is able to observe the interior of gastrointestinal tract directly, it has been widely used for gastrointestinal examination. But it is hard for clinicians to accurately detect gastrointestinal disease due to its great dependence on doctors experiences. Therefore, a computer-aided lesion detection system can offer great help for clinicians. In this paper, we propose a new scheme for endoscopy image lesion detection. A trainable feature extractor based on convolutional neural network (CNN) is utilized to get more generic features for endoscopy images. And features are fed to support vector machine (SVM) to enhance the generalization ability. Experiments show that the proposed scheme outperforms the previous conventional methods based on color and texture features.},
  file = {/home/henrik/Zotero/storage/9ILDKALR/Zhu et al. - 2015 - Lesion detection of endoscopy images based on conv.pdf;/home/henrik/Zotero/storage/XMXK62TU/7407907.html},
  keywords = {cancer,CNN,color features,computer-aided lesion detection system,convolutional neural network features,detect gastrointestinal disease,endoscopes,Endoscopes,endoscopy image lesion detection,feature extraction,Feature extraction,gastrointestinal examination,Gastrointestinal tract,gastrointestinal tract directly,Histograms,Image color analysis,image colour analysis,image texture,Lesions,medical image processing,neural nets,support vector machine,support vector machines,Support vector machines,SVM,texture features,trainable feature extractor}
}

@misc{MachineLearning18,
  title = {Machine {{Learning}} Is {{Fun}}!},
  author = {Geitgey, Adam},
  year = {2018},
  month = nov,
  abstract = {The world's easiest introduction to Machine Learning},
  file = {/home/henrik/Zotero/storage/J4ZYW6CC/machine-learning-is-fun-80ea3ec3c471.html},
  howpublished = {https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471},
  journal = {Medium},
  language = {en}
}

@phdthesis{MachineLearning19,
  title = {A {{Machine Learning Approach To Improve Consistency In User}}-{{Driven Medical Image Analysis}}},
  author = {Eriksen, Edvarda},
  year = {2019},
  abstract = {The work we present in this thesis stems out from the need of standardisation of training image analysts. We will particularly focus on the training
of image analysis using a growingly popular medical imaging technique,
T1 mapping MRI. Its usefulness relies on the ability to detect abnormalities
in the cardiac tissue myocardial structure due to a range of pathologies in a
non invasive and mostly contrast-agent free manner. T1 mapping is not yet
a routinely used clinical imaging modality, but as more evidence of its potential is published, it is foreseen by experts to soon become a fundamental
clinical tool.},
  school = {University of Oslo}
}

@phdthesis{MedicalMultimedia17,
  title = {A {{Medical Multimedia Real}}-{{Time Polyp Detection System}} Using {{Low Computational Resources}}},
  author = {Khan, Asif Qayyum},
  year = {2017},
  abstract = {In this research, the focal point has been on real-time polyp detection on computers
with low computational resources with the help of open source libraries such as LIRE
Lucene, OpenCV.},
  file = {/home/henrik/Zotero/storage/5RYV6QNB/Khan - 2017 - A Medical Multimedia Real-Time Polyp Detection Sys.pdf},
  school = {University of Oslo},
  type = {Master's {{Thesis}}}
}

@misc{MedicalPhysics,
  title = {Medical {{Physics}} - {{Endoscopes}}},
  file = {/home/henrik/Zotero/storage/YTY6DUKB/index.html},
  howpublished = {http://www.genesis.net.au/\textasciitilde{}ajs/projects/medical\_physics/endoscopes/index.html}
}

@phdthesis{MimirAutomatic18,
  title = {{Mimir: An Automatic Reporting and Reasoning System for Screening of the Gastrointestinal Tract Using Deep Neural Networks}},
  shorttitle = {{Mimir}},
  author = {Hicks, Steven Alexander},
  year = {2018},
  abstract = {Data is arguably one of the most valuable resources available today. More
than ever, data is collected on such a large scale that we do not have
the capacity to process it efficiently. In healthcare alone, there is an
estimated 162 exabyte of data throughout the world, which is growing at
the speed of approximately 2.5 exabytes per year [18]. Medical data in
and of itself can be used for many things, such as patient follow-ups or
recommendations. Nevertheless, to enable the use of this information to
its fullest potential, we need sophisticated data analysis methods such as
statistics or machine learning. Machine learning is a field where machines
learn from data without explicitly being programmed. This process is
often applied through supervised learning (machines learning from labeled
data), unsupervised learning (machines learning from unlabeled data), or
semi-supervised (machines learning from a combination of labeled and
unlabeled data). Over the past few years, this field has been dominated
by a growing class of algorithms known as deep learning. Inspired by
the neurological connections in the animal brain, deep learning has made
immense strides in the production of state-of-the-art results within many
areas of data analytics [4]. Nowadays, deep learning based methods have
become a popular topic within the medical field as well [7]. This has
brought up some specific challenges which may make the application of
these methods difficult, such as the lack of data or poor understanding
of their internal workings. The latter issue, namely that deep learning is
something of a ``black box'', is one of the biggest hurdles since it hinders
the application of deep learning from being used in hospitals due to lack
of trust and understanding. For this reason, we developed a medical
reporting system, which focuses on transparency and understanding of
its internal processes. In this thesis, we present this system and show
how it may aid us in the development and understanding of deep neural
networks.},
  file = {/home/henrik/Zotero/storage/M6ILEV7V/Hicks - 2018 - Mimir An Automatic Reporting and Reasoning System.pdf;/home/henrik/Zotero/storage/69JSKP4S/65179.html},
  language = {nob}
}

@article{NewMethod54,
  title = {A {{New Method}} of Transporting {{Optical Images}} without {{Aberrations}}},
  author = {Van Heel, A. C. S.},
  year = {1954},
  month = jan,
  volume = {173},
  pages = {39--39},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/173039a0},
  file = {/home/henrik/Zotero/storage/R3TXS4M8/Van Heel - 1954 - A New Method of transporting Optical Images withou.pdf},
  journal = {Nature},
  language = {en},
  number = {4392}
}

@article{NotesBackpropagation16,
  title = {Notes on Backpropagation},
  author = {Sadowski, Peter},
  year = {2016},
  file = {/home/henrik/Zotero/storage/SC592RJV/Sadowski - 2016 - Notes on backpropagation.pdf}
}

@article{ObjectTracking15,
  title = {Object {{Tracking Benchmark}}},
  author = {Wu, Y. and Lim, J. and Yang, M.},
  year = {2015},
  month = sep,
  volume = {37},
  pages = {1834--1848},
  issn = {0162-8828},
  doi = {10.1109/TPAMI.2014.2388226},
  abstract = {Object tracking has been one of the most important and active research areas in the field of computer vision. A large number of tracking algorithms have been proposed in recent years with demonstrated success. However, the set of sequences used for evaluation is often not sufficient or is sometimes biased for certain types of algorithms. Many datasets do not have common ground-truth object positions or extents, and this makes comparisons among the reported quantitative results difficult. In addition, the initial conditions or parameters of the evaluated tracking algorithms are not the same, and thus, the quantitative results reported in literature are incomparable or sometimes contradictory. To address these issues, we carry out an extensive evaluation of the state-of-the-art online object-tracking algorithms with various evaluation criteria to understand how these methods perform within the same framework. In this work, we first construct a large dataset with ground-truth object positions and extents for tracking and introduce the sequence attributes for the performance analysis. Second, we integrate most of the publicly available trackers into one code library with uniform input and output formats to facilitate large-scale performance evaluation. Third, we extensively evaluate the performance of 31 algorithms on 100 sequences with different initialization settings. By analyzing the quantitative results, we identify effective approaches for robust tracking and provide potential future research directions in this field.},
  file = {/home/henrik/Zotero/storage/QI7DVTQK/Wu et al. - 2015 - Object Tracking Benchmark.pdf;/home/henrik/Zotero/storage/SA4AYB9C/7001050.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {Algorithm design and analysis,benchmark dataset,Histograms,object tracking,Object tracking,performance evaluation,Performance evaluation,Robustness,Target tracking},
  number = {9}
}

@misc{PillCamCamera,
  title = {{{PillCam}} - {{A Camera Pill System}} for {{Automatic Screening}} of the {{Digestive System}} - {{Institutt}} for Informatikk.},
  abstract = {In this project, we aim to design and develop a system for analysing video from a camera pill. The pill is~swallowed and records video of the~digestive system - the goal is to be able to automatically detect cancer in the colon.},
  file = {/home/henrik/Zotero/storage/JAQ2XTAL/SRL-media-pill-cam.html},
  howpublished = {https://www.mn.uio.no/ifi/studier/masteroppgaver/nd/SRL-media-pill-cam.html}
}

@phdthesis{PolypDetection17,
  title = {Polyp {{Detection}} Using {{Neural Networks}} - {{Data Enhancement}} and {{Training Optimization}}},
  author = {Jensen, Rune},
  year = {2017},
  abstract = {Colorectal cancer is the third most common type of cancer diagnosed for men and the second most for women. Today's main methods of examination are expensive, time consuming and intrusive for the patient. Recent technologies, such as CAD and ACD, aims to increase automation in the screening and examination processes. CAD could aid medical professionals during examinations by providing a second opinion, while ACD could be used to screen entire populations, and thus relieving pressure on the health care system. In recent years, neural networks have gained traction among researchers in topics regarding recognition, and we believe it can be utilized in these automated systems. In this thesis, we examine the performance of neural networks for polyp detection. We also explore how data enhancement affect the training and evaluation of the networks, and if it can be used to increase the polyp detection rate. Finally, we experiment with how various training techniques can be used to increase performance. We conclude that neural networks are suitable for polyp detection. We show how data enhancement and training optimization can be used to increase different aspects of the performance. We discuss what aspects are suitable for different scenarios. At the end, we also discuss how our system can be used to detect polyps per frame, per sequence and per polyp, and what the results of our system look like using the different metrics. Detection per frame can be considered a computer science viewpoint, while detection per sequence or per polyp is more of a medical field viewpoint.},
  file = {/home/henrik/Zotero/storage/DG89EWW9/Jensen - 2017 - Polyp Detection using Neural Networks - Data Enhan.pdf},
  keywords = {Artificial neural network,Biological Neural Networks,Classification,Computer Assisted Diagnosis,Computer-aided design,Endoscopy of stomach,Greater,Machine learning,Mathematical optimization,Neoplasms,Neural Network Simulation,Object detection,Open-source software,polyps,Population,TensorFlow,Tract (literature),Traction TeamPage,Urinary tract infection,Weight}
}

@phdthesis{PolypDetection17a,
  title = {Polyp {{Detection}}: {{Effect}} of {{Early}} and {{Late Feature Fusion}}},
  shorttitle = {Polyp {{Detection}}},
  author = {Asskali, Salman},
  year = {2017},
  abstract = {In this thesis, we look at a specific component of these learning
methods and how they affect performance in aiding medical systems. This
component, called feature fusion, has two widely adopted variations: early
fusion and late fusion. We seek to compare the performance of early and
late fusion for medical diagnosis problems through image datasets, and
provide some insight to data scientists on how our results can help their
decision when building a practical system.},
  file = {/home/henrik/Zotero/storage/TAPV4X24/Asskali - 2017 - Polyp Detection Effect of Early and Late Feature .pdf},
  school = {University of Oslo},
  type = {Master's {{Thesis}}}
}

@article{PricingSurgeries12,
  title = {Pricing of Surgeries for Colon Cancer},
  author = {Dor, Avi and Koroukian, Siran and Xu, Fang and Stulberg, Jonah and Delaney, Conor and Cooper, Gregory},
  year = {2012},
  volume = {118},
  pages = {5741--5748},
  issn = {1097-0142},
  doi = {10.1002/cncr.27573},
  abstract = {BACKGROUND: This study examined effects of health maintenance organization (HMO) penetration, hospital competition, and patient severity on the uptake of laparoscopic colectomy and its price relative to open surgery for colon cancer. METHODS: The MarketScan Database (data from 2002-2007) was used to identify admissions for privately insured colorectal cancer patients undergoing laparoscopic or open partial colectomy (n = 1035 and n = 6389, respectively). Patient and health plan characteristics were retrieved from these data; HMO market penetration rates and an index of hospital market concentration, the Herfindahl-Hirschman index (HHI), were derived from national databases. Logistic and logarithmic regressions were used to examine the odds of having laparoscopic colectomy, effect of covariates on colectomy prices, and the differential price of laparoscopy. RESULTS: Adoption of laparoscopy was highly sensitive to market forces, with a 10\% increase in HMO penetration leading to a 10.9\% increase in the likelihood of undergoing laparoscopic colectomy (adjusted odds ratio = 1.109; 95\% confidence interval [CI] = 1.062, 1.158) and a 10\% increase in HHI resulting in 6.6\% lower likelihood (adjusted odds ratio = 0.936; 95\% CI = 0.880, 0.996). Price models indicated that the price of laparoscopy was 7.6\% lower than that of open surgery (transformed coefficient = 0.927; 95\% CI = 0.895, 0.960). A 10\% increase in HMO penetration was associated with 1.6\% lower price (transformed coefficient = 0.985; 95\% CI = 0.977, 0.992), whereas a 10\% increase in HHI was associated with 1.6\% higher price (transformed coefficient = 1.016; 95\% CI = 1.006, 1.027; P {$<$} .001 for all comparisons). CONCLUSIONS: Laparoscopy was significantly associated with lower hospital prices. Moreover, laparoscopic surgery may result in cost savings, while market pressures contribute to its adoption. Cancer 2012. \textcopyright{} 2012 American Cancer Society.},
  file = {/home/henrik/Zotero/storage/B5XYNSIT/Dor et al. - 2012 - Pricing of surgeries for colon cancer.pdf;/home/henrik/Zotero/storage/IJYE4VB6/cncr.html},
  journal = {Cancer},
  keywords = {colon cancer,laparoscopy,medical prices,pricing,surgery},
  language = {en},
  number = {23}
}

@article{ProgressChallenges10,
  title = {Progress and {{Challenges}} in {{Colorectal Cancer Screening}} and {{Surveillance}}},
  author = {Lieberman, David},
  year = {2010},
  month = may,
  volume = {138},
  pages = {2115--2126},
  issn = {00165085},
  doi = {10.1053/j.gastro.2010.02.006},
  file = {/home/henrik/Zotero/storage/5ZJHN63L/Lieberman - 2010 - Progress and Challenges in Colorectal Cancer Scree.pdf},
  journal = {Gastroenterology},
  language = {en},
  number = {6}
}

@incollection{RealData17,
  title = {Real {{Data Augmentation}} for {{Medical Image Classification}}},
  booktitle = {Intravascular {{Imaging}} and {{Computer Assisted Stenting}}, and {{Large}}-{{Scale Annotation}} of {{Biomedical Data}} and {{Expert Label Synthesis}}},
  author = {Zhang, Chuanhai and Tavanapong, Wallapak and Wong, Johnny and {de Groen}, Piet C. and Oh, JungHwan},
  editor = {Cardoso, M. Jorge and Arbel, Tal and Lee, Su-Lin and Cheplygina, Veronika and Balocco, Simone and Mateus, Diana and Zahnd, Guillaume and {Maier-Hein}, Lena and Demirci, Stefanie and Granger, Eric and Duong, Luc and Carbonneau, Marc-Andr{\'e} and Albarqouni, Shadi and Carneiro, Gustavo},
  year = {2017},
  volume = {10552},
  pages = {67--76},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-67534-3_8},
  abstract = {Many medical image classification tasks share a common unbalanced data problem. That is images of the target classes, e.g., certain types of diseases, only appear in a very small portion of the entire dataset. Nowadays, large collections of medical images are readily available. However, it is costly and may not even be feasible for medical experts to manually comb through a huge unlabeled dataset to obtain enough representative examples of the rare classes. In this paper, we propose a new method called Unified LF\&SM to recommend most similar images for each class from a large unlabeled dataset for verification by medical experts and inclusion in the seed labeled dataset. Our real data augmentation significantly reduces expensive manual labeling time. In our experiments, Unified LF\&SM performed best, selecting a high percentage of relevant images in its recommendation and achieving the best classification accuracy. It is easily extendable to other medical image classification problems.},
  file = {/home/henrik/Zotero/storage/8RVN78RU/Zhang et al. - 2017 - Real Data Augmentation for Medical Image Classific.pdf},
  isbn = {978-3-319-67533-6 978-3-319-67534-3},
  language = {en}
}

@article{ResnetResnet16,
  title = {Resnet in {{Resnet}}: {{Generalizing Residual Architectures}}},
  shorttitle = {Resnet in {{Resnet}}},
  author = {Targ, Sasha and Almeida, Diogo and Lyman, Kevin},
  year = {2016},
  month = mar,
  abstract = {Residual networks (ResNets) have recently achieved state-of-the-art on challenging computer vision tasks. We introduce Resnet in Resnet (RiR): a deep dual-stream architecture that generalizes ResNets and standard CNNs and is easily implemented with no computational overhead. RiR consistently improves performance over ResNets, outperforms architectures with similar amounts of augmentation on CIFAR-10, and establishes a new state-of-the-art on CIFAR-100.},
  archivePrefix = {arXiv},
  eprint = {1603.08029},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/QVYQJAZT/Targ et al. - 2016 - Resnet in Resnet Generalizing Residual Architectu.pdf;/home/henrik/Zotero/storage/YWN2LB44/1603.html},
  journal = {arXiv:1603.08029 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{SelftrainingNoisy20,
  title = {Self-Training with {{Noisy Student}} Improves {{ImageNet}} Classification},
  author = {Xie, Qizhe and Luong, Minh-Thang and Hovy, Eduard and Le, Quoc V.},
  year = {2020},
  month = jan,
  abstract = {We present a simple self-training method that achieves 88.4\% top-1 accuracy on ImageNet, which is 2.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 61.0\% to 83.7\%, reduces ImageNet-C mean corruption error from 45.7 to 28.3, and reduces ImageNet-P mean flip rate from 27.8 to 12.2. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as accurate as possible. However, during the learning of the student, we inject noise such as dropout, stochastic depth and data augmentation via RandAugment to the student so that the student generalizes better than the teacher.},
  archivePrefix = {arXiv},
  eprint = {1911.04252},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/3T9DEQAR/Xie et al. - 2020 - Self-training with Noisy Student improves ImageNet.pdf;/home/henrik/Zotero/storage/TNMWI2MA/1911.html},
  journal = {arXiv:1911.04252 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ShapeShading94,
  title = {Shape from Shading Using Linear Approximation},
  author = {{Ping-Sing}, Tsai and Shah, Mubarak},
  year = {1994},
  month = oct,
  volume = {12},
  pages = {487--498},
  issn = {0262-8856},
  doi = {10.1016/0262-8856(94)90002-7},
  abstract = {In this paper, we present an extremely simple algorithm for shape from shading, which can be implemented in 25 lines of C code{${_\ast}{_\ast}$}C code and some images can be obtained by anonymous ftp from under the directory /pub/shading.. The algorithm is very fast, taking 0.2 seconds on a Sun SparcStation-1 for a 128 \texttimes{} 128 image, and is purely local and highly parallelizable (parallel implementation included). In our approach, we employ a linear approximation of the reflectance function, as used by others. However, the main difference is that we first use the discrete approximations for surface normal, p and q, using finite differences, and then linearize the reflectance function in depth, Z(x, y), instead of p and q. The algorithm has been tested on several synthetic and real images of both Lambertian and specular surfaces, and good results have been obtained.},
  file = {/home/henrik/Zotero/storage/RP6KRHXZ/0262885694900027.html},
  journal = {Image and Vision Computing},
  keywords = {3D shape,physics-based vision,shape from shading},
  number = {8}
}

@article{StackedCapsule19,
  title = {Stacked {{Capsule Autoencoders}}},
  author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
  year = {2019},
  month = jun,
  abstract = {An object can be seen as a geometrically organized set of interrelated parts. A system that makes explicit use of these geometric relationships to recognize objects should be naturally robust to changes in viewpoint, because the intrinsic geometric relationships are viewpoint-invariant. We describe an unsupervised version of capsule networks, in which a neural encoder, which looks at all of the parts, is used to infer the presence and poses of object capsules. The encoder is trained by backpropagating through a decoder, which predicts the pose of each already discovered part using a mixture of pose predictions. The parts are discovered directly from an image, in a similar manner, by using a neural encoder, which infers parts and their affine transformations. The corresponding decoder models each image pixel as a mixture of predictions made by affine-transformed parts. We learn object- and their part-capsules on unlabeled data, and then cluster the vectors of presences of object capsules. When told the names of these clusters, we achieve state-of-the-art results for unsupervised classification on SVHN (55\%) and near state-of-the-art on MNIST (98.5\%).},
  archivePrefix = {arXiv},
  eprint = {1906.06818},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/TL56H4EG/Kosiorek et al. - 2019 - Stacked Capsule Autoencoders.pdf;/home/henrik/Zotero/storage/ANLX6ZT3/1906.html},
  journal = {arXiv:1906.06818 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{StackedHourglass16,
  title = {Stacked {{Hourglass Networks}} for {{Human Pose Estimation}}},
  author = {Newell, Alejandro and Yang, Kaiyu and Deng, Jia},
  year = {2016},
  month = jul,
  abstract = {This work introduces a novel convolutional network architecture for the task of human pose estimation. Features are processed across all scales and consolidated to best capture the various spatial relationships associated with the body. We show how repeated bottom-up, top-down processing used in conjunction with intermediate supervision is critical to improving the performance of the network. We refer to the architecture as a "stacked hourglass" network based on the successive steps of pooling and upsampling that are done to produce a final set of predictions. State-of-the-art results are achieved on the FLIC and MPII benchmarks outcompeting all recent methods.},
  archivePrefix = {arXiv},
  eprint = {1603.06937},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/Q8M2CKRY/Newell et al. - 2016 - Stacked Hourglass Networks for Human Pose Estimati.pdf;/home/henrik/Zotero/storage/L27TVEVT/1603.html},
  journal = {arXiv:1603.06937 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@misc{StreetView,
  title = {The {{Street View House Numbers}} ({{SVHN}}) {{Dataset}}},
  file = {/home/henrik/Zotero/storage/4EP6T55T/housenumbers.html},
  howpublished = {http://ufldl.stanford.edu/housenumbers/}
}

@article{SurveyTransfer10,
  title = {A {{Survey}} on {{Transfer Learning}}},
  author = {Pan, Sinno Jialin and Yang, Qiang},
  year = {2010},
  month = oct,
  volume = {22},
  pages = {1345--1359},
  issn = {1041-4347},
  doi = {10.1109/TKDE.2009.191},
  abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as co-variate shift. We also explore some potential future issues in transfer learning research.},
  file = {/home/henrik/Zotero/storage/8RCRSP7B/Pan and Yang - 2010 - A Survey on Transfer Learning.pdf},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  language = {en},
  number = {10}
}

@article{Temporal3D17,
  title = {Temporal {{3D ConvNets}}: {{New Architecture}} and {{Transfer Learning}} for {{Video Classification}}},
  shorttitle = {Temporal {{3D ConvNets}}},
  author = {Diba, Ali and Fayyaz, Mohsen and Sharma, Vivek and Karami, Amir Hossein and Arzani, Mohammad Mahdi and Yousefzadeh, Rahman and Van Gool, Luc},
  year = {2017},
  month = nov,
  abstract = {The work in this paper is driven by the question how to exploit the temporal cues available in videos for their accurate classification, and for human action recognition in particular? Thus far, the vision community has focused on spatio-temporal approaches with fixed temporal convolution kernel depths. We introduce a new temporal layer that models variable temporal convolution kernel depths. We embed this new temporal layer in our proposed 3D CNN. We extend the DenseNet architecture - which normally is 2D - with 3D filters and pooling kernels. We name our proposed video convolutional network `Temporal 3D ConvNet'\textasciitilde{}(T3D) and its new temporal layer `Temporal Transition Layer'\textasciitilde{}(TTL). Our experiments show that T3D outperforms the current state-of-the-art methods on the HMDB51, UCF101 and Kinetics datasets. The other issue in training 3D ConvNets is about training them from scratch with a huge labeled dataset to get a reasonable performance. So the knowledge learned in 2D ConvNets is completely ignored. Another contribution in this work is a simple and effective technique to transfer knowledge from a pre-trained 2D CNN to a randomly initialized 3D CNN for a stable weight initialization. This allows us to significantly reduce the number of training samples for 3D CNNs. Thus, by finetuning this network, we beat the performance of generic and recent methods in 3D CNNs, which were trained on large video datasets, e.g. Sports-1M, and finetuned on the target datasets, e.g. HMDB51/UCF101. The T3D codes will be released},
  archivePrefix = {arXiv},
  eprint = {1711.08200},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/LDFKECWH/Diba et al. - 2017 - Temporal 3D ConvNets New Architecture and Transfe.pdf;/home/henrik/Zotero/storage/EG8XUIVI/1711.html},
  journal = {arXiv:1711.08200 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@misc{TfKeras19,
  title = {Tf.Keras and {{TensorFlow}}: {{Batch Normalization}} to Train Deep Neural Networks Faster},
  shorttitle = {Tf.Keras and {{TensorFlow}}},
  author = {Rawles, Chris},
  year = {2019},
  month = nov,
  abstract = {Training deep neural networks can be time consuming. In particular, training can be significantly impeded by vanishing gradients, which\ldots{}},
  file = {/home/henrik/Zotero/storage/VJ36ZSMI/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faste.html},
  howpublished = {https://towardsdatascience.com/how-to-use-batch-normalization-with-tensorflow-and-tf-keras-to-train-deep-neural-networks-faster-60ba4d054b73},
  journal = {Medium},
  language = {en}
}

@misc{UnderstandingConvolutional19,
  title = {Understanding {{Convolutional Neural Networks}} through {{Visualizations}} in {{PyTorch}}},
  author = {Kurama, Vihar},
  year = {2019},
  month = jan,
  abstract = {Getting down to the nitty-gritty of CNNs},
  file = {/home/henrik/Zotero/storage/KJXQZXET/understanding-convolutional-neural-networks-through-visualizations-in-pytorch-b5444de08b91.html},
  howpublished = {https://towardsdatascience.com/understanding-convolutional-neural-networks-through-visualizations-in-pytorch-b5444de08b91},
  journal = {Towards Data Science}
}

@inproceedings{UNetConvolutional15,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  booktitle = {Proceedings of the {{Medical Image Computing}} and {{Computer}}-{{Assisted Intervention}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  editor = {Navab, Nassir and Hornegger, Joachim and Wells, William M. and Frangi, Alejandro F.},
  year = {2015},
  pages = {234--241},
  publisher = {{Springer International Publishing}},
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  file = {/home/henrik/Zotero/storage/HVJEMHFB/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf},
  isbn = {978-3-319-24574-4},
  keywords = {Betina,Convolutional Layer,Data Augmentation,Deep Network,Ground Truth Segmentation,Training Image},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{UNetNested18,
  title = {{{UNet}}++: {{A Nested U}}-{{Net Architecture}} for {{Medical Image Segmentation}}},
  shorttitle = {{{UNet}}++},
  author = {Zhou, Zongwei and Siddiquee, Md Mahfuzur Rahman and Tajbakhsh, Nima and Liang, Jianming},
  year = {2018},
  month = jul,
  abstract = {In this paper, we present UNet++, a new, more powerful architecture for medical image segmentation. Our architecture is essentially a deeply-supervised encoder-decoder network where the encoder and decoder sub-networks are connected through a series of nested, dense skip pathways. The re-designed skip pathways aim at reducing the semantic gap between the feature maps of the encoder and decoder sub-networks. We argue that the optimizer would deal with an easier learning task when the feature maps from the decoder and encoder networks are semantically similar. We have evaluated UNet++ in comparison with U-Net and wide U-Net architectures across multiple medical image segmentation tasks: nodule segmentation in the low-dose CT scans of chest, nuclei segmentation in the microscopy images, liver segmentation in abdominal CT scans, and polyp segmentation in colonoscopy videos. Our experiments demonstrate that UNet++ with deep supervision achieves an average IoU gain of 3.9 and 3.4 points over U-Net and wide U-Net, respectively.},
  archivePrefix = {arXiv},
  eprint = {1807.10165},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/E5WHVWBK/Zhou et al. - 2018 - UNet++ A Nested U-Net Architecture for Medical Im.pdf;/home/henrik/Zotero/storage/Z59EZZFN/1807.html},
  journal = {arXiv:1807.10165 [cs, eess, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@inproceedings{UnsupervisedObject19,
  title = {Unsupervised {{Object Discovery}} via {{Capsule Decoders}}},
  author = {Kosiorek, Adam Roman and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey},
  year = {2019},
  file = {/home/henrik/Zotero/storage/8JTECSMB/Kosiorek et al. - 2019 - Unsupervised Object Discovery via Capsule Decoders.pdf}
}

@phdthesis{UnsupervisedPreprocessing19,
  title = {Unsupervised Preprocessing of Medical Imaging Data with Generative Adversarial Networks},
  author = {Kirker{\o}d, Mathias},
  year = {2019},
  abstract = {As an attempt to address the challenge of improving the field of computer-aided
diagnosis, our work explores ways to help existing models to increase their accuracy
when it comes to finding anomalies in medical images. In this thesis, we tackle the
problems associated with the misclassification of data based on overlays and other
artefacts in the medical image data.
We will look at how we can use machine learning to develop a system to increase
the classification accuracy of existing models, as well as going in-depth into the topic
of preprocessing to see if it has a place in modern classification models based on deep
learning.},
  file = {/home/henrik/Zotero/storage/VACL4IVN/KirkerÃ¸d - 2019 - Unsupervised preprocessing of medical imaging data.pdf},
  school = {University of Oslo},
  type = {Master's {{Thesis}}}
}

@article{VideoPixel16,
  title = {Video {{Pixel Networks}}},
  author = {Kalchbrenner, Nal and van den Oord, Aaron and Simonyan, Karen and Danihelka, Ivo and Vinyals, Oriol and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = oct,
  pages = {16},
  abstract = {We propose a probabilistic video model, the Video Pixel Network (VPN), that estimates the discrete joint distribution of the raw pixel values in a video. The model and the neural architecture reflect the time, space and color structure of video tensors and encode it as a four-dimensional dependency chain. The VPN approaches the best possible performance on the Moving MNIST benchmark, a leap over the previous state of the art, and the generated videos show only minor deviations from the ground truth. The VPN also produces detailed samples on the action-conditional Robotic Pushing benchmark and generalizes to the motion of novel objects.},
  archivePrefix = {arXiv},
  eprint = {1610.00527},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/WNL632ME/Kalchbrenner et al. - 2016 - Video Pixel Networks.pdf;/home/henrik/Zotero/storage/DVCSEY8H/1610.html},
  journal = {arXiv:1610.00527 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{WhyShould16,
  title = {"{{Why Should I Trust You}}?": {{Explaining}} the {{Predictions}} of {{Any Classifier}}},
  shorttitle = {"{{Why Should I Trust You}}?},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  year = {2016},
  month = aug,
  abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
  archivePrefix = {arXiv},
  eprint = {1602.04938},
  eprinttype = {arxiv},
  file = {/home/henrik/Zotero/storage/AQEMGTQ3/Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf;/home/henrik/Zotero/storage/PWMHJ2TI/1602.html},
  journal = {arXiv:1602.04938 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{WirelessCapsule00,
  title = {Wireless Capsule Endoscopy},
  author = {Iddan, Gavriel and Meron, Gavriel and Glukhovsky, Arkady and Swain, Paul},
  year = {2000},
  month = may,
  volume = {405},
  pages = {417},
  issn = {1476-4687},
  doi = {10.1038/35013140},
  abstract = {The discomfort of internal gastrointestinal examination may soon be a thing of the past.},
  copyright = {2000 Macmillan Magazines Ltd.},
  file = {/home/henrik/Zotero/storage/U6JJ6MIK/Iddan et al. - 2000 - Wireless capsule endoscopy.pdf;/home/henrik/Zotero/storage/HS7TCR72/35013140.html},
  journal = {Nature},
  language = {En},
  number = {6785}
}


