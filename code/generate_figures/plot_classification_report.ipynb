{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2d(line):\n",
    "    return \" \".join(line.split()).split(\" \")\n",
    "\n",
    "def parse_classification_report(path):\n",
    "    class_m = {}\n",
    "    tot_m = {}\n",
    "    \n",
    "    if \"teacher\" in path:\n",
    "        model = \"teacher\"\n",
    "    else:\n",
    "        model = \"student\"\n",
    "    \n",
    "    with open(path) as file:\n",
    "        line = file.readline()\n",
    "        line = file.readline() # skip first line\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            data = l2d(line)\n",
    "            \n",
    "            class_m[data[0]] = {\n",
    "                \"prec\": float(data[1]),\n",
    "                \"rec\": float(data[2]),\n",
    "                \"f1\": float(data[3]),\n",
    "                \"model\": model\n",
    "            }\n",
    "            line = file.readline()\n",
    "\n",
    "            if len(line) == 1:\n",
    "                line = False\n",
    "                \n",
    "        line = file.readline()\n",
    "        tot_m[\"acc\"] = l2d(line)[1]\n",
    "        line = file.readline()\n",
    "        tot_m[\"macro\"] = l2d(line)[2:5]\n",
    "        line = file.readline()\n",
    "        tot_m[\"weighted\"] = l2d(line)[2:5]\n",
    "                \n",
    "    return class_m, tot_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All class metrics from all iterations in one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"0_teacher\", \"0_student\", \"1_teacher\", \"1_student\", \"2_teacher\", \"2_student\"]\n",
    "\n",
    "contents = []\n",
    "for f in folders:\n",
    "    log_dir = \"/home/henriklg/master-thesis/code/hyper-kvasir/experiments/model-size/teachb0-studb6/{}/\".format(f)\n",
    "    report_file = log_dir+\"classification_report.txt\"\n",
    "    \n",
    "    class_m, _ = parse_classification_report(report_file)\n",
    "    contents.append(class_m)\n",
    "\n",
    "prec = []\n",
    "rec = []\n",
    "for content in contents:\n",
    "    prec += [value[\"prec\"] for key, value in content.items()]\n",
    "    rec += [value[\"rec\"] for key, value in content.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.JointGrid(rec, rec, height=5, ratio=2)\n",
    "g = g.plot_joint(sns.kdeplot, cmap=\"Reds_d\")\n",
    "g = g.plot_marginals(sns.kdeplot, color=\"r\", shade=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "\n",
    "ax = sns.jointplot(prec, rec, height=6, kind=\"reg\", color=\"#4CB391\", xlim=(0,1.02), ylim=(0,1.02) );\n",
    "# ax.plot_joint(plt.scatter, c=\"w\", s=50, linewidth=1.5, marker=\"+\")\n",
    "ax = (ax.set_axis_labels(\"Precision\", \"Recall\"))\n",
    "\n",
    "plt.savefig(\"precision_recall.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "\n",
    "ax = sns.jointplot(prec, rec, height=6, kind=\"kde\", color=\"#4CB391\", xlim=(0,1.02), ylim=(0,1.02) );\n",
    "ax.plot_joint(plt.scatter, c=\"w\", s=30, linewidth=1.0, marker=\"o\")\n",
    "\n",
    "ax = (ax.set_axis_labels(\"Precision\", \"Recall\"))\n",
    "\n",
    "plt.savefig(\"precision_recall.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate teacher and student class metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfolders = [\"0_teacher\", \"1_teacher\", \"2_teacher\"]\n",
    "tprec = []\n",
    "trec = []\n",
    "tf1 = []\n",
    "\n",
    "contents = []\n",
    "for f in tfolders:\n",
    "    log_dir = \"/home/henriklg/master-thesis/code/hyper-kvasir/experiments/model-size/teachb0-studb6/{}/\".format(f)\n",
    "    report_file = log_dir+\"classification_report.txt\"\n",
    "    \n",
    "    contents, _ = parse_classification_report(report_file)\n",
    "    tprec += [value[\"prec\"] for key, value in contents.items()]\n",
    "    trec += [value[\"rec\"] for key, value in contents.items()]\n",
    "    tf1 += [value[\"f1\"] for key, value in contents.items()]\n",
    "\n",
    "\n",
    "sfolders = [\"0_student\", \"1_student\", \"2_student\"]\n",
    "sprec = []\n",
    "srec = []\n",
    "sf1 = []\n",
    "\n",
    "contents = []\n",
    "for f in sfolders:\n",
    "    log_dir = \"/home/henriklg/master-thesis/code/hyper-kvasir/experiments/model-size/teachb0-studb6/{}/\".format(f)\n",
    "    report_file = log_dir+\"classification_report.txt\"\n",
    "    \n",
    "    contents, _ = parse_classification_report(report_file)\n",
    "    sprec += [value[\"prec\"] for key, value in contents.items()]\n",
    "    srec += [value[\"rec\"] for key, value in contents.items()]\n",
    "    sf1 += [value[\"f1\"] for key, value in contents.items()]\n",
    "    \n",
    "print (\"teacher avg f1 = {:.4f}\".format (np.sum(tf1)/len(tf1)))\n",
    "print (\"student avg f1 = {:.4f}\".format (np.sum(sf1)/len(sf1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "\n",
    "ax = sns.jointplot(tprec, trec, height=6, kind=\"reg\", color=\"#4CB391\", xlim=(0,1.02), ylim=(0,1.02) );\n",
    "ax.x = sprec\n",
    "ax.y = srec\n",
    "ax.plot_joint(plt.scatter, c=\"b\", s=50, linewidth=1.5, marker=\"+\")\n",
    "ax = (ax.set_axis_labels(\"Precision\", \"Recall\"))\n",
    "\n",
    "plt.savefig(\"precision_recall.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\")\n",
    "\n",
    "ax = sns.jointplot(tprec, trec, height=6, kind=\"kde\", color=\"#4CB391\", xlim=(0,1.02), ylim=(0,1.02) );\n",
    "ax.plot_joint(plt.scatter, c=\"r\", s=30, linewidth=1.0, marker=\"o\")\n",
    "ax.x = sprec\n",
    "ax.y = srec\n",
    "ax.plot_joint(plt.scatter, c=\"b\", s=30, linewidth=1.0, marker=\"o\")\n",
    "ax = ax.set_axis_labels(\"Precision\", \"Recall\")\n",
    "\n",
    "plt.savefig(\"precision_recall_con.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"0_teacher\", \"0_student\", \"1_teacher\", \"1_student\", \"2_teacher\", \"2_student\"]\n",
    "\n",
    "first = True\n",
    "\n",
    "for f in folders:\n",
    "    log_dir = \"/home/henriklg/master-thesis/code/hyper-kvasir/experiments/model-size/teachb0-studb6/{}/\".format(f)\n",
    "    report_file = log_dir+\"classification_report.txt\"\n",
    "    \n",
    "    contents, _ = parse_classification_report(report_file)\n",
    "    if first:\n",
    "        df = pd.DataFrame.from_dict(contents, orient='index')\n",
    "        first = False\n",
    "    else: \n",
    "        new_df = pd.DataFrame.from_dict(contents, orient='index')\n",
    "        df = new_df.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.lmplot(x=\"prec\", y=\"rec\", hue=\"model\", data=df, height=6, legend=True, palette=dict(student=\"b\", teacher=\"r\"));\n",
    "ax = (ax.set_axis_labels(\"Precision\", \"Recall\"))\n",
    "\n",
    "plt.savefig(\"precision_recall_reg.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bar chart - f1-score per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"0_teacher\", \"0_student\", \"1_teacher\", \"1_student\", \"2_teacher\", \"2_student\"]\n",
    "experiment = \"all-b0\"\n",
    "\n",
    "contents = []\n",
    "contents_tot = []\n",
    "\n",
    "for f in folders:\n",
    "    log_dir = \"/home/henriklg/master-thesis/code/hyper-kvasir/experiments/model-size/{}/{}/\".format(experiment, f)\n",
    "    report_file = log_dir+\"classification_report.txt\"\n",
    "    \n",
    "    class_m, tot_m = parse_classification_report(report_file)\n",
    "    contents.append(class_m)\n",
    "    contents_tot.append(tot_m)\n",
    "    \n",
    "f1 = []\n",
    "avg_f1 = []\n",
    "for cnt, content in enumerate(contents):\n",
    "    f1.append([value[\"f1\"] for key, value in content.items()])\n",
    "    avg_f1.append(contents_tot[cnt][\"weighted\"][2])\n",
    "\n",
    "class_names = [key for key, value in contents[0].items()]\n",
    "conf = {\"num_classes\": 23,\n",
    "        \"class_names\": class_names,\n",
    "        \"log_dir\": \"./\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_bar_chart(data, conf, title=None, fname=None, figsize=(15,6)):\n",
    "    \"\"\"\n",
    "    Takes in list of data and makes a bar chart of it.\n",
    "    Dynamically allocates placement for bars.\n",
    "    \"\"\"\n",
    "    folders = [\"Teach 0\", \"Stud 0\", \"Teach 1\", \"Stud 1\", \"Teach 2\", \"Stud 2\"]\n",
    "    \n",
    "    x = np.arange(conf[\"num_classes\"])\n",
    "    width = 0.8      # 1.0 = bars side by side\n",
    "    width = width/len(data)\n",
    "\n",
    "    num_bars = len(data)\n",
    "    if num_bars == 1:\n",
    "        bar_placement = [0]\n",
    "    # even number of bars\n",
    "    elif (num_bars % 2) == 0:\n",
    "        bar_placement = np.arange(-num_bars/2, num_bars/2+1)    #[-2, -1, 0, 1, 2]\n",
    "        bar_placement = np.delete(bar_placement, num_bars//2)   #delete 0\n",
    "        bar_placement = [bar+0.5 if bar<0 else bar-0.5 for bar in bar_placement]\n",
    "    # odd number of bars\n",
    "    else:\n",
    "        bar_placement = np.arange(-np.floor(num_bars/2), np.floor(num_bars/2)+1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "\n",
    "    rects = []\n",
    "    for cnt, (dat, placement) in enumerate(zip(data, bar_placement)):\n",
    "        if avg_f1:\n",
    "            labels = \"Iteration {} - avg {:.2f}\".format(cnt, float(avg_f1[cnt]))\n",
    "        else:\n",
    "            labels = \"Iteration {}\".format(cnt)\n",
    "        rects.append(ax.bar(x+placement*width, dat, width, label=labels))\n",
    "\n",
    "    ax.set_ylabel('F1 score')\n",
    "    if title:\n",
    "        title_string = title\n",
    "        ax.set_title(title_string)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(conf[\"class_names\"])\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.legend(loc='upper left');\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=25, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    plt.grid(axis='y')\n",
    "    fig.tight_layout()\n",
    "    if fname:\n",
    "        plt.savefig('{}/{}.pdf'.format(conf[\"log_dir\"], fname), format='pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_bar_chart(f1, conf, fname=\"f1_per_class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"0_teacher\", \"0_student\", \"1_teacher\", \"1_student\", \"2_teacher\", \"2_student\"]\n",
    "experiment = \"all-b0\"\n",
    "# iters = [\"Teach 0\", \"Stud 0\", \"Teach 1\", \"Stud 1\", \"Teach 2\", \"Stud 2\"]\n",
    "\n",
    "num_classes = 23\n",
    "contents = []\n",
    "\n",
    "for f in folders:\n",
    "    log_dir = \"/home/henriklg/master-thesis/code/hyper-kvasir/experiments/model-size/{}/{}/\".format(experiment, f)\n",
    "    report_file = log_dir+\"classification_report.txt\"\n",
    "    \n",
    "    _, tot_m = parse_classification_report(report_file)\n",
    "    contents.append(tot_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weighted and macro precision, recall and f1-score\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "acc = []\n",
    "\n",
    "metric = \"weighted\"\n",
    "\n",
    "for idx in range(len(contents)):\n",
    "    content = contents[idx]\n",
    "    \n",
    "    acc.append(float(content[\"acc\"]))\n",
    "    prec.append(float(content[metric][0]))\n",
    "    rec.append(float(content[metric][1]))\n",
    "    f1.append(float(content[metric][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(len(folders)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "fig = plt.plot(x,prec,'r', x,rec,'b', x,f1,'g', marker='o');\n",
    "fig = plt.legend([\"Precision\",\"Recall\",\"F1-score\", \"acc\"], fontsize=12)\n",
    "plt.tight_layout(pad=1.5)\n",
    "fig = plt.xlabel(\"Iteration\")\n",
    "fig = plt.ylabel(\"Weighted average score\")\n",
    "\n",
    "plt.savefig(\"prec_rec_f1.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(len(folders)))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "fig = plt.plot(x,acc,'r--', x,f1,'g-', linewidth=1.5, marker='o');\n",
    "fig = plt.legend([\"Accuracy\", \"F1-Score\"])\n",
    "plt.tight_layout(pad=1.5)\n",
    "fig = plt.xlabel(\"Iteration\")\n",
    "fig = plt.ylabel(\"Weighted average score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare F1-score for different model sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"0_teacher\", \"0_student\", \"1_teacher\", \"1_student\", \"2_teacher\", \"2_student\"]\n",
    "experiments = [\"all-b0\", \"all-b2\", \"all-b4\", \"all-b6\"]#, \"teachb0-studb6\"]\n",
    "legends = [\"EfficientNetB0\", \"EfficientNetB2\", \"EfficientNetB4\", \"EfficientNetB6\"]\n",
    "\n",
    "plt_val = []\n",
    "\n",
    "for exp in experiments:\n",
    "    \n",
    "    # parse all classification reports\n",
    "    contents = []\n",
    "    for f in folders:\n",
    "        log_dir = \"/home/henriklg/master-thesis/code/hyper-kvasir/experiments/model-size/{}/{}/\".format(exp, f)\n",
    "        report_file = log_dir+\"classification_report.txt\"\n",
    "\n",
    "        _, tot_m = parse_classification_report(report_file)\n",
    "        contents.append(tot_m)\n",
    "    \n",
    "    # get experiment metrics\n",
    "    prec = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    acc = []\n",
    "\n",
    "    metric = \"weighted\"\n",
    "    for idx in range(len(contents)):\n",
    "        content = contents[idx]\n",
    "\n",
    "        acc.append(float(content[\"acc\"]))\n",
    "        prec.append(float(content[metric][0]))\n",
    "        rec.append(float(content[metric][1]))\n",
    "        f1.append(float(content[metric][2]))\n",
    "    plt_val.append(f1)\n",
    "    \n",
    "    \n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "for ex in plt_val:\n",
    "    fig = plt.plot(x,ex, linewidth=1.5, marker='o')\n",
    "fig = plt.legend(legends)\n",
    "fig = plt.xlabel(\"Iteration\")\n",
    "fig = plt.ylabel(\"Weighted F1-score\")\n",
    "plt.tight_layout(True)\n",
    "plt.savefig(\"model_sizes.pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare noising student vs normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average f1 for every model size\n",
    "avg = [0, 0, 0, 0, 0, 0]\n",
    "for i in range(6):\n",
    "    for j in range(4):\n",
    "        avg[i] += plt_val[j][i]\n",
    "        \n",
    "y = [x/4.0 for x in avg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [\"0_teacher\", \"0_student\", \"1_teacher\", \"1_student\", \"2_teacher\", \"2_student\"]\n",
    "experiment = \"teachb0-studb6\"\n",
    "\n",
    "contents = []\n",
    "\n",
    "for f in folders:\n",
    "    log_dir = \"/home/henriklg/master-thesis/code/hyper-kvasir/experiments/model-size/{}/{}/\".format(experiment, f)\n",
    "    report_file = log_dir+\"classification_report.txt\"\n",
    "    \n",
    "    _, tot_m = parse_classification_report(report_file)\n",
    "    contents.append(tot_m)\n",
    "\n",
    "# Compare weighted and macro precision, recall and f1-score\n",
    "prec = []\n",
    "rec = []\n",
    "f1 = []\n",
    "acc = []\n",
    "\n",
    "metric = \"weighted\"\n",
    "for idx in range(len(contents)):\n",
    "    content = contents[idx]\n",
    "    \n",
    "    acc.append(float(content[\"acc\"]))\n",
    "    prec.append(float(content[metric][0]))\n",
    "    rec.append(float(content[metric][1]))\n",
    "    f1.append(float(content[metric][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "fig = plt.plot(x,y, x,f1, marker=\"o\")\n",
    "fig = plt.legend([\"Avg B0-B6\", \"Noisy student\"])\n",
    "fig = plt.xlabel(\"Iteration\")\n",
    "fig = plt.ylabel(\"Weighted average F1-score\")\n",
    "plt.tight_layout(True)\n",
    "plt.savefig(\"noisy_vs_nonoise.pdf\", format=\"pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
