{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will try to split the dataset into positive and negative classes and classify them as a binary dataset.\n",
    "Tested on laptop with CIFAR10 dataset but should transfer nicely to PillCam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "#from PIL import Image\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/henrik/anaconda3/envs/TF2/bin:/usr/local/texlive/2019/bin/x86_64-linux:/home/henrik/Documents/anaconda3/bin:/home/henrik/Documents/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n"
     ]
    }
   ],
   "source": [
    "# To make some of the conda packages work (Tensorboard and pydot)\n",
    "PATH = os.getenv('PATH')\n",
    "%env PATH=/home/henrik/anaconda3/envs/TF2/bin:$PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 128\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "NUM_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane' 'frog' 'cat' 'bird' 'horse' 'automobile' 'ship' 'deer' 'truck']\n",
      "Class names:  ['airplane' 'frog' 'cat' 'bird' 'horse' 'dog' 'automobile' 'ship' 'deer'\n",
      " 'truck']\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path('/home/henrik/master-thesis/data/cifar10/train/')\n",
    "\n",
    "DATASET_SIZE = len(list(data_dir.glob('*/*.png')))\n",
    "STEPS_PER_EPOCH = np.ceil(DATASET_SIZE/BATCH_SIZE)\n",
    "\n",
    "class_names = np.array([item.name for item in data_dir.glob('*') if item.name != 'metadata.json'])\n",
    "\n",
    "# split into true/neg\n",
    "neg_class_name = 'dog'\n",
    "pos_class_names = np.delete(class_names, np.where(neg_class_name == class_names))\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "print (\"Class names: \",class_names)\n",
    "\n",
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "airplane          : 5000\n",
      "frog              : 5000\n",
      "cat               : 5000\n",
      "bird              : 5000\n",
      "horse             : 5000\n",
      "dog               : 5000\n",
      "automobile        : 5000\n",
      "ship              : 5000\n",
      "deer              : 5000\n",
      "truck             : 5000\n",
      "\n",
      "Total number of images: 50000, in 2 classes\n"
     ]
    }
   ],
   "source": [
    "# Extract some info about each class\n",
    "samples_per_class = []\n",
    "for class_name in class_names:\n",
    "    class_samples = len(list(data_dir.glob(class_name+'/*.png')))\n",
    "    samples_per_class.append(class_samples)\n",
    "    print('{0:18}: {1:3d}'.format(class_name, class_samples))\n",
    "\n",
    "print ('\\nTotal number of images: {}, in {} classes'.format(DATASET_SIZE, NUM_CLASSES))\n",
    "\n",
    "# If one class contains more than half of the entire sample size\n",
    "if np.max(samples_per_class) > DATASET_SIZE//2:\n",
    "    print (\"But the dataset is mainly shit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images with `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'/home/henrik/master-thesis/data/cifar10/train/dog/1208.png', shape=(), dtype=string)\n",
      "tf.Tensor(b'/home/henrik/master-thesis/data/cifar10/train/horse/1137.png', shape=(), dtype=string)\n",
      "tf.Tensor(b'/home/henrik/master-thesis/data/cifar10/train/airplane/1856.png', shape=(), dtype=string)\n",
      "tf.Tensor(b'/home/henrik/master-thesis/data/cifar10/train/truck/3385.png', shape=(), dtype=string)\n",
      "tf.Tensor(b'/home/henrik/master-thesis/data/cifar10/train/frog/0211.png', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
    "\n",
    "# Print some examples\n",
    "for path in list_ds.take(5):\n",
    "    print (path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short pure-tensorflow function that converts a file path to an `image_data, label` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_int(file_path):\n",
    "    # Should return 0 or 1\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # get class integer from class-list\n",
    "    label_int64 = tf.reduce_min(tf.where(tf.equal(parts[-2], class_names)))\n",
    "    # cast to tensor array with dtype=uint8\n",
    "    label_uint8 = tf.dtypes.cast(label_int64, tf.uint8)\n",
    "    return tf.reshape(label_uint8, [-1])\n",
    "\n",
    "\n",
    "\n",
    "def get_label_int_new(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    \n",
    "    bc = parts[-2] == pos_class_names\n",
    "    nz_cnt = tf.math.count_nonzero(bc)\n",
    "    \n",
    "    if (nz_cnt > 0):\n",
    "        return tf.reshape(tf.dtypes.cast(tf.fill([1, 1], value=1), tf.uint8),[-1])\n",
    "    \n",
    "    return tf.reshape(tf.dtypes.cast(tf.fill([1, 1], value=0), tf.uint8),[-1])\n",
    "    #tf.Tensor([1], shape=(1,), dtype=uint8)\n",
    "    \n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label_int_new(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Dataset.map` to create a dataset of `Image, label` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'num_parallel_calls' so multiple images are loaded and processed in parallel\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0], shape=(1,), dtype=uint8)\n",
      "tf.Tensor([1], shape=(1,), dtype=uint8)\n",
      "tf.Tensor([1], shape=(1,), dtype=uint8)\n",
      "tf.Tensor([1], shape=(1,), dtype=uint8)\n",
      "tf.Tensor([1], shape=(1,), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "for img, label in labeled_ds.take(5):\n",
    "    print (label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m           DatasetV1Adapter\n",
       "\u001b[0;31mString form:\u001b[0m    <DatasetV1Adapter shapes: ((32, 32, 3), (1,)), types: (tf.float32, tf.uint8)>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/Documents/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\n",
       "\u001b[0;31mDocstring:\u001b[0m      Wraps a V2 `Dataset` object in the `tf.compat.v1.data.Dataset` API.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Creates a DatasetV2 object.\n",
       "\n",
       "This is a difference between DatasetV1 and DatasetV2. DatasetV1 does not\n",
       "take anything in its constructor whereas in the DatasetV2, we expect\n",
       "subclasses to create a variant_tensor and pass it in to the super() call.\n",
       "\n",
       "Args:\n",
       "  variant_tensor: A DT_VARIANT tensor that represents the dataset.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "labeled_ds?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset for training\n",
    "Want the data to be shuffled and batched. Here we use the `tf.data` api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training, test and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "train_ds = labeled_ds.take(train_size)\n",
    "test_ds = labeled_ds.skip(train_size)\n",
    "val_ds = test_ds.skip(val_size)\n",
    "test_ds = test_ds.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAAD8CAYAAACM5bN4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAXx0lEQVR4nO2daYwl51WG31ru1uv0dM9Mz8JkYieONyAQtoQAZgkOEQhEgDgiAkcgORBEQvIDIUBEigT8QkAgCVviCBEQKEJCxsgmiZzNMUnQxHZMxvuMZ99677tV3Sp+1LXcbn/v6SWdmRP7faRRS3Xmq/pqeW/de853zonKsoQQwh/x1Z6AECKMxCmEUyROIZwicQrhFIlTCKdInEI4ReL85nMfgCsRrzoyPM6dV+BY4gogcQrhlPRqT+AlwK8AGLnakxDfekic33yeudoTEN+a6Gvt9rgdwCcAPAWgA2AJwBcAvC3wf+/DC39z3jLc9j4A3wfgPwHMDbcdGf6f48N/kwD+CsBpAF0A/wfgtwFEm5zrdQD+FMBXAFwE0ANwAsDfAjgU+P9r5/bq4dwWALQBfAbA68hxUgC/CeABVNejDeAogN+CnrNtoYu2PT6ESkSfBfDnAP4FwMsA/COA929hP68F8DkATQAfAfAxAP019jqATwK4dXiMvwOwC8BfoBLsZvh5AO8AcBLAPwP4ACqB/zqALwM4SMZ9D4D7h3P7ewB3AXg9gE8BeNW6/1sb2v96OL+PoxJ/PDzexzY5V7GWsiz1b+v/rg1sq5dl+amyLLOyLA+u2X5fWbH2/95SPscd5BjHh/bPl2XZWLN9d1mWTw5tP7xm+5HhtjvX7efguvHP/vvJsiwHZVl+yJjb7etsdwy3f3Dd9vcNt3+gLMtkzfakLMt/GNp+doeu/Uvmn96c2+PJwLY+qjdHCuDHN7mfrwL4mw3+z++h+ir6LHN47u389k0c4/S68c9yL4BHUL2VQ3wBLwzLfARAjuqr+LPEqL66ngPwOwAGa2wDAO9F9TX5lzcxV7EGOYS2x2EAv4tKhIcBtNbZ2VfF9XxpA3uO6qvleu4b/v2uTRwjQiWM2wF8J4ApAMkaez8wBqh+o64nA3B+uI9nuQ7ANIDHAfwB2VcHwA2bmKtYg8S5da5BJaopVL8X7wWwiOotcQTArwJobHJf5zawX8Lz30Trx01u4hh/BuDdAM4CuAfVm7QztN2O6rdyiAWyPcfzxT09/PtKAH9kzGNsE3MVa5A4t857UD2Qb8cLv/a9FZU4N8tGK4dmUAlhvUBnh38XNxi/F5Vn92uovKzL6+xv3cQcN+LZOfw7KueT2CH0m3PrvGL49xMB24/s8LFShEMXtwz/Ht1g/DWo7vG9eKEwDw3t3yjHUL1lfwCV11bsEBLn1jk+/HvLuu23ogpP7DR/gud/Td6N537bfXSDsceHf1+P538VHUMVltmJb045qnDJfgB/iRf+/sbQduMOHOslhb7Wbp0PovpK+2+o3p6nAdwM4I0A/hXAW3bwWGdRCfNrAP4D1ZvpF1A97B9EFWe1OIcqPnobKs/wvah+p74B1YKGr6JaaPCN8n5UzqZ3APgZAJ9GdV32ovot+oMAfh9VfFVsEr05t85DAH4UlRf1TQB+A8AEqt9bH97hY/UB/AQqUd0G4A5Uv/HehSp8sRl+DcAfo3qjvRPVG/4uVF+XN/rNulkyAD+Hah3xowB+GlUI5Y2onrE/BPBPO3SslwxRWar6nlOOD/8euYpzEFcRvTmFcIrEKYRTJE4hnKLfnEI4xQyl3PSqFlVuszVOx03PHg5uH5/mS05XuqFVahVHrudLSF9/65uNebw8uL0s+AdSElwtVxEZC3qiiKdXWomXJTFaH5lszEZHM+dBPqQT41plUZPaVrtL1Pbkg/dR26XzJ4Pbly9eoGP2jvNVjFkvtOa/4ktf/By1zc2dpbazl8O2XTMH6Jhmnc/xwQe+HLw1+lorhFMkTiGcInEK4RSJUwinSJxCOEXiFMIpZihl13Qo+6ciMlL3BnkR3N5ps4oYwE03vYbaTpybo7anH3uc2vYfvDa4PTNiCgk3IbbCLEYoZTuEr+BmMEIpxhRLdsCI3+ckrVPboMdDKZ+8++PU9sSj/xvc3oxH6Zjpsb3U1l7tUtuFczxcUq/xi1XkYduZk6fpmH6X2xh6cwrhFIlTCKdInEI4ReIUwikSpxBOkTiFcIoZSsn63KGfxBm1Xb50Jrj94NgEHdOsc9f1vine3nLPBD+FKCclcmKeTVFaKR9Gep2dKbKz2El+xhy3MawwslJ4/AXIVtZX4nyOyyd5V8SV8xeD2/v1Nh0zf/YytbVXePhukPNzi2NuGyAPbrfCX3HEw050zJZHCCGuCBKnEE6ROIVwisQphFMkTiGcYnpr24vc/9Roci9YmoY1317mi3+PPRxqQ1kRJ7yj3qlJfgplFJ7jgeu/m44ZaU5TW1lan2U76641awh9M/ZKpl9E/BmIo7DXEgBiw8vbAl/Ejnb4Xlt9nvtdXicoMu5LHPN9loYnmu2zyPn1KGkbVI7enEI4ReIUwikSpxBOkTiFcIrEKYRTJE4hnGKGUsabU9SWJNx9ndbCbvRBhy+GvnTmKWqLjc+QxQvh8v0AUH843Ej5p97Gz2v2EK+bVPC1/mZ9nu2EWcrtLmC39mn2xQnbitKomxTz81qeX6C20QYPpcQIh1J6bX7xsx4PU5TGIx4n3JakvJpUvx8OmUQFvx6lcR0ZenMK4RSJUwinSJxCOEXiFMIpEqcQTpE4hXCKGUqZnZmhtiLjNV2yQTjM0h/wlf6tUe5eH20ZZf8H3H3d64Xd18cefIyOOX6qw/dHXOjABp2tt9GqobDqFW15bxuPLApybwoeArDaU5x8iGcZnTjJW2ggJdc448dq1nnWUm48H0bkAzBqCNXSsGxiY4eFkeVCp7DlEUKIK4LEKYRTJE4hnCJxCuEUiVMIp0icQjjFDKWkhjs5Sbn7GlnYbVwM+P4io/XDyDgPpTTGeKuGbm0suP3xh4/RMck0z5zp5EZmRG5lbxj9ssk1KcyslO2GbbhtMAiHMOrg51y056nt1CNfpLZsmY9rkFs9MTZJx6Dk1zet8ed0cZl33+7lvCN2XAt3+zYegY3SlsLH2fIIIcQVQeIUwikSpxBOkTiFcIrEKYRTJE4hnGKGUrqrPPNkrMbDG5Oj4fBGXDcOZxRA6ne4W7vZ4l2qZ3eHM13mLxshncRweadGdkyPFzxjCR8AEEfhMEBJQhsAUEZGmMVosR0ZvUGiiGRa5Kt0zNJ53qF6sMrDFEnJr+PU+K7w9hnewyY3LnCjwUMptQs8BHN5jnfL7hBdWNGSNA2HXyz05hTCKRKnEE6ROIVwisQphFMkTiGcYnprdxmLyg/PzvJxuyaC2431ycgzvsC6bXQubrKV0gBQD3/27J4Ozw8Almr886pmLBwfSXgbh16fe6KTQfh4idEZujBq91htHCLjoziOw97EbJEfa9m4L6XRcqFrtE9Is/BDUjMSIwqjT8biCo84rBrzILkbAIA+qUsUW203zK7oYfTmFMIpEqcQTpE4hXCKxCmEUyROIZwicQrhFDOUcmh2L7XNGrYWWYxeb/KwxyVjoXGR8/BAveCfL22yYD4Z5TGdJDEWh4OPaxi1arpdvogdxGUfG7fG6jZdGO584zLSLs/1lC84P9J8NbX1l3idoPOn+IL5laXF4PaLXR7biKywR5+fdF4YiQwNXrMoScK1qdi9BIDC6ALO0JtTCKdInEI4ReIUwikSpxBOkTiFcIrEKYRTzFBKY4TX51np87o+RRoOOUzM7KZjRoxa9gPD5d1e4LVqTpw7G9w+v49npdRI3SEAiAoeErE85eNGHZs+6ZadGyGAWsxvW0RCIgAwMCZZks/peILvr7Y7XO8HAHqre/i4Sd4xvbMSbofR6/Dskjzj2SWDgfFcGc+c1Vk8Iuk9iXHtY4VShHjxIHEK4RSJUwinSJxCOEXiFMIpEqcQTjFDKatGkakTZ85RW420ari4ysMvi8srfB5tXsCps8xd7MdOnglunxj9NjrmusO8qNmI0c7g2NefoLaFRX5uBQmLLC/wMTWjq/j47ilqm5ndT21pMxw2K41QRGb0H8iMcE9jkofU0lY446Pe48/AoDAycXIe/sqNlheRkd0Tp+Fzs1ouJIlR3Y4dZ8sjhBBXBIlTCKdInEI4ReIUwikSpxBOkTiFcIoZSvn8g49Q20K7Y4wMa36seYKOyHNeHaldGIW1WrxHychsOGQyMcX7vJx9+mlqe+V+nmlx0+ED1HbX3fdQ24C42PfM8DmOjfJzvjjHQ1x79/IsksmRcDbOpUVeqCtp8Kylwghv9IxGJAnpKRIlPEwRG93IrRBGLebXMTbCZizAaHYVNwrH0TlseYQQ4oogcQrhFIlTCKdInEI4ReIUwikSpxBOMUMpcxnPSuml3I0ekRbmiLg7PG1yG4wW5sku3tNi10w4C6PeIL0uADz9dR4+OnX0f6jtF9/8Fmp77zvvoLaHH30suL1W57fm6RM8JDW/tEpth2bHqe2a6w4Ht58+y+/LcodniqzymltoJ7xdfVyEs2BiIxSRGZkn/T4/VmFkXZXg2TglycbpGwXD0siUWhC9OYVwisQphFMkTiGcInEK4RSJUwinmC6kfJwv5qarf8G7JA+MujJFjXsFW2RRNgDsmeWL0Scnwwu9506fpmOyJd7eYekyXwT+uc/cR23ves+7qW3/gXBrgrv+6246ptngF3/fDPfIHtrL21DMToVrJ73i4M10zPISr3OUkwXsANDrcldupx32NretyEHGvaT9Lq9btbLKPbmLyzyxY64bHldGfB6RUW+JoTenEE6ROIVwisQphFMkTiGcInEK4RSJUwin2KtxR3kH4thYNByRLr5pnbcRSFu8DcLBmWlu28dL+7da4eMVlxbpmHgP399ojZ/zhcvhLtoAcPSrD1Dbbbf9UnB7Xv4YHbNgtK5ISFdxALjhxhupbfdM+F7XjLYEtdhqMWC0MzC6PPf74TBLd8DfIzWjc3hhLEY/fe4StT309Sep7TMPHQtub7d52MZq78DQm1MIp0icQjhF4hTCKRKnEE6ROIVwisQphFPMUMrIGK/PYxGTFfh1w+U9MsLr+kxPGN2ax3mmxaFDe4Pb3/S930/HXDrzDLUdPfoVarvu+uuo7dZb30Bt4+Ph877lh15LxwyMsv+s6zIAFEYqUVmGbUWfj8l6vHbP0uIyty3zzJ/5+bng9uWlBTrmwnneguKJJ5+itqOPhEMiALCS8/fWvhteE9yeNng4sCx5CwqG3pxCOEXiFMIpEqcQTpE4hXCKxCmEUyROIZxihlIs17C1xj4l7nwrlNIyuiTPTPCQzk3XXkNte0ixq5kmP68bXv46brvxVdTWbPJzgxH6eObEqeD2hXkeiuj3eabFglGg7PS5M3zcYjhT58wzPExR5DzMsmyESxYWeFhkbi4cSoljHooY5DwbpCTF5gAgHecZSNOHwu0pqoHhd1o/M9pMmBk8ZMyWRwghrggSpxBOkTiFcIrEKYRTJE4hnCJxCuEUM5RSS3l4wOr9UKuFd9swCnzVjKJPccZ7awxIbw0AWJ4Lu7ZHJng2xRNz3M3/2fvvp7bTp3iY4tKly9S2uhou1nXxfDikAAD9Pu8onQ94mKVt9A3J8vA+u13eM6RRr1NbHPPP/WaTh80mJ8Nhs337ZumYQy/jYY9xUrgMAC6s8ut4ucdDNx1SNCytWddDBb6EeNEgcQrhFIlTCKdInEI4ReIUwimmt7Zu1KMZDLjHsyQew9ioYTPW5J6uosvbD1w8c5Ladt/8yuD2tMm7aH/0zjup7Z7//jS1FUZ7isxYqJ6whdlG53DLU94wPKG7pngtpjFSy2hk1EhIMNpkzO7fT20HDvCO6dPT4X1anc/nV7jH/sTZ89S22OPe69WMv7cKhD25ccLv83bQm1MIp0icQjhF4hTCKRKnEE6ROIVwisQphFPMUEpuuJrZ4nYAYGt8o4K7mlPw0MzAWPjeWeExh4K0GDh9nrvX0yav9fLt33E9tS3M827ZzeYotU1NhUMHo2O8ztH4BG9dMTm1i9omdnHbNAmLtEb43I1oD5KEX8d2u01t50hrhRPn+D2zQimLy3zhfjdqUVvPsKWkXhTryg3YiQB0zJZHCCGuCBKnEE6ROIVwisQphFMkTiGcInEK4RQzlJJYdX3McWFrWfC6LI8+/ji1TYGHYOanwi0XAOBCJ9wSYHqa15WZ2sNL9LfGuXu9LPkVGR3hc4xJVorleS8jHsSwOluvZDzksHQ6nPmTF/wZWFrm+7NqD1k1lZaWw20ojJI+KCJ+sbIBn39znI8rjVAQinCGTGrUyCqMMCJDb04hnCJxCuEUiVMIp0icQjhF4hTCKRKnEE4xQylj47yjdEkyPgAgIiEYqzDVxB5e9Km7wjM+Hrswz/e5Gnb1H+jw7AErY2Jhgc/D6lxsZW8MSGn/xAiJwAgdJEYhrF7GM38iMv844fsrjf7mAyNslg+MazUSLkKWJkbn8MRog2C0pygjPo/E6kRdhs+tbrSnGBjzYOjNKYRTJE4hnCJxCuEUiVMIp0icQjhF4hTCKXZn6wZ3X1sZJqyYkRV+adR52GZgzKM1ygtQjTbC7vDlHu9ovNrlc+wWRlEz63PO6gJOeptYmRYs7FHtkLvzWVsWAIhJFgbLmgGARoP3USmN+Xd6PJTFWs5kpXHOMQ/3pEY2iFV0K8952KmgISR+nyPjejD05hTCKRKnEE6ROIVwisQphFMkTiGcYnprY8PLmBue104n3MYhM9oqREadIJTcVku5F6+PsBdvYKxE75aGR7bFawGVRmfrxJhjRLyhudE5PB8YnnJqAepG1+ucTd+oIRTDOC9jUTlSq0BSePOgzz3sVpsPPnu7ZURhRCOYyYpGaOG7EC8iJE4hnCJxCuEUiVMIp0icQjhF4hTCKWYoxVqsWzNq3DA3dGmEKVJjgbW1QDlNrRo3YXJjHoPUWPCcc3d+afjs0zpv45CQujMxqVMDAKWxcLxmLEa32gUMSHjGWgC+0uXXw1pk32zxZIU+qamE/s4vYIdhs8b1svBcmi0+R4VShHgRIXEK4RSJUwinSJxCOEXiFMIpEqcQTjFDKVZmRNOqL0S2W+XqWy2+v9wKYRgxDJZ9UjfK91v767R5J+fCKvtv1LiJayz0wTMc6sb+WJZLZTTCVbXwebfqPDSTGffFyvwpjRpISS1sK0p+7WOr9YMREsky47miFk63F87G2i56cwrhFIlTCKdInEI4ReIUwikSpxBOkTiFcIoZSrFczVbowMoSYPSMTAurI3ZhZG+kJHOm2eCnnRjZJX2jyFSWd6ita7R/aLRGgttbRqgqTY3sByMGkBgZPD1ybp0eP6+YdDCvjsXDVbn1XJH5WwW3MqO4mvUMW7RaPJOoIO8061jWM8zQm1MIp0icQjhF4hTCKRKnEE6ROIVwisQphFPMUIoVLrHc4bVa2GVv9ZIoDFuzaRSmMvqvJBErNGYUz9pGx+6NsM6b9aOxXO+Wzboe1jVmoYNmnT8i3U6bH8vIBonJ8wHwZ6c+tYuO6XV7fB7G/bTuS2rMMcvC+8yMUFucqLO1EC8aJE4hnCJxCuEUiVMIp0icQjhF4hTCKXavFKNpd2G0Pu/l4UJHsdHmOzEyHAaGW97q2ZKS41lFnwrSBwMARo1MhbHRcHYJAHRWjcJgvXAYoJNZvTqM8JFR4GtydJza2L2pG9kxNaNQ10qbZ7MkRkt69swVxpiBkYqT1vj8G8a5WS3pI4TvTdbn9yypmVILojenEE6ROIVwisQphFMkTiGcInEK4RTThZQYC73rI9w7yRYbDwruCWX1fgCglnLPmTWOtX8YMxY19zt88XLfaD8Qp/xajTT48UZJt+mu4TXuWAusjXtWGh72nCQ5lKzTNOyF3o0aryFktb3OyPGWVlb4/oyoQrPJn1PLW2vB2nnUjRpNsWGjY7Y8QghxRZA4hXCKxCmEUyROIZwicQrhFIlTCKeYoRSjxAqt9QIAzWa4G7JVz6Xf53Vg6sbi5YFR56jTDi/Az4x2BqmxmLtr1KqpG525J8f4gnN2AyLSaRoA0oZ1HfmieMvGQjBWvSVz4bgVOjCucU5aK9QbvMN2w7BZLSN6JOlgI1uNnNuIEV5EpBpCQrxokDiFcIrEKYRTJE4hnCJxCuEUiVMIp0RWSXohxNVDb04hnCJxCuEUiVMIp0icQjhF4hTCKRKnEE75fzKW4O2Hxm88AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(train_ds))\n",
    "\n",
    "plt.figure();\n",
    "plt.figure(frameon=False, facecolor='white');\n",
    "plt.title(class_names[label.numpy()][0], fontdict={'color':'white','size':20});\n",
    "plt.imshow(image.numpy());\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(ds):\n",
    "    return tf.data.experimental.cardinality(ds).numpy()\n",
    "\n",
    "print (\"{:32} {:>5}\".format(\"Full dataset sample size:\", get_size(labeled_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Train dataset sample size:\", get_size(train_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Test dataset sample size:\", get_size(test_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Validation dataset sample size:\", get_size(val_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "      if isinstance(cache, str):\n",
    "        ds = ds.cache(cache)\n",
    "      else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "train_ds = prepare_for_training(train_ds, cache=\"./cache/cifar_train_ds.tfcache\")\n",
    "# Create test dataset\n",
    "test_ds = prepare_for_training(test_ds, cache=\"./cache/cifar_test_ds.tfcache\")\n",
    "# Create validation dataset\n",
    "val_ds = prepare_for_training(val_ds, cache=\"./cache/cifar_val_ds.tfcache\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "See https://www.tensorflow.org/tutorials/images/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, 'multi_input_and_output_model.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "history = model.fit(train_ds,\n",
    "                    steps_per_epoch = train_size // BATCH_SIZE, # number of batches\n",
    "                    epochs = EPOCHS, \n",
    "                    validation_data = test_ds,\n",
    "                    validation_steps = test_size // BATCH_SIZE\n",
    "                   )\n",
    "\n",
    "end = time.time()\n",
    "print(\"Time spent on training: {:0.2f}\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_ds, verbose=2, steps=test_size//BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
