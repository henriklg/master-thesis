{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import resnet\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "NUM_GPUS = 1\n",
    "BS_PER_GPU = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 8\n",
    "NUM_TRAIN_SAMPLES = 50000\n",
    "\n",
    "BASE_LEARNING_RATE = 0.1\n",
    "LR_SCHEDULE = [(0.1, 30), (0.01, 45)]\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def augmentation(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(\n",
    "        x, HEIGHT + 8, WIDTH + 8)\n",
    "    x = tf.image.random_crop(x, [HEIGHT, WIDTH, NUM_CHANNELS])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\t\n",
    "\n",
    "\n",
    "def schedule(epoch):\n",
    "  initial_learning_rate = BASE_LEARNING_RATE * BS_PER_GPU / 128\n",
    "  learning_rate = initial_learning_rate\n",
    "  for mult, start_epoch in LR_SCHEDULE:\n",
    "    if epoch >= start_epoch:\n",
    "      learning_rate = initial_learning_rate * mult\n",
    "    else:\n",
    "      break\n",
    "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "  return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x,y), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_dataset_old = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "test_dataset_old = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_old = train_dataset_old.shuffle(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m            EagerTensor\n",
       "\u001b[0;31mString form:\u001b[0m     tf.Tensor([4], shape=(1,), dtype=uint8)\n",
       "\u001b[0;31mLength:\u001b[0m          1\n",
       "\u001b[0;31mFile:\u001b[0m            ~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\n",
       "\u001b[0;31mDocstring:\u001b[0m       <no docstring>\n",
       "\u001b[0;31mClass docstring:\u001b[0m Base class for EagerTensor.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, label = (next(iter(train_dataset_old.take(1))))\n",
    "label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_dataset_old.take(2):\n",
    "    plt.figure()\n",
    "    plt.imshow(image.numpy())\n",
    "    #print(repr(image.numpy()))\n",
    "    print(label)\n",
    "    #print(class_names[np.where(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/augere_export_class/')\n",
    "\n",
    "DATASET_SIZE = len(list(data_dir.glob('*/*.png')))\n",
    "class_names = np.array([item.name for item in data_dir.glob('*') if item.name != 'metadata.json'])\n",
    "\n",
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Anatomic landmarks', 'Unknown', 'Protruding lesions',\n",
       "       'Flat lesions', 'Lumen', 'Mucosa', 'Normal', 'Excavated lesions'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m        tuple\n",
       "\u001b[0;31mString form:\u001b[0m (<tf.Tensor: id=586411, shape=(), dtype=int32, numpy=6>,)\n",
       "\u001b[0;31mLength:\u001b[0m      1\n",
       "\u001b[0;31mDocstring:\u001b[0m  \n",
       "Built-in immutable sequence.\n",
       "\n",
       "If no argument is given, the constructor returns an empty tuple.\n",
       "If iterable is specified the tuple is initialized from iterable's items.\n",
       "\n",
       "If the argument is a tuple, the return value is the same object.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_process(file_path):\n",
    "    \n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    \n",
    "    label = [i for i, s in enumerate(class_names) if 'Normal' in s]\n",
    "    return label\n",
    "\n",
    "test_list_ds = list_ds.map(test_process)\n",
    "test = test_list_ds.take(100).shuffle(10)\n",
    "test = next(iter(test))\n",
    "test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m            uint8\n",
       "\u001b[0;31mString form:\u001b[0m     6\n",
       "\u001b[0;31mFile:\u001b[0m            ~/anaconda3/envs/TF2/lib/python3.7/site-packages/numpy/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m       <no docstring>\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "Unsigned integer type, compatible with C ``unsigned char``.\n",
       "Character code: ``'B'``.\n",
       "Canonical name: ``np.ubyte``.\n",
       "Alias *on this platform*: ``np.uint8``: 8-bit unsigned integer (0 to 255).\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_process(file_path):\n",
    "    \n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    \n",
    "    test = tf.reduce_min(tf.where(tf.equal(parts[-2], class_names)))\n",
    "    test = tf.dtypes.cast(test, tf.uint8)\n",
    "    return test\n",
    "\n",
    "test_list_ds = list_ds.map(test_process)\n",
    "test = test_list_ds.take(100).shuffle(10)\n",
    "test = next(iter(test)).numpy()\n",
    "test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_int(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    label_int64 = tf.reduce_min(tf.where(tf.equal(parts[-2], class_names)))\n",
    "    label_uint8 = tf.dtypes.cast(label_int64, tf.uint8)\n",
    "    return label_uint8\n",
    "\n",
    "def get_label_bool(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    #label = parts[-2] == class_names\n",
    "    label = [i for i, s in enumerate(class_names) if 'Normal' in s]\n",
    "    return np.uint8(label)\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label_bool(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=586494, shape=(32, 32, 3), dtype=float32, numpy=\n",
      "array([[[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.0002451 , 0.        , 0.01372549],\n",
      "        ...,\n",
      "        [0.0002451 , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.0002451 , 0.00220588, 0.        ],\n",
      "        ...,\n",
      "        [0.00441177, 0.        , 0.01004902],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.00098039, 0.        , 0.00196078],\n",
      "        [0.00245098, 0.        , 0.        ],\n",
      "        [0.00294118, 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]]], dtype=float32)>, <tf.Tensor: id=586495, shape=(1,), dtype=int32, numpy=array([6], dtype=int32)>)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(labeled_ds)))\n",
    "\n",
    "#print(labeled_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anatomic landmarks']\n",
      "['Anatomic landmarks']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in labeled_ds.take(2):\n",
    "    plt.figure()\n",
    "    plt.imshow(image.numpy())\n",
    "    #print(repr(image.numpy()))\n",
    "    print(class_names[np.where(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "full_ds = labeled_ds\n",
    "train_ds = full_ds.take(train_size)\n",
    "test_ds = full_ds.skip(train_size)\n",
    "val_ds = test_ds.skip(val_size)\n",
    "test_ds = test_ds.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44711\n"
     ]
    }
   ],
   "source": [
    "num_elements = tf.data.experimental.cardinality(full_ds).numpy()\n",
    "print (num_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "      if isinstance(cache, str):\n",
    "        ds = ds.cache(cache)\n",
    "      else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    #ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "# Create training dataset\n",
    "train_ds = prepare_for_training(train_ds, cache=\"./train_ds.tfcache\")\n",
    "# Create test dataset\n",
    "test_ds = prepare_for_training(test_ds, cache=\"./test_ds.tfcache\")\n",
    "# Create validation dataset\n",
    "val_ds = prepare_for_training(val_ds, cache=\"./val_ds.tfcache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_ds\n",
    "test_dataset = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "245/245 [==============================] - 25s 104ms/step - loss: 0.8412 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/60\n",
      "245/245 [==============================] - 14s 59ms/step - loss: 0.6891 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6230 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/60\n",
      "245/245 [==============================] - 14s 58ms/step - loss: 0.5660 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5117 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/60\n",
      "179/245 [====================>.........] - ETA: 3s - loss: 0.4769 - sparse_categorical_accuracy: 1.0000"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(22)\n",
    "#train_dataset = train_dataset.map(augmentation).map(preprocess).shuffle(NUM_TRAIN_SAMPLES).batch(BS_PER_GPU * NUM_GPUS, drop_remainder=True)\n",
    "#test_dataset = test_dataset.map(preprocess).batch(BS_PER_GPU * NUM_GPUS, drop_remainder=True)\n",
    "\n",
    "input_shape = (32, 32, 3)\n",
    "img_input = tf.keras.layers.Input(shape=input_shape)\n",
    "opt = keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "if NUM_GPUS == 1:\n",
    "    model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES)\n",
    "    model.compile(\n",
    "              optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "else:\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "      model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES)\n",
    "      model.compile(\n",
    "                optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['sparse_categorical_accuracy'])  \n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = TensorBoard(\n",
    "  log_dir=log_dir,\n",
    "  update_freq='batch',\n",
    "  histogram_freq=1)\n",
    "\n",
    "lr_schedule_callback = LearningRateScheduler(schedule)\n",
    "\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=test_dataset,\n",
    "          validation_freq=1,\n",
    "          callbacks=[tensorboard_callback, lr_schedule_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n",
      "53/53 [==============================] - 2s 45ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0018190223534749646, 1.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)\n",
    "\n",
    "model.save('model.h5')\n",
    "\n",
    "new_model = keras.models.load_model('model.h5')\n",
    " \n",
    "new_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/augere_export_class/')\n",
    "\n",
    "DATASET_SIZE = len(list(data_dir.glob('*/*.png')))\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "IMG_HEIGHT = 32 #224\n",
    "IMG_WIDTH = 32\n",
    "STEPS_PER_EPOCH = np.ceil(DATASET_SIZE/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array([item.name for item in data_dir.glob('*') if item.name != 'metadata.json'])\n",
    "\n",
    "for classes in class_names:\n",
    "    class_samples = len(list(data_dir.glob(classes+'/*.png')))\n",
    "    print('{0:18}: {1:3d}'.format(classes, class_samples))\n",
    "\n",
    "print ('\\nTotal number of images:', DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images with `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
    "\n",
    "# Show some examples\n",
    "for path in list_ds.take(5):\n",
    "    print(path.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short pure-tensorflow function that converts a file path to an `image_data, label` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == class_names\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Dataset.map` to create a dataset of `Image, label` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'num_parallel_calls' so multiple images are loaded and processed in parallel\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(\n",
    "        x, IMG_HEIGHT + 8, IMG_WIDTH + 8)\n",
    "    x = tf.image.random_crop(x, [IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\n",
    "\n",
    "def normalize(x, y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (labeled_ds\n",
    "                 .map(augmentation)\n",
    "                 .shuffle(buffer_size=500)\n",
    "                 .map(normalize)\n",
    "                 .batch(BATCH_SIZE, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "          loss='sparse_categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset,\n",
    "          epochs=60,\n",
    "          validation_data=test_ds,\n",
    "          validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in labeled_ds.take(1):\n",
    "    print ('Image shape: ', image.numpy().shape)\n",
    "    print ('Label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset for training\n",
    "Want the data to be shuffled and batched. Here we use the `tf.data` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "      if isinstance(cache, str):\n",
    "        ds = ds.cache(cache)\n",
    "      else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training, test and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "full_ds = labeled_ds\n",
    "train_ds = full_ds.take(train_size)\n",
    "test_ds = full_ds.skip(train_size)\n",
    "val_ds = test_ds.skip(val_size)\n",
    "test_ds = test_ds.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elements = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "print (num_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "train_ds = prepare_for_training(train_ds, cache=\"./train_ds.tfcache\")\n",
    "# Create test dataset\n",
    "test_ds = prepare_for_training(test_ds, cache=\"./test_ds.tfcache\")\n",
    "# Create validation dataset\n",
    "val_ds = prepare_for_training(val_ds, cache=\"./val_ds.tfcache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch \n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "# Display images in current batch\n",
    "show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = train_size // BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = val_ds,\n",
    "    validation_steps = val_size // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (2D-CNN)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
