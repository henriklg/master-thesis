{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import resnet\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "NUM_GPUS = 1\n",
    "BS_PER_GPU = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 8\n",
    "NUM_TRAIN_SAMPLES = 50000\n",
    "\n",
    "BASE_LEARNING_RATE = 0.1\n",
    "LR_SCHEDULE = [(0.1, 30), (0.01, 45)]\n",
    "\n",
    "\n",
    "def preprocess(x, y):\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def augmentation(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(\n",
    "        x, HEIGHT + 8, WIDTH + 8)\n",
    "    x = tf.image.random_crop(x, [HEIGHT, WIDTH, NUM_CHANNELS])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\t\n",
    "\n",
    "\n",
    "def schedule(epoch):\n",
    "  initial_learning_rate = BASE_LEARNING_RATE * BS_PER_GPU / 128\n",
    "  learning_rate = initial_learning_rate\n",
    "  for mult, start_epoch in LR_SCHEDULE:\n",
    "    if epoch >= start_epoch:\n",
    "      learning_rate = initial_learning_rate * mult\n",
    "    else:\n",
    "      break\n",
    "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "  return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(x,y), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "train_dataset_old = tf.data.Dataset.from_tensor_slices((x,y))\n",
    "test_dataset_old = tf.data.Dataset.from_tensor_slices((x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_old = train_dataset_old.shuffle(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=142, shape=(32, 32, 3), dtype=uint8, numpy=\n",
      "array([[[156, 194, 129],\n",
      "        [167, 212, 127],\n",
      "        [169, 215, 129],\n",
      "        ...,\n",
      "        [243, 247, 226],\n",
      "        [230, 235, 206],\n",
      "        [208, 215, 180]],\n",
      "\n",
      "       [[174, 210, 147],\n",
      "        [192, 229, 159],\n",
      "        [172, 204, 163],\n",
      "        ...,\n",
      "        [237, 243, 223],\n",
      "        [213, 221, 200],\n",
      "        [186, 193, 169]],\n",
      "\n",
      "       [[192, 234, 145],\n",
      "        [194, 230, 178],\n",
      "        [174, 201, 194],\n",
      "        ...,\n",
      "        [226, 231, 218],\n",
      "        [198, 208, 186],\n",
      "        [195, 200, 165]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[187, 211, 151],\n",
      "        [211, 234, 180],\n",
      "        [231, 244, 221],\n",
      "        ...,\n",
      "        [210, 204, 199],\n",
      "        [217, 208, 210],\n",
      "        [203, 192, 192]],\n",
      "\n",
      "       [[231, 238, 218],\n",
      "        [220, 236, 209],\n",
      "        [200, 219, 177],\n",
      "        ...,\n",
      "        [183, 170, 153],\n",
      "        [189, 175, 160],\n",
      "        [186, 169, 153]],\n",
      "\n",
      "       [[238, 242, 224],\n",
      "        [238, 247, 232],\n",
      "        [229, 239, 217],\n",
      "        ...,\n",
      "        [141, 126, 106],\n",
      "        [151, 137, 117],\n",
      "        [157, 141, 118]]], dtype=uint8)>, <tf.Tensor: id=143, shape=(1,), dtype=uint8, numpy=array([2], dtype=uint8)>)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(train_dataset_old.take(1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in train_dataset_old.take(2):\n",
    "    plt.figure()\n",
    "    plt.imshow(image.numpy())\n",
    "    #print(repr(image.numpy()))\n",
    "    print(label)\n",
    "    #print(class_names[np.where(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/augere_export_class/')\n",
    "\n",
    "DATASET_SIZE = len(list(data_dir.glob('*/*.png')))\n",
    "class_names = np.array([item.name for item in data_dir.glob('*') if item.name != 'metadata.json'])\n",
    "\n",
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Anatomic landmarks', 'Unknown', 'Protruding lesions',\n",
       "       'Flat lesions', 'Lumen', 'Mucosa', 'Normal', 'Excavated lesions'],\n",
       "      dtype='<U18')"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parts = tf.strings.split(file_path, os.path.sep)\n",
    "\n",
    "print (parts[-2])\n",
    "label = [i for i, s in enumerate(class_names) if 'Normal' in s]\n",
    "indices\n",
    "#print(class_names[np.where('Normal')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lumen\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.python.util as pyutil\n",
    "\n",
    "test = next(iter(list_ds))\n",
    "test_string = test.numpy()\n",
    "\n",
    "filename = pyutil.compat.as_text(test_string)\n",
    "file_split = filename.split('/')\n",
    "print (file_split[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m           DatasetV1Adapter\n",
       "\u001b[0;31mString form:\u001b[0m    <DatasetV1Adapter shapes: (), types: tf.string>\n",
       "\u001b[0;31mFile:\u001b[0m           ~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\n",
       "\u001b[0;31mDocstring:\u001b[0m      Wraps a V2 `Dataset` object in the `tf.compat.v1.data.Dataset` API.\n",
       "\u001b[0;31mInit docstring:\u001b[0m\n",
       "Creates a DatasetV2 object.\n",
       "\n",
       "This is a difference between DatasetV1 and DatasetV2. DatasetV1 does not\n",
       "take anything in its constructor whereas in the DatasetV2, we expect\n",
       "subclasses to create a variant_tensor and pass it in to the super() call.\n",
       "\n",
       "Args:\n",
       "  variant_tensor: A DT_VARIANT tensor that represents the dataset.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list_ds?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mType:\u001b[0m            uint8\n",
       "\u001b[0;31mString form:\u001b[0m     6\n",
       "\u001b[0;31mFile:\u001b[0m            ~/anaconda3/envs/TF2/lib/python3.7/site-packages/numpy/__init__.py\n",
       "\u001b[0;31mDocstring:\u001b[0m       <no docstring>\n",
       "\u001b[0;31mClass docstring:\u001b[0m\n",
       "Unsigned integer type, compatible with C ``unsigned char``.\n",
       "Character code: ``'B'``.\n",
       "Canonical name: ``np.ubyte``.\n",
       "Alias *on this platform*: ``np.uint8``: 8-bit unsigned integer (0 to 255).\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_process(file_path):\n",
    "    \n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    \n",
    "    test = tf.reduce_min(tf.where(tf.equal(parts[-2], class_names)))\n",
    "    test = tf.dtypes.cast(test, tf.uint8)\n",
    "    return test\n",
    "\n",
    "test_list_ds = list_ds.map(test_process)\n",
    "test = test_list_ds.take(100).shuffle(10)\n",
    "test = next(iter(test)).numpy()\n",
    "test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_int(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    label_int64 = tf.reduce_min(tf.where(tf.equal(parts[-2], class_names)))\n",
    "    label_uint8 = tf.dtypes.cast(label_int64, tf.uint8)\n",
    "    return label_uint8\n",
    "\n",
    "def get_label_boolean(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    label = parts[-2] == class_names\n",
    "    return label\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label_int(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: id=557411, shape=(32, 32, 3), dtype=float32, numpy=\n",
      "array([[[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.00073529, 0.        , 0.00490196],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.0002451 , 0.00147059],\n",
      "        [0.00073529, 0.        , 0.0004902 ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.00098039],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.00073529, 0.00220588, 0.00147059],\n",
      "        ...,\n",
      "        [0.00073529, 0.        , 0.00392157],\n",
      "        [0.        , 0.        , 0.00196078],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.00098039, 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]],\n",
      "\n",
      "       [[0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        ...,\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ],\n",
      "        [0.        , 0.        , 0.        ]]], dtype=float32)>, <tf.Tensor: id=557412, shape=(), dtype=uint8, numpy=6>)\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(labeled_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next(iter(labeled_ds)))\n",
    "\n",
    "print(labeled_ds.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Anatomic landmarks']\n",
      "['Anatomic landmarks']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image, label in labeled_ds.take(2):\n",
    "    plt.figure()\n",
    "    plt.imshow(image.numpy())\n",
    "    #print(repr(image.numpy()))\n",
    "    print(class_names[np.where(label)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "full_ds = labeled_ds\n",
    "train_ds = full_ds.take(train_size)\n",
    "test_ds = full_ds.skip(train_size)\n",
    "val_ds = test_ds.skip(val_size)\n",
    "test_ds = test_ds.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44711\n"
     ]
    }
   ],
   "source": [
    "num_elements = tf.data.experimental.cardinality(full_ds).numpy()\n",
    "print (num_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "      if isinstance(cache, str):\n",
    "        ds = ds.cache(cache)\n",
    "      else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    #ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "# Create training dataset\n",
    "train_ds = prepare_for_training(train_ds, cache=\"./train_ds.tfcache\")\n",
    "# Create test dataset\n",
    "test_ds = prepare_for_training(test_ds, cache=\"./test_ds.tfcache\")\n",
    "# Create validation dataset\n",
    "val_ds = prepare_for_training(val_ds, cache=\"./val_ds.tfcache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_ds\n",
    "test_dataset = test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "      1/Unknown - 10s 10s/step"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " Cannot update variable with shape [] using a Tensor with shape [128], shapes must be equal.\n\t [[node metrics/sparse_categorical_accuracy/AssignAddVariableOp (defined at /home/henrik/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_582548]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-257-47cb2670f91c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m           callbacks=[tensorboard_callback, lr_schedule_callback])\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/anaconda3/envs/TF2/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  Cannot update variable with shape [] using a Tensor with shape [128], shapes must be equal.\n\t [[node metrics/sparse_categorical_accuracy/AssignAddVariableOp (defined at /home/henrik/anaconda3/envs/TF2/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1751) ]] [Op:__inference_distributed_function_582548]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(22)\n",
    "#train_dataset = train_dataset.map(augmentation).map(preprocess).shuffle(NUM_TRAIN_SAMPLES).batch(BS_PER_GPU * NUM_GPUS, drop_remainder=True)\n",
    "#test_dataset = test_dataset.map(preprocess).batch(BS_PER_GPU * NUM_GPUS, drop_remainder=True)\n",
    "\n",
    "input_shape = (32, 32, 3)\n",
    "img_input = tf.keras.layers.Input(shape=input_shape)\n",
    "opt = keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "if NUM_GPUS == 1:\n",
    "    model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES)\n",
    "    model.compile(\n",
    "              optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "else:\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "      model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES)\n",
    "      model.compile(\n",
    "                optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['sparse_categorical_accuracy'])  \n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "tensorboard_callback = TensorBoard(\n",
    "  log_dir=log_dir,\n",
    "  update_freq='batch',\n",
    "  histogram_freq=1)\n",
    "\n",
    "lr_schedule_callback = LearningRateScheduler(schedule)\n",
    "\n",
    "\n",
    "model.fit(train_dataset,\n",
    "          epochs=NUM_EPOCHS,\n",
    "          validation_data=test_dataset,\n",
    "          validation_freq=1,\n",
    "          callbacks=[tensorboard_callback, lr_schedule_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 1s 18ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n",
      "53/53 [==============================] - 2s 45ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0018190223534749646, 1.0]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)\n",
    "\n",
    "model.save('model.h5')\n",
    "\n",
    "new_model = keras.models.load_model('model.h5')\n",
    " \n",
    "new_model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/augere_export_class/')\n",
    "\n",
    "DATASET_SIZE = len(list(data_dir.glob('*/*.png')))\n",
    "\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "IMG_HEIGHT = 32 #224\n",
    "IMG_WIDTH = 32\n",
    "STEPS_PER_EPOCH = np.ceil(DATASET_SIZE/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = np.array([item.name for item in data_dir.glob('*') if item.name != 'metadata.json'])\n",
    "\n",
    "for classes in class_names:\n",
    "    class_samples = len(list(data_dir.glob(classes+'/*.png')))\n",
    "    print('{0:18}: {1:3d}'.format(classes, class_samples))\n",
    "\n",
    "print ('\\nTotal number of images:', DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images with `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))\n",
    "\n",
    "# Show some examples\n",
    "for path in list_ds.take(5):\n",
    "    print(path.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short pure-tensorflow function that converts a file path to an `image_data, label` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == class_names\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `Dataset.map` to create a dataset of `Image, label` pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set 'num_parallel_calls' so multiple images are loaded and processed in parallel\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augmentation(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(\n",
    "        x, IMG_HEIGHT + 8, IMG_WIDTH + 8)\n",
    "    x = tf.image.random_crop(x, [IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\n",
    "\n",
    "def normalize(x, y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (labeled_ds\n",
    "                 .map(augmentation)\n",
    "                 .shuffle(buffer_size=500)\n",
    "                 .map(normalize)\n",
    "                 .batch(BATCH_SIZE, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "          loss='sparse_categorical_crossentropy',\n",
    "          optimizer='adam',\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset,\n",
    "          epochs=60,\n",
    "          validation_data=test_ds,\n",
    "          validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in labeled_ds.take(1):\n",
    "    print ('Image shape: ', image.numpy().shape)\n",
    "    print ('Label: ', label.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset for training\n",
    "Want the data to be shuffled and batched. Here we use the `tf.data` api."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "      if isinstance(cache, str):\n",
    "        ds = ds.cache(cache)\n",
    "      else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training, test and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "full_ds = labeled_ds\n",
    "train_ds = full_ds.take(train_size)\n",
    "test_ds = full_ds.skip(train_size)\n",
    "val_ds = test_ds.skip(val_size)\n",
    "test_ds = test_ds.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elements = tf.data.experimental.cardinality(val_ds).numpy()\n",
    "print (num_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training dataset\n",
    "train_ds = prepare_for_training(train_ds, cache=\"./train_ds.tfcache\")\n",
    "# Create test dataset\n",
    "test_ds = prepare_for_training(test_ds, cache=\"./test_ds.tfcache\")\n",
    "# Create validation dataset\n",
    "val_ds = prepare_for_training(val_ds, cache=\"./val_ds.tfcache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one batch \n",
    "image_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "# Display images in current batch\n",
    "show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = train_size // BATCH_SIZE,\n",
    "    epochs = EPOCHS,\n",
    "    validation_data = val_ds,\n",
    "    validation_steps = val_size // BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize training results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (2D-CNN)",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
