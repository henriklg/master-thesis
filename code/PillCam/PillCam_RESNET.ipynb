{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import resnet\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To make some of the conda packages work (Tensorboard and pydot)\n",
    "PATH = os.getenv('PATH')\n",
    "%env PATH=/home/henrik/anaconda3/envs/TF2/bin:$PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_GPUS = 1\n",
    "BS_PER_GPU = 128\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "NUM_CHANNELS = 3\n",
    "NUM_CLASSES = 8\n",
    "#NUM_TRAIN_SAMPLES = 40000\n",
    "\n",
    "BASE_LEARNING_RATE = 0.1\n",
    "LR_SCHEDULE = [(0.1, 30), (0.01, 45)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  return x, y\n",
    "\n",
    "\n",
    "def augmentation(x, y):\n",
    "    x = tf.image.resize_with_crop_or_pad(\n",
    "        x, IMG_HEIGHT + 8, IMG_WIDTH + 8)\n",
    "    x = tf.image.random_crop(x, [IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS])\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    return x, y\t\n",
    "\n",
    "\n",
    "def schedule(epoch):\n",
    "  initial_learning_rate = BASE_LEARNING_RATE * BS_PER_GPU / 128\n",
    "  learning_rate = initial_learning_rate\n",
    "  for mult, start_epoch in LR_SCHEDULE:\n",
    "    if epoch >= start_epoch:\n",
    "      learning_rate = initial_learning_rate * mult\n",
    "    else:\n",
    "      break\n",
    "  tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "  return learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names:  ['Anatomic landmarks' 'Unknown' 'Protruding lesions' 'Flat lesions'\n",
      " 'Lumen' 'Mucosa' 'Normal' 'Excavated lesions']\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/augere_export_class/')\n",
    "\n",
    "DATASET_SIZE = len(list(data_dir.glob('*/*.png')))\n",
    "class_names = np.array([item.name for item in data_dir.glob('*') if item.name != 'metadata.json'])\n",
    "print (\"Class names: \",class_names)\n",
    "\n",
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anatomic landmarks: 6868\n",
      "Unknown           : 274\n",
      "Protruding lesions: 583\n",
      "Flat lesions      : 908\n",
      "Lumen             : 1446\n",
      "Mucosa            : 251\n",
      "Normal            : 33129\n",
      "Excavated lesions : 1252\n",
      "\n",
      "Total number of images: 44711\n",
      "But the dataset is mainly shit\n"
     ]
    }
   ],
   "source": [
    "samples_per_class = []\n",
    "\n",
    "for class_name in class_names:\n",
    "    class_samples = len(list(data_dir.glob(class_name+'/*.png')))\n",
    "    samples_per_class.append(class_samples)\n",
    "    print('{0:18}: {1:3d}'.format(class_names, class_samples))\n",
    "\n",
    "print ('\\nTotal number of images:', DATASET_SIZE)\n",
    "\n",
    "# If one class contains more than half of the entire sample size\n",
    "if np.max(samples_per_class) > DATASET_SIZE//2:\n",
    "    print (\"But the dataset is mainly shit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_test(file_path):\n",
    "    # Not used, mainly for log\n",
    "    label = [i for i, s in enumerate(class_names) if 'Normal' in s]\n",
    "    return np.uint8(label)\n",
    "    \n",
    "def get_label_int(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # get class integer from class-list\n",
    "    label_int64 = tf.reduce_min(tf.where(tf.equal(parts[-2], class_names)))\n",
    "    # cast to tensor array with dtype=uint8\n",
    "    label_uint8 = tf.dtypes.cast(label_int64, tf.uint8)\n",
    "    return tf.reshape(label_uint8, [-1])\n",
    "\n",
    "def get_label_bool(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # The second to last is the class-directory\n",
    "    return parts[-2] == class_names\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label_int(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing an example image/label pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, label in labeled_ds.take(1):\n",
    "    plt.figure()\n",
    "    #plt.figure(frameon=False, facecolor='white')\n",
    "    fig = plt.imshow(image.numpy())\n",
    "    plt.axis('off')\n",
    "    print(\"Class:\",class_names[label.numpy()][0])\n",
    "    \n",
    "# print(next(iter(labeled_ds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "See https://lambdalabs.com/blog/tensorflow-2-0-tutorial-01-image-classification-basics/\n",
    "\n",
    "https://github.com/lambdal/TensorFlow2-tutorial/tree/master/01-basic-image-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into training, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "train_ds = labeled_ds.take(train_size)\n",
    "test_ds = labeled_ds.skip(train_size)\n",
    "val_ds = test_ds.skip(val_size)\n",
    "test_ds = test_ds.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset sample size:        44711\n",
      "Train dataset sample size:       31297\n",
      "Test dataset sample size:         6706\n",
      "Validation dataset sample size:   6708\n"
     ]
    }
   ],
   "source": [
    "def get_size(ds):\n",
    "    return tf.data.experimental.cardinality(ds).numpy()\n",
    "\n",
    "print (\"{:32} {:>5}\".format(\"Full dataset sample size:\", get_size(labeled_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Train dataset sample size:\", get_size(train_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Test dataset sample size:\", get_size(test_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Validation dataset sample size:\", get_size(val_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=1000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "      if isinstance(cache, str):\n",
    "        ds = ds.cache(cache)\n",
    "      else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    #ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Create training dataset\n",
    "train_ds = prepare_for_training(train_ds, cache=\"./cache/train_ds.tfcache\")\n",
    "# Create test dataset\n",
    "test_ds = prepare_for_training(test_ds, cache=\"./cache/test_ds.tfcache\")\n",
    "# Create validation dataset\n",
    "val_ds = prepare_for_training(val_ds, cache=\"./cache/val_ds.tfcache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "245/245 [==============================] - 25s 103ms/step - loss: 0.8399 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.0000e+00 - val_sparse_categorical_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "245/245 [==============================] - 14s 58ms/step - loss: 0.6879 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.6219 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "245/245 [==============================] - 14s 59ms/step - loss: 0.5650 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.5108 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "245/245 [==============================] - 14s 58ms/step - loss: 0.4641 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.4196 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "245/245 [==============================] - 14s 59ms/step - loss: 0.3812 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.3447 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "245/245 [==============================] - 14s 59ms/step - loss: 0.3131 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2831 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "245/245 [==============================] - 14s 59ms/step - loss: 0.2572 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.2325 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "245/245 [==============================] - 14s 59ms/step - loss: 0.2113 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1910 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "245/245 [==============================] - 14s 59ms/step - loss: 0.1735 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1569 - val_sparse_categorical_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "245/245 [==============================] - 15s 61ms/step - loss: 0.1425 - sparse_categorical_accuracy: 1.0000 - val_loss: 0.1289 - val_sparse_categorical_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(22)\n",
    "#train_dataset = train_dataset.map(augmentation).map(preprocess).shuffle(NUM_TRAIN_SAMPLES).batch(BS_PER_GPU * NUM_GPUS, drop_remainder=True)\n",
    "#test_dataset = test_dataset.map(preprocess).batch(BS_PER_GPU * NUM_GPUS, drop_remainder=True)\n",
    "\n",
    "input_shape = (IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)\n",
    "img_input = tf.keras.layers.Input(shape=input_shape)\n",
    "opt = keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "\n",
    "if NUM_GPUS == 1:\n",
    "    model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES)\n",
    "    model.compile(\n",
    "              optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['sparse_categorical_accuracy'])\n",
    "else:\n",
    "    mirrored_strategy = tf.distribute.MirroredStrategy()\n",
    "    with mirrored_strategy.scope():\n",
    "      model = resnet.resnet56(img_input=img_input, classes=NUM_CLASSES)\n",
    "      model.compile(\n",
    "                optimizer=opt,\n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['sparse_categorical_accuracy'])  \n",
    "\n",
    "log_dir=\"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "  log_dir=log_dir,\n",
    "  update_freq='batch',\n",
    "  histogram_freq=1)\n",
    "\n",
    "lr_schedule_callback = LearningRateScheduler(schedule)\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    validation_data=test_ds,\n",
    "    validation_freq=1,\n",
    "    #steps_per_epoch=245, if ds.repeat() should be ceil(num_samples/batch_size)\n",
    "    #validation_steps=245, sould be ceil(num_val_samples/batch_size)\n",
    "    callbacks=[tensorboard_callback, lr_schedule_callback])\n",
    "\n",
    "# Print record of loss and metric values during training\n",
    "# print('\\nhistory dict:', history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the results\n",
    "\n",
    "`Tensorboard`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PATH=/home/henrik/anaconda3/envs/TF2/bin:/home/henrik/anaconda3/bin:/home/henrik/anaconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/snap/bin\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs (move to .old instead?)\n",
    "# !rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "No known TensorBoard instances running.\n"
     ]
    }
   ],
   "source": [
    "from tensorboard import notebook\n",
    "\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "notebook.list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start tensorboard\n",
    "%tensorboard --logdir logs\n",
    "\n",
    "# !kill 20058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)\n",
    "\n",
    "model.save('model.h5')\n",
    "\n",
    "new_model = keras.models.load_model('model.h5')\n",
    " \n",
    "new_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(val_ds))\n",
    "image = image.numpy()\n",
    "print (\"True label:\", class_names[label.numpy()[0][0]])\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image[0])\n",
    "#print (image.numpy())\n",
    "res = model.predict(image)\n",
    "\n",
    "print (\"Predicted label:\", class_names[np.argmax(res[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "for lab in res:\n",
    "    print ('{:3} True:{:20} Pred:{}'.format(idx, class_names[label[idx]], class_names[np.argmax(lab)]))\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_classes(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
