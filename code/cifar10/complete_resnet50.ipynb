{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 dataset trained on all classes with resnet56 without augmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "MODEL = 'resnet50' \n",
    "DS_INFO = 'complete'\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 64\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "\n",
    "NUM_CHANNELS = 3\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)\n",
    "\n",
    "# epoch*batch_size*img_size\n",
    "model_name = '{}x{}x{}_{}_{}'.format(NUM_EPOCHS, BATCH_SIZE, IMG_WIDTH, DS_INFO, MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class names:  ['cat' 'airplane' 'deer' 'automobile' 'ship' 'horse' 'truck' 'dog' 'bird'\n",
      " 'frog']\n"
     ]
    }
   ],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/cifar10/')\n",
    "outcast = 'None'\n",
    "\n",
    "DATASET_SIZE = len(list(data_dir.glob('*/*/*.*g')))\n",
    "STEPS_PER_EPOCH = np.ceil(DATASET_SIZE/BATCH_SIZE)\n",
    "\n",
    "directories = np.array([item.name for item in data_dir.glob('train/*') if item.name != 'metadata.json'])\n",
    "\n",
    "class_names = directories\n",
    "NUM_CLASSES = len(directories)\n",
    "print (\"Class names: \", class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat               : 6000\n",
      "airplane          : 6000\n",
      "deer              : 6000\n",
      "automobile        : 6000\n",
      "ship              : 6000\n",
      "horse             : 6000\n",
      "truck             : 6000\n",
      "dog               : 6000\n",
      "bird              : 6000\n",
      "frog              : 6000\n",
      "\n",
      "Total number of images: 60000, in 10 classes\n"
     ]
    }
   ],
   "source": [
    "samples_per_class = []\n",
    "for class_name in class_names:\n",
    "    class_samples = len(list(data_dir.glob('*/'+class_name+'/*.*g')))\n",
    "    samples_per_class.append(class_samples)\n",
    "    print('{0:18}: {1:3d}'.format(class_name, class_samples))\n",
    "\n",
    "print ('\\nTotal number of images: {}, in {} classes'.format(DATASET_SIZE, NUM_CLASSES))\n",
    "\n",
    "# If one class contains more than half of the entire sample size\n",
    "if np.max(samples_per_class) > DATASET_SIZE//2:\n",
    "    print (\"But the dataset is mainly shit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images with `tf.data.Dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of the file paths\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'*/*/*.*g'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short pure-tensorflow function that converts a file path to an `image_data, label` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    # convert the path to a list of path components\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # get class integer from class-list\n",
    "    class_ = parts[-2], class_names\n",
    "    label_int = tf.reduce_min(tf.where(tf.equal(parts[-2], class_names)))\n",
    "    # cast to tensor array with dtype=uint8\n",
    "    return tf.dtypes.cast(label_int, tf.int32)\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "# Set 'num_parallel_calls' so multiple images are loaded and processed in parallel\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing an example image/label pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image(img):\n",
    "    if (isinstance(img, tf.data.Dataset)):\n",
    "        for image, label in img:\n",
    "            plt.figure(frameon=False, facecolor='white')\n",
    "            title = class_names[label.numpy()]#+\" [\"+str(label.numpy())+\"]\"\n",
    "            plt.title(title, fontdict={'color':'white','size':20})\n",
    "            plt.imshow(image.numpy())\n",
    "            plt.axis('off')\n",
    "    else:\n",
    "        plt.figure(frameon=False, facecolor='white')\n",
    "        plt.title(\"None\", fontdict={'color':'white','size':20})\n",
    "        plt.imshow(img.numpy())\n",
    "        plt.axis('off')\n",
    "\n",
    "# Take one image\n",
    "show_image(labeled_ds.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare dataset for training\n",
    "Want the data to be shuffled and batched. Here we use the `tf.data` api."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into training, test and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "train_ds = labeled_ds.take(train_size)\n",
    "test_ds = labeled_ds.skip(train_size)\n",
    "val_ds = test_ds.skip(val_size)\n",
    "test_ds = test_ds.take(test_size)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def augment(img, label):\n",
    "    # Augment the image using tf.image\n",
    "    # Standardize\n",
    "    img = tf.image.per_image_standardization(img)\n",
    "    # Pad with 8 pixels\n",
    "    img = tf.image.resize_with_crop_or_pad(img, IMG_HEIGHT + 8, IMG_WIDTH + 8)\n",
    "    # Randomly crop the image back to original size\n",
    "    img = tf.image.random_crop(img, [IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS])\n",
    "    # Randomly flip image\n",
    "    img = tf.image.random_flip_left_right(img)\n",
    "    return img, label\n",
    "\n",
    "# Augment the training data\n",
    "train_ds = train_ds.map(augment, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset sample size:        60000\n",
      "Train dataset sample size:       42000\n",
      "Test dataset sample size:         9000\n",
      "Validation dataset sample size:   9000\n"
     ]
    }
   ],
   "source": [
    "def get_size(ds):\n",
    "    return tf.data.experimental.cardinality(ds).numpy()\n",
    "\n",
    "print (\"{:32} {:>5}\".format(\"Full dataset sample size:\", get_size(labeled_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Train dataset sample size:\", get_size(train_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Test dataset sample size:\", get_size(test_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Validation dataset sample size:\", get_size(val_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=3000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "      if isinstance(cache, str):\n",
    "        ds = ds.cache(cache)\n",
    "      else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Create training, test and validation dataset\n",
    "train_ds = prepare_for_training(train_ds, cache=\"./cache/{}_train.tfcache\".format(IMG_WIDTH))\n",
    "test_ds = prepare_for_training(test_ds, cache=\"./cache/{}_test.tfcache\".format(IMG_WIDTH))\n",
    "val_ds = prepare_for_training(val_ds, cache=\"./cache/{}_val.tfcache\".format(IMG_WIDTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50 transfer learning\n",
    "https://adventuresinmachinelearning.com/transfer-learning-tensorflow-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net = tf.keras.applications.ResNet50(\n",
    "                weights='imagenet', \n",
    "                include_top=False, \n",
    "                input_shape=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net.trainable = False\n",
    "\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "output_layer = layers.Dense(NUM_CLASSES, activation='softmax')\n",
    "\n",
    "resnet50_model = tf.keras.Sequential([\n",
    "        res_net,\n",
    "        global_average_layer,\n",
    "        output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 1, 1, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                20490     \n",
      "=================================================================\n",
      "Total params: 23,608,202\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet50_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save image of layers\n",
    "tf.keras.utils.plot_model(resnet50_model, 'models/{}.png'.format(model_name), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "resnet50_model.compile(\n",
    "        optimizer=opt,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(\n",
    "                log_dir='./logs/resnet50_model', \n",
    "                update_freq='batch')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 656 steps, validate for 140 steps\n",
      "Epoch 1/20\n",
      "656/656 [==============================] - 19s 29ms/step - loss: 11.1915 - accuracy: 0.1011 - val_loss: 10.1352 - val_accuracy: 0.1035\n",
      "Epoch 2/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.0275 - accuracy: 0.1012 - val_loss: 10.0169 - val_accuracy: 0.1033\n",
      "Epoch 3/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 9.9975 - accuracy: 0.1011 - val_loss: 10.0118 - val_accuracy: 0.1035\n",
      "Epoch 4/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.0032 - accuracy: 0.1011 - val_loss: 10.0849 - val_accuracy: 0.1035\n",
      "Epoch 5/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.3016 - accuracy: 0.1012 - val_loss: 10.0783 - val_accuracy: 0.1035\n",
      "Epoch 6/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.0278 - accuracy: 0.1013 - val_loss: 10.0321 - val_accuracy: 0.1032\n",
      "Epoch 7/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1531 - accuracy: 0.1013 - val_loss: 10.1032 - val_accuracy: 0.1035\n",
      "Epoch 8/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1120 - accuracy: 0.1011 - val_loss: 10.0325 - val_accuracy: 0.1037\n",
      "Epoch 9/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.0786 - accuracy: 0.1011 - val_loss: 10.0737 - val_accuracy: 0.1035\n",
      "Epoch 10/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1758 - accuracy: 0.1013 - val_loss: 10.1644 - val_accuracy: 0.1036\n",
      "Epoch 11/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.0831 - accuracy: 0.1010 - val_loss: 10.1131 - val_accuracy: 0.1037\n",
      "Epoch 12/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1310 - accuracy: 0.1011 - val_loss: 10.1681 - val_accuracy: 0.1037\n",
      "Epoch 13/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1049 - accuracy: 0.1012 - val_loss: 10.0341 - val_accuracy: 0.1036\n",
      "Epoch 14/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1278 - accuracy: 0.1013 - val_loss: 10.0692 - val_accuracy: 0.1032\n",
      "Epoch 15/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.0976 - accuracy: 0.1010 - val_loss: 10.0697 - val_accuracy: 0.1033\n",
      "Epoch 16/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1330 - accuracy: 0.1013 - val_loss: 10.1176 - val_accuracy: 0.1033\n",
      "Epoch 17/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1184 - accuracy: 0.1010 - val_loss: 10.1721 - val_accuracy: 0.1037\n",
      "Epoch 18/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1175 - accuracy: 0.1013 - val_loss: 10.0383 - val_accuracy: 0.1038\n",
      "Epoch 19/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.0848 - accuracy: 0.1013 - val_loss: 10.0455 - val_accuracy: 0.1035\n",
      "Epoch 20/20\n",
      "656/656 [==============================] - 12s 18ms/step - loss: 10.1688 - accuracy: 0.1008 - val_loss: 10.0881 - val_accuracy: 0.1035\n"
     ]
    }
   ],
   "source": [
    "history = resnet50_model.fit(\n",
    "        train_ds,\n",
    "        steps_per_epoch = train_size // BATCH_SIZE,\n",
    "        epochs = NUM_EPOCHS,\n",
    "        validation_data = test_ds,\n",
    "        validation_steps = test_size // BATCH_SIZE,\n",
    "        validation_freq = 1,\n",
    "        callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras`\n",
    "Save/load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet50_model.save('models/{}.h5'.format(model_name))\n",
    "# resnet50_model = tf.keras.models.load_model('models/{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140/140 - 6s - loss: 9.9699 - accuracy: 0.1018\n"
     ]
    }
   ],
   "source": [
    "resnet50_evaluate = resnet50_model.evaluate(val_ds, verbose=2, steps=val_size//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sparse_categorical_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-408654d96a64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sparse_categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_sparse_categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sparse_categorical_accuracy'"
     ]
    }
   ],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(NUM_EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorboard`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorboard import notebook\n",
    "# Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Start tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Clear any logs from previous runs (move to .old instead?)\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# Stop tensorboard\n",
    "notebook.list()\n",
    "!kill 20058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one batch\n",
    "images, labels = next(iter(val_ds))\n",
    "\n",
    "# Convert from tensor to numpy array\n",
    "images = images.numpy()\n",
    "labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image and label\n",
    "rand = np.random.randint(0, BATCH_SIZE)\n",
    "image = images[rand]\n",
    "label = labels[rand]\n",
    "\n",
    "# Predict one image\n",
    "predictions = resnet50_model.predict(np.expand_dims(image, axis=0))[0]\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(\"{:0.4f} {}\".format(pred,class_names[i]))\n",
    "\n",
    "print (\"\\nLabel:\", class_names[label])\n",
    "print (\"Predicton:\", class_names[np.argmax(predictions)])\n",
    "\n",
    "plt.figure(frameon=False, facecolor='white')\n",
    "plt.imshow(image)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict one batch\n",
    "predictions = resnet50_model.predict(images)\n",
    "\n",
    "print ('{:3}  {:10}  {:3}'.format('idx', 'label', 'pred'))\n",
    "print ('---  -------     --------')\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    label = class_names[labels[i]]\n",
    "    prediction = class_names[np.argmax(pred)]\n",
    "    print ('\\n{:3}  {:10}  {:10}'.format(i, label, prediction), end='')\n",
    "    if (label != prediction): print (\"  Wrong\", end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
