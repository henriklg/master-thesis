{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kvasir dataset split into neg/pos and trained using resnet50 without augmentation. With ds resampling (see https://www.tensorflow.org/guide/data#resampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL = 'resnet50' \n",
    "DS_INFO = 'binary'\n",
    "NUM_EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "IMG_HEIGHT = 32\n",
    "IMG_WIDTH = 32\n",
    "\n",
    "NUM_CHANNELS = 3\n",
    "IMG_SIZE = (IMG_HEIGHT, IMG_WIDTH, NUM_CHANNELS)\n",
    "\n",
    "# epoch*batch_size*img_size\n",
    "model_name = '{}x{}x{}_{}_{}'.format(NUM_EPOCHS, BATCH_SIZE, IMG_WIDTH, DS_INFO, MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_info(directories_ex_outcast, class_names, neg, pos):\n",
    "    # Extract and print info about the class split \n",
    "    \n",
    "    for i, class_ in enumerate([neg,pos]):\n",
    "        print (\"{} class names:\".format(class_names[i]))\n",
    "        for cl in class_:\n",
    "            print (\"{}- {}\".format(\" \"*8, cl))\n",
    "    \n",
    "    neg_count = pos_count = 0\n",
    "    for dir_name in directories_ex_outcast:\n",
    "        # Number of samples in 'class_name' folder\n",
    "        class_samples = len(list(data_dir.glob(dir_name+'/*.*g')))\n",
    "\n",
    "        if (dir_name == neg_class_name[0]):\n",
    "            neg_count += class_samples\n",
    "        else:\n",
    "            pos_count += class_samples\n",
    "\n",
    "    print ('\\nNegative samples: {0:5} | {1:5.2f}%'.format(neg_count, neg_count/DATASET_SIZE*100))\n",
    "    print ('Positive samples: {0:5} | {1:5.2f}%'.format(pos_count, pos_count/DATASET_SIZE*100))\n",
    "    # Print number of images in dataset (excluded samples in outcast)\n",
    "    print ('\\nTotal number of images:', DATASET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/cifar10/train/')\n",
    "outcast = 'None'\n",
    "\n",
    "DATASET_SIZE = len(list(data_dir.glob('[!'+str(outcast)+']*/*.*g')))\n",
    "directories = np.array([item.name for item in data_dir.glob('*') if item.name != 'metadata.json'])\n",
    "print (\"Directories: \", directories)\n",
    "\n",
    "# Remove outcasts\n",
    "directories_ex_outcast = np.delete(directories, np.where(outcast == directories))\n",
    "print (\"\\nRemoved outcast: \", outcast, \"\\n\")\n",
    "\n",
    "class_names = np.array(['Negative','Positive'])\n",
    "NUM_CLASSES = len(class_names)\n",
    "neg_class_name = ['ship'] # 'normal'-class\n",
    "pos_class_names = np.delete(directories_ex_outcast, np.where(neg_class_name == directories_ex_outcast))\n",
    "\n",
    "print_class_info(directories_ex_outcast, class_names, neg_class_name, pos_class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up pipeline for loading images from given list of paths, using `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of the file paths | data_dir/*/* but subract class\n",
    "list_ds = tf.data.Dataset.list_files(str(data_dir/'[!')+str(outcast+']*/*'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert a file path to an `image_data, label` pair\n",
    "\n",
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    bc = parts[-2] == pos_class_names\n",
    "    nz_cnt = tf.math.count_nonzero(bc)\n",
    "    if (nz_cnt > 0):\n",
    "        return tf.constant(1, tf.int32)\n",
    "    return tf.constant(0, tf.int32)\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [IMG_WIDTH, IMG_HEIGHT])\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label\n",
    "\n",
    "labeled_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Showing an example image/label pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image(img):\n",
    "    if (isinstance(img, tf.data.Dataset)):\n",
    "        for image, label in img:\n",
    "            plt.figure(frameon=False, facecolor='white')\n",
    "            title = class_names[label.numpy()]+\" [\"+str(label.numpy())+\"]\"\n",
    "            plt.title(title, fontdict={'color':'white','size':20})\n",
    "            plt.imshow(image.numpy())\n",
    "            plt.axis('off')\n",
    "    else:\n",
    "        plt.figure(frameon=False, facecolor='white')\n",
    "        plt.title(\"None\", fontdict={'color':'white','size':20})\n",
    "        plt.imshow(img.numpy())\n",
    "        plt.axis('off')\n",
    "\n",
    "# Take one image\n",
    "show_image(labeled_ds.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resampling the dataset to a 50/50 distribution\n",
    "https://www.tensorflow.org/guide/data#resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make large batches to be certain a good amount of minority class makes it in\n",
    "labeled_ds = labeled_ds.batch(1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(counts, batch):\n",
    "        images, labels = batch\n",
    "        \n",
    "        class_1 = labels == 1\n",
    "        class_1 = tf.cast(class_1, tf.int32)\n",
    "\n",
    "        class_0 = labels == 0\n",
    "        class_0 = tf.cast(class_0, tf.int32)\n",
    "\n",
    "        counts['class_0'] += tf.reduce_sum(class_0)\n",
    "        counts['class_1'] += tf.reduce_sum(class_1)\n",
    "\n",
    "        return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fractions:  [0.8936523  0.10634766]\n",
      "Counts:  [9151. 1089.]\n"
     ]
    }
   ],
   "source": [
    "counts = labeled_ds.take(10).reduce(\n",
    "        initial_state={'class_0': 0, 'class_1': 0},\n",
    "        reduce_func = count)\n",
    "\n",
    "counts = np.array([counts['class_0'].numpy(),\n",
    "                   counts['class_1'].numpy()]).astype(np.float32)\n",
    "\n",
    "counts_sum = counts.sum()\n",
    "assert counts_sum != 0, \"Can't divide by zero\"\n",
    "\n",
    "fractions = counts/counts_sum\n",
    "print(\"Fractions: \", fractions)\n",
    "print(\"Counts: \", counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_ds = labeled_ds.unbatch().filter(lambda image, label: label==0).repeat()\n",
    "positive_ds = labeled_ds.unbatch().filter(lambda image, label: label==1).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_ds = tf.data.experimental.sample_from_datasets(\n",
    "    [negative_ds, positive_ds], [0.5, 0.5]).batch(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 0 1 1 1 1]\n",
      "[0 1 0 0 0 0 0 1 0 1]\n",
      "[1 1 1 1 1 0 0 1 1 1]\n",
      "[0 0 0 0 0 1 1 1 1 1]\n",
      "[0 1 1 1 0 0 0 1 0 0]\n",
      "[1 0 0 1 0 0 1 1 1 1]\n",
      "[0 0 0 0 1 0 0 1 1 1]\n",
      "[0 1 1 1 1 1 0 1 1 0]\n",
      "[0 0 1 0 0 1 1 1 1 0]\n",
      "[0 0 1 0 1 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "for features, labels in balanced_ds.take(10):\n",
    "  print(labels.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49 0.51]\n"
     ]
    }
   ],
   "source": [
    "# Verify that it has been resampled\n",
    "counts = balanced_ds.take(30).reduce(\n",
    "        initial_state={'class_0': 0, 'class_1': 0},\n",
    "        reduce_func = count)\n",
    "\n",
    "counts = np.array([counts['class_0'].numpy(),\n",
    "                   counts['class_1'].numpy()]).astype(np.float32)\n",
    "\n",
    "print(counts/counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting into training, test and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.7 * DATASET_SIZE)\n",
    "val_size = int(0.15 * DATASET_SIZE)\n",
    "test_size = int(0.15 * DATASET_SIZE)\n",
    "\n",
    "shuffled_ds = balanced_ds.unbatch()#.shuffle(buffer_size=10000)\n",
    "\n",
    "train_ds = shuffled_ds.take(train_size)\n",
    "test_ds = shuffled_ds.skip(train_size)\n",
    "val_ds = test_ds.skip(val_size)\n",
    "test_ds = test_ds.take(test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset sample size:           -2\n",
      "Train dataset sample size:          -2\n",
      "Test dataset sample size:           -2\n",
      "Validation dataset sample size:     -2\n"
     ]
    }
   ],
   "source": [
    "# Print info about the dataset split\n",
    "def get_size(ds):\n",
    "    return tf.data.experimental.cardinality(ds).numpy()\n",
    "\n",
    "print (\"{:32} {:>5}\".format(\"Full dataset sample size:\", get_size(shuffled_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Train dataset sample size:\", get_size(train_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Test dataset sample size:\", get_size(test_ds)))\n",
    "print (\"{:32} {:>5}\".format(\"Validation dataset sample size:\", get_size(val_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_training(ds, cache=True, shuffle_buffer_size=3000):\n",
    "    # This is a small dataset, only load it once, and keep it in memory.\n",
    "    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
    "    # fit in memory.\n",
    "    if cache:\n",
    "      if isinstance(cache, str):\n",
    "        ds = ds.cache(cache)\n",
    "      else:\n",
    "        ds = ds.cache()\n",
    "\n",
    "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "\n",
    "    # Repeat forever\n",
    "    ds = ds.repeat()\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "    # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "    # is training.\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Create training, test and validation dataset\n",
    "train_ds = prepare_for_training(train_ds, cache=\"./cache/{}_train.tfcache\".format(IMG_WIDTH))\n",
    "test_ds = prepare_for_training(test_ds, cache=\"./cache/{}_test.tfcache\".format(IMG_WIDTH))\n",
    "val_ds = prepare_for_training(val_ds, cache=\"./cache/{}_val.tfcache\".format(IMG_WIDTH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnet50 transfer learning\n",
    "https://adventuresinmachinelearning.com/transfer-learning-tensorflow-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net.trainable = False\n",
    "\n",
    "global_average_layer = layers.GlobalAveragePooling2D()\n",
    "output_layer = layers.Dense(1, activation='softmax')\n",
    "\n",
    "tl_model = tf.keras.Sequential([\n",
    "        res_net,\n",
    "        global_average_layer,\n",
    "        output_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 1, 1, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 2049      \n",
      "=================================================================\n",
      "Total params: 23,589,761\n",
      "Trainable params: 2,049\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tl_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save image of layers\n",
    "tf.keras.utils.plot_model(tl_model, 'models/{}.png'.format(model_name), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "callbacks = [tf.keras.callbacks.TensorBoard(log_dir='./logs/transfer_learning_model', update_freq='batch')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 546 steps, validate for 117 steps\n",
      "Epoch 1/10\n",
      "546/546 [==============================] - 13s 23ms/step - loss: nan - accuracy: 0.4954 - val_loss: nan - val_accuracy: 0.5025\n",
      "Epoch 2/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4955 - val_loss: nan - val_accuracy: 0.5021\n",
      "Epoch 3/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4956 - val_loss: nan - val_accuracy: 0.5025\n",
      "Epoch 4/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4957 - val_loss: nan - val_accuracy: 0.5023\n",
      "Epoch 5/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4957 - val_loss: nan - val_accuracy: 0.5023\n",
      "Epoch 6/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4950 - val_loss: nan - val_accuracy: 0.5025\n",
      "Epoch 7/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4963 - val_loss: nan - val_accuracy: 0.5028\n",
      "Epoch 8/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4954 - val_loss: nan - val_accuracy: 0.5025\n",
      "Epoch 9/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4951 - val_loss: nan - val_accuracy: 0.5027\n",
      "Epoch 10/10\n",
      "546/546 [==============================] - 10s 18ms/step - loss: nan - accuracy: 0.4958 - val_loss: nan - val_accuracy: 0.5021\n"
     ]
    }
   ],
   "source": [
    "history = tl_model.fit(\n",
    "        train_ds,\n",
    "        steps_per_epoch = train_size // BATCH_SIZE,\n",
    "        epochs = NUM_EPOCHS,\n",
    "        validation_data = test_ds,\n",
    "        validation_steps = test_size // BATCH_SIZE,\n",
    "        validation_freq = 1,\n",
    "        callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras`\n",
    "Save/load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_model.save('models/{}.h5'.format(model_name))\n",
    "# tl_model = tf.keras.models.load_model('models/{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_evaluate = tl_model.evaluate(val_ds, verbose=2, steps=val_size//BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(NUM_EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorboard`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorboard import notebook\n",
    "# Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Start tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Clear any logs from previous runs (move to .old instead?)\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# Stop tensorboard\n",
    "notebook.list()\n",
    "!kill 20058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one batch\n",
    "images, labels = next(iter(val_ds))\n",
    "\n",
    "# Convert from tensor to numpy array\n",
    "images = images.numpy()\n",
    "labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image and label\n",
    "rand = np.random.randint(0, BATCH_SIZE)\n",
    "image = images[rand]\n",
    "label = labels[rand]\n",
    "\n",
    "# Predict one image\n",
    "predictions = cnn_model.predict(np.expand_dims(image, axis=0))[0]\n",
    "\n",
    "for i, pred in enumerate(predictions, start=-1):\n",
    "    print(\"{:0.4f} | {}\".format(pred, class_names[np.abs(i)]))\n",
    "\n",
    "prediction = ('Boat') if np.argmax(predictions)==1 else ('Not boat')\n",
    "\n",
    "print (\"Image {} of {}\".format(rand, BATCH_SIZE))\n",
    "\n",
    "plt.figure(frameon=False, facecolor='white');\n",
    "plt.title(prediction, fontdict={'color':'white','size':20})\n",
    "plt.imshow(image)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict one batch\n",
    "predictions = cnn_model.predict(images)\n",
    "\n",
    "print ('{:3}  {:5}  {:3}'.format('idx', 'label', 'pred'))\n",
    "print ('---  -----  ----')\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    label = class_names[labels[i]][0:3]\n",
    "    prediction = class_names[np.argmin(pred)][0:3]\n",
    "    print ('\\n{:3}  {:5}  {:5}'.format(i, label, prediction), end='')\n",
    "    if (label != prediction): print (\"Wrong\", end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
