{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kvasir dataset split into neg/pos and trained using Resnet50 without augmentation. Getting some decent results after training on resampled data with large step-size.  \n",
    "- Class weighting  \n",
    "- Resampling  \n",
    "- Initial Bias-estimation\n",
    "- Decreasing learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some stuff to make utils-function work\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from data_prep import create_dataset, print_class_info, show_image\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/hyper-kvasir/labeled/')\n",
    "\n",
    "config = {\n",
    "    # Dataset\n",
    "    \"data_dir\": data_dir,\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"ds_info\": 'complete',\n",
    "    \"resample\": False,\n",
    "    \"neg_class\": ['normal-cecum'],\n",
    "    \"outcast\": None,\n",
    "    # Model\n",
    "    \"model\": 'EfficientNetB3',\n",
    "    \"num_epochs\": 40,\n",
    "    \"batch_size\": 128,\n",
    "    \"img_shape\": (128, 128, 3),\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"final_activation\": 'softmax',\n",
    "    # Callbacks\n",
    "    \"learning_schedule\": False,\n",
    "    \"early_stopping\": True,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"decay_rate\": 0.6,              # higher number gives steeper dropoff\n",
    "    # Misc\n",
    "    \"verbosity\": 1\n",
    "    }\n",
    "\n",
    "model_name = '{}x{}x{}_{}_{}'.format(config[\"num_epochs\"], config[\"batch_size\"], \n",
    "                                     config[\"img_shape\"][1], config[\"ds_info\"], config[\"model\"])\n",
    "\n",
    "fine_tune_from = 130\n",
    "fine_tune_epochs = 30\n",
    "early_stopping_patience = config[\"early_stopping_patience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, testing and validation dataset from utils/data_prep.py.  \n",
    "Returns tf.dataset for shuffled, cached and batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemorrhoids                 :    6 | 0.06%\n",
      "barretts                    :   41 | 0.38%\n",
      "esophagitis-a               :  403 | 3.78%\n",
      "esophagitis-b-d             :  260 | 2.44%\n",
      "ulcerative-colitis-0-1      :   35 | 0.33%\n",
      "barretts-short-segment      :   53 | 0.50%\n",
      "cecum                       : 1009 | 9.46%\n",
      "pylorus                     :  999 | 9.37%\n",
      "retroflex-rectum            :  391 | 3.67%\n",
      "ulcerative-colitis-grade-2  :  443 | 4.15%\n",
      "ulcerative-colitis-grade-1  :  201 | 1.89%\n",
      "bbps-2-3                    : 1148 | 10.77%\n",
      "bbps-0-1                    :  646 | 6.06%\n",
      "ileum                       :    9 | 0.08%\n",
      "retroflex-stomach           :  764 | 7.17%\n",
      "normal-z-line               :  932 | 8.74%\n",
      "ulcerative-colitis-2-3      :   28 | 0.26%\n",
      "impacted-stool              :  131 | 1.23%\n",
      "polyps                      : 1028 | 9.64%\n",
      "dyed-resection-margins      :  989 | 9.28%\n",
      "dyed-lifted-polyps          : 1002 | 9.40%\n",
      "ulcerative-colitis-grade-3  :  133 | 1.25%\n",
      "ulcerative-colitis-1-2      :   11 | 0.10%\n",
      "\n",
      "Total number of images: 10662, in 23 classes.\n",
      "\n",
      "Dataset.list_files:  /mnt/sdb/hyper-kvasir/labeled/*/*.*g \n",
      "\n",
      "WARNING:tensorflow:Entity <function create_dataset.<locals>.get_label at 0x7f026af99b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Cell is empty\n",
      "WARNING: Entity <function create_dataset.<locals>.get_label at 0x7f026af99b90> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Cell is empty\n",
      "[ 6 10 20 19 18  8 10 21 17 20]\n",
      "[15 14 11  9 15  6 19  3 18 11]\n",
      "[ 6 15 11  2 21 19  7 12 20  7]\n",
      "[14  6  9  9  6 18 11 18 12 11]\n",
      "[11 15 18 18 19  7  3  7 18 20]\n",
      "[17 15  3  6 15 14 18 11 14  1]\n",
      "[15  4 11  3  8 14 15 18 20  7]\n",
      "[11 11 12 14  1  7  6 11 11  7]\n",
      "[12 11  9  9  6 11 19 11  8  9]\n",
      "[18 18 11 11 18  2 19 11 11 10]\n",
      "\n",
      "Full dataset sample size:        10662\n",
      "Train dataset sample size:        7463\n",
      "Test dataset sample size:         1599\n",
      "Validation dataset sample size:   1600\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds, val_ds, params = create_dataset(config)\n",
    "\n",
    "train_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "test_steps = params[\"test_size\"] // config[\"batch_size\"]\n",
    "val_steps = params[\"val_size\"] // config[\"batch_size\"]\n",
    "class_names = params[\"class_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Train a teacher model on labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('./efficientnet_keras_transfer_learning/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from efficientnet import EfficientNetB3 as Net\n",
    "from efficientnet import center_crop_and_resize, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_base = Net(\n",
    "    weights=\"imagenet\", \n",
    "    include_top=False, \n",
    "    input_shape=config[\"img_shape\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers in resnet\n",
    "efficientnet_base.trainable = True\n",
    "\n",
    "# Define model\n",
    "en_model = Sequential()\n",
    "\n",
    "en_model.add(efficientnet_base)\n",
    "en_model.add(layers.GlobalAveragePooling2D())\n",
    "en_model.add(layers.Dense(params[\"num_classes\"], activation=config[\"final_activation\"]))\n",
    "\n",
    "if config['optimizer'] == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "en_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b3 (Model)      (None, 4, 4, 1536)        10783528  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 23)                35351     \n",
      "=================================================================\n",
      "Total params: 10,818,879\n",
      "Trainable params: 10,731,583\n",
      "Non-trainable params: 87,296\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "en_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using LearnignRateScheduler\n",
    "initial_learning_rate = config[\"learning_rate\"]\n",
    "decay_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "batch_size = config['batch_size']\n",
    "decay_rate = config['decay_rate']\n",
    "\n",
    "def schedule(epoch):\n",
    "    # calculate new learning rate\n",
    "    learning_rate = initial_learning_rate / (1 + decay_rate * (epoch*batch_size) / decay_steps)\n",
    "    \n",
    "    # update tensorboard\n",
    "    tf.summary.scalar(name='learning_rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "log_dir=\"./logs/{}/\".format(config[\"model\"]) + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "lr_schedule_cb = LearningRateScheduler(schedule, verbose=1)\n",
    "earlystopp_cb = EarlyStopping(monitor='val_loss',verbose=1, patience=early_stopping_patience, restore_best_weights=True)\n",
    "checkpoint_cb = ModelCheckpoint(filepath='./models/best_cp-{epoch:03d}.hdf', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "\n",
    "callbacks = [tensorboard_cb]\n",
    "if config[\"early_stopping\"]: callbacks.append(earlystopp_cb)\n",
    "if config[\"learning_schedule\"]: callbacks.append(lr_schedule_cb)\n",
    "\n",
    "# Write config dictionary to text file\n",
    "f = open(log_dir+\"/config.txt\",\"w\")\n",
    "f.write(str(config))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 58 steps, validate for 12 steps\n",
      "Epoch 1/40\n",
      "58/58 [==============================] - 10s 166ms/step - loss: 0.3891 - sparse_categorical_accuracy: 0.8700 - val_loss: 0.3592 - val_sparse_categorical_accuracy: 0.9004\n",
      "Epoch 2/40\n",
      "58/58 [==============================] - 9s 161ms/step - loss: 0.4044 - sparse_categorical_accuracy: 0.8697 - val_loss: 0.3566 - val_sparse_categorical_accuracy: 0.8991\n",
      "Epoch 3/40\n",
      "58/58 [==============================] - 9s 159ms/step - loss: 0.4066 - sparse_categorical_accuracy: 0.8679 - val_loss: 0.3610 - val_sparse_categorical_accuracy: 0.8978\n",
      "Epoch 4/40\n",
      "58/58 [==============================] - 9s 160ms/step - loss: 0.3933 - sparse_categorical_accuracy: 0.8697 - val_loss: 0.3563 - val_sparse_categorical_accuracy: 0.9004\n",
      "Epoch 5/40\n",
      "58/58 [==============================] - 9s 161ms/step - loss: 0.3870 - sparse_categorical_accuracy: 0.8734 - val_loss: 0.3566 - val_sparse_categorical_accuracy: 0.9030\n",
      "Epoch 6/40\n",
      "58/58 [==============================] - 9s 159ms/step - loss: 0.3896 - sparse_categorical_accuracy: 0.8677 - val_loss: 0.3535 - val_sparse_categorical_accuracy: 0.8997\n",
      "Epoch 7/40\n",
      "58/58 [==============================] - 10s 166ms/step - loss: 0.3824 - sparse_categorical_accuracy: 0.8735 - val_loss: 0.3496 - val_sparse_categorical_accuracy: 0.9017\n",
      "Epoch 8/40\n",
      "58/58 [==============================] - 9s 159ms/step - loss: 0.3903 - sparse_categorical_accuracy: 0.8724 - val_loss: 0.3529 - val_sparse_categorical_accuracy: 0.8991\n",
      "Epoch 9/40\n",
      "58/58 [==============================] - 9s 158ms/step - loss: 0.3901 - sparse_categorical_accuracy: 0.8675 - val_loss: 0.3544 - val_sparse_categorical_accuracy: 0.8991\n",
      "Epoch 10/40\n",
      "58/58 [==============================] - 9s 160ms/step - loss: 0.3837 - sparse_categorical_accuracy: 0.8706 - val_loss: 0.3434 - val_sparse_categorical_accuracy: 0.9023\n",
      "Epoch 11/40\n",
      "58/58 [==============================] - 9s 158ms/step - loss: 0.3803 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.3483 - val_sparse_categorical_accuracy: 0.9004\n",
      "Epoch 12/40\n",
      "58/58 [==============================] - 9s 160ms/step - loss: 0.3659 - sparse_categorical_accuracy: 0.8796 - val_loss: 0.3522 - val_sparse_categorical_accuracy: 0.9036\n",
      "Epoch 13/40\n",
      "58/58 [==============================] - 9s 160ms/step - loss: 0.3647 - sparse_categorical_accuracy: 0.8774 - val_loss: 0.3430 - val_sparse_categorical_accuracy: 0.8984\n",
      "Epoch 14/40\n",
      "58/58 [==============================] - 9s 159ms/step - loss: 0.3763 - sparse_categorical_accuracy: 0.8755 - val_loss: 0.3488 - val_sparse_categorical_accuracy: 0.8965\n",
      "Epoch 15/40\n",
      "58/58 [==============================] - 9s 160ms/step - loss: 0.3660 - sparse_categorical_accuracy: 0.8789 - val_loss: 0.3387 - val_sparse_categorical_accuracy: 0.8984\n",
      "Epoch 16/40\n",
      "58/58 [==============================] - 9s 160ms/step - loss: 0.3726 - sparse_categorical_accuracy: 0.8704 - val_loss: 0.3480 - val_sparse_categorical_accuracy: 0.8952\n",
      "Epoch 17/40\n",
      "13/58 [=====>........................] - ETA: 5s - loss: 0.3667 - sparse_categorical_accuracy: 0.8840"
     ]
    }
   ],
   "source": [
    "history = en_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = train_steps,\n",
    "    epochs = config[\"num_epochs\"],\n",
    "    validation_data = test_ds,\n",
    "    validation_steps = test_steps,\n",
    "    validation_freq = 1,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_acc =  str(history.history[\"val_sparse_categorical_accuracy\"][-1])[2:4]\n",
    "en_model.save('./model/{}.h5'.format(model_name+best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_evaluate = en_model.evaluate(val_ds, verbose=2, steps=val_steps)\n",
    "\n",
    "# Write evaluate dictionary to text file\n",
    "f = open(log_dir+\"/evaluate.txt\",\"w\")\n",
    "f.write( str(en_evaluate) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# lr = history.history['lr']\n",
    "epochs_range = range(history.epoch[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning rate\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(epochs_range, lr, label='Learning Rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learnign rate')\n",
    "plt.savefig(log_dir+'/learning_rate.png')\n",
    "plt.title('Adaptive Learning Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-val accuracy and loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0.0, 3])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(log_dir+'/accuracy_and_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the predictions on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch of validation data\n",
    "for images, labels in val_ds.take(1):\n",
    "    # Take one image and convert it to numpy\n",
    "    img = images.numpy()[0]\n",
    "    lab = labels.numpy()[0]\n",
    "    # Add one dimension\n",
    "    print (\"label:\", class_names[lab], end='\\n\\n')\n",
    "    show_image(img)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "        if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # the last item of parts is the filename\n",
    "    filename = parts[-1]\n",
    "    print (type(filename))\n",
    "    return filename\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [config[\"img_shape\"][0], config[\"img_shape\"][1] ])\n",
    "\n",
    "def process_path(file_path):\n",
    "    filename = get_filename(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the unlabeled `test` dataset (which are images taken from the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir_unlabeled_test = pathlib.Path('/mnt/sdb/hyper-kvasir/unlabeled-test/')\n",
    "\n",
    "ds_size_unlabeled_test = len(list(data_dir_unlabeled_test.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled_test/'*.*g')\n",
    "list_ds_unlabeled_test = tf.data.Dataset.list_files(files_string)\n",
    "\n",
    "unlabeled_ds_test = list_ds_unlabeled_test.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next hurdle: get access to both dataset sample and prediction.  \n",
    "- Predict one and one image?\n",
    "- Predict all at once?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works, but predicts one image at a time.. Slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one image of unlabeled-test set\n",
    "for img, name in unlabeled_ds_test.take(1):\n",
    "    # Convert to numpy and add dimension\n",
    "    print (\"Filename:\", str(name.numpy())[2:-1], end='\\n\\n')\n",
    "    show_image(img.numpy())\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "        if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the `full` unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_unlabeled = pathlib.Path('/mnt/sdb/hyper-kvasir/unlabeled/')\n",
    "\n",
    "ds_size_unlabeled = len(list(data_dir_unlabeled.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled/'*.*g')\n",
    "list_ds_unlabeled = tf.data.Dataset.list_files(files_string)\n",
    "\n",
    "unlabeled_ds = list_ds_unlabeled.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one image of unlabeled-test set\n",
    "for img, name in unlabeled_ds.take(1):\n",
    "    # Convert to numpy and add dimension\n",
    "    print (\"File:\",str(name.numpy())[2:-1], end='\\n\\n')\n",
    "    show_image(img.numpy())\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    \n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "         if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
