{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InverseTimeDecay Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_size = 7400\n",
    "batch_size = 128\n",
    "epochs = 40\n",
    "initial_learning_rate = 0.01\n",
    "decay_rate = 0.2\n",
    "\n",
    "# By using InverseTimeDecay\n",
    "lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(\n",
    "    initial_learning_rate = initial_learning_rate,\n",
    "    decay_steps = train_size // batch_size,\n",
    "    decay_rate = decay_rate,\n",
    "    staircase = False\n",
    ")\n",
    "\n",
    "# Plot the function\n",
    "step_per_epoch = train_size // batch_size\n",
    "step = np.linspace(0,step_per_epoch*epochs)\n",
    "\n",
    "lr = lr_schedule(step)\n",
    "\n",
    "plt.figure(figsize = (8,6))\n",
    "plt.plot(step/step_per_epoch, lr)\n",
    "plt.ylim([0,max(plt.ylim())])\n",
    "plt.title('Learning rate')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Learning Rate');\n",
    "\n",
    "print (lr[35])\n",
    "# opt = tf.keras.optimizers.SGD(lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dr = 1\n",
    "bs = 32\n",
    "mult = 1\n",
    "\n",
    "initial_learning_rate = 0.01\n",
    "epochs = 30\n",
    "batch_size = bs*mult\n",
    "train_size = 7400\n",
    "decay_steps = train_size//batch_size\n",
    "decay_rate = 4\n",
    "\n",
    "def schedule(epoch):\n",
    "    # calculate new learning rate\n",
    "    learning_rate = initial_learning_rate / (1 + decay_rate * (epoch*batch_size) / decay_steps)\n",
    "    return learning_rate\n",
    "\n",
    "epoch_list = list(range(epochs))\n",
    "lr = [schedule(epo) for epo in epoch_list]\n",
    "\n",
    "print (lr[15])\n",
    "print (lr[-1])\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.plot(epoch_list, lr);\n",
    "plt.yticks(np.arange(min(lr), max(lr), 0.0004))\n",
    "plt.grid('on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = create_callbacks(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = get_class_weights(ds[\"train\"], conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_file(conf, conf, \"conf\") #ignore stupid arguments\n",
    "start_time = time.time()\n",
    "\n",
    "teacher_history = teacher_model.fit(\n",
    "        ds[\"train\"],\n",
    "        steps_per_epoch = conf[\"steps\"][\"train\"],\n",
    "        epochs = conf[\"num_epochs\"],\n",
    "        validation_data = ds[\"test\"],\n",
    "        validation_steps = conf[\"steps\"][\"test\"],\n",
    "        validation_freq = 1,\n",
    "        class_weight = class_weights,\n",
    "        callbacks = callbacks,\n",
    "        verbose = 1\n",
    ")\n",
    "print (\"Time spent on training: {}\".format(time.time() - start_time))\n",
    "\n",
    "# Save the metrics from training\n",
    "write_to_file(teacher_history.history, conf, \"teacher_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_preprocess(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "    image = tf.image.random_brightness(image, max_delta=32.0 / 255.0)\n",
    "    image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
    "\n",
    "    #Make sure the image is still in [0, 1]\n",
    "    image = tf.clip_by_value(image, 0.0, 1.0)\n",
    "\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras Functional API (VS sequencial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(output_size, input_shape=(224, 224, 3), final_activation=\"softmax\"):\n",
    "\n",
    "    base_model = ResNet50(\n",
    "        input_shape=input_shape,\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False\n",
    "    )\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    \n",
    "    output = Dense(output_size, activation=final_activation)(x)\n",
    "    \n",
    "    return Model(outputs=output, inputs=base_model.input)\n",
    "\n",
    "\n",
    "model = build_model(output_size = 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old way of adding findings from unlabeled ds to original training ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with 1 sample from training dataset\n",
    "for img, lab in train_ds.unbatch().take(1):\n",
    "    pass\n",
    "\n",
    "new_samples = tf.data.Dataset.from_tensors((img, lab))\n",
    "\n",
    "pred_confidence = 0.90\n",
    "new_samples_counter = 0\n",
    "\n",
    "for batch in unlabeled_ds_test.batch(20):\n",
    "    images, filenames = batch\n",
    "    batch_preds = en_model.predict(images, verbose=0)\n",
    "    \n",
    "    for pred, image in zip(batch_preds, images):\n",
    "        highest_pred = np.max(pred)\n",
    "        if highest_pred > pred_confidence:\n",
    "            pred_idx = np.argmax(pred).astype(np.int32)\n",
    "            pred_class = class_names[pred_idx]\n",
    "            \n",
    "            # print(\"{:>5.2f}% {}\".format(highest_pred*100, pred_class))\n",
    "            \n",
    "            # Make a tensor of the unlabeled sample\n",
    "            new_sample = tf.data.Dataset.from_tensors((image, pred_idx))\n",
    "            # Add tensor to dataset\n",
    "            new_samples = new_samples.concatenate(new_sample)\n",
    "            new_samples_counter += 1\n",
    "        \n",
    "print (\"Found\", new_samples_counter, \"new samples from unlabeled_ds_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge new dataset with original but exclude the first sample of dataset to add\n",
    "new_train_ds = train_ds.unbatch().take(params[\"train_size\"]).concatenate(new_samples.skip(1))\n",
    "\n",
    "print (\"Added new samples from unlabeled dataset to the training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training dataset size:\", params[\"train_size\"])\n",
    "\n",
    "num_elements = new_train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "print (\"New size:\", num_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New way of adding findings (with lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_confidence = 0.99\n",
    "new_samples_counter = 0\n",
    "total_time = time.time()\n",
    "round_time = time.time()\n",
    "img_list = []\n",
    "lab_list = []\n",
    "idx = 0\n",
    "\n",
    "for image, label in unlabeled_ds:\n",
    "    if not idx%1000: \n",
    "        print (\"{:.1f}s\\nBatch {}\".format(time.time()-round_time,idx), end='')\n",
    "        round_time = time.time()\n",
    "    elif not idx%100:\n",
    "        print (\".\",end='')\n",
    "    img = np.expand_dims(image, 0)\n",
    "    pred = en_model.predict(img)\n",
    "    highest_pred = np.max(pred)\n",
    "    if highest_pred > pred_confidence:\n",
    "        pred_idx = np.argmax(pred).astype(np.int32)\n",
    "        #pred_class = class_names[pred_idx]\n",
    "\n",
    "        img_list.append(image)\n",
    "        lab_list.append(pred_idx)\n",
    "\n",
    "        new_samples_counter += 1\n",
    "    idx += 1\n",
    "        \n",
    "print (\"\\nTotal time: {:.3f} s\".format( time.time() - total_time ))\n",
    "print (\"Found\", new_samples_counter, \"new samples in unlabeled_ds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over dataset shards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Managed to cache the sharded dataset (64x64 res) for 1000 shards. Not good solution because  \n",
    "- Takes loooong time to cache (and need to be cached to be usable)  \n",
    "- Highly unifficient; need to run multiple resolutions.. So chache multiple times\n",
    "- For iterating over student / teacher model multiple times there is no way to remove 'used' samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the unlabeled dataset\n",
    "\n",
    "ds_shards = []\n",
    "num_splits = 1000 # yields 100 images in each split\n",
    "\n",
    "for i in range(num_splits):\n",
    "    ds_shards.append(unlabeled_ds.shard(num_shards=num_splits, index=i).cache('./cache/shard_'+str(i)+'tfcache'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elements = ds_shards[0].reduce(0, lambda x, _: x + 1).numpy()\n",
    "print (\"Each shard contains {} samples.\".format(num_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "for img, lab in train_ds.unbatch().take(1):\n",
    "    pass\n",
    "new_samples = tf.data.Dataset.from_tensors((img, lab))\n",
    "\n",
    "pred_confidence = 0.90\n",
    "new_samples_counter = 0\n",
    "total_time = time.time()\n",
    "round_time = time.time()\n",
    "\n",
    "for i in range(0, 200):\n",
    "    if not i%10:\n",
    "        print (\"{:.2f}s\\nShard {}-{}:\".format( time.time()-round_time, i, i+10 ), end='')\n",
    "    else:\n",
    "        print (\".\", end='')\n",
    "    \n",
    "    round_time = time.time()\n",
    "    for batch in ds_shards[i].batch(10):\n",
    "        images, filenames = batch\n",
    "        batch_preds = en_model.predict(images, verbose=0)\n",
    "\n",
    "        for pred, image in zip(batch_preds, images):\n",
    "            highest_pred = np.max(pred)\n",
    "            if highest_pred > pred_confidence:\n",
    "                pred_idx = np.argmax(pred).astype(np.int32)\n",
    "                pred_class = class_names[pred_idx]\n",
    "\n",
    "                # print(\"{:>5.2f}% {}\".format(highest_pred*100, pred_class))\n",
    "\n",
    "                # Make a tensor of the unlabeled sample\n",
    "                new_sample = tf.data.Dataset.from_tensors((image, pred_idx))\n",
    "                # Add tensor to dataset\n",
    "                new_samples = new_samples.concatenate(new_sample)\n",
    "                new_samples_counter += 1\n",
    "#     print (\"{:.2f}s\".format(time.time() - start1))\n",
    "        \n",
    "print (\"\\nAdded\", new_samples_counter, \"new samples to the training dataset\")\n",
    "print (\"Total time {:.2f} s\".format( time.time() - total_time ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge new dataset with original but exclude the first sample of dataset to add\n",
    "new_train_ds = train_ds.unbatch().take(params[\"train_size\"]).concatenate(new_samples.skip(1))\n",
    "\n",
    "print (\"Added {} new samples from unlabeled dataset to the training dataset\".format(new_samples_counter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Old train_ds size:\", params[\"train_size\"])\n",
    "\n",
    "num_elements = new_train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "print (\"New train_ds size:\", num_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate over take-datasets  \n",
    "- Tar lenger tid per iterasjon av take/skip  \n",
    "- Mye GPU/CPU idle time  \n",
    "- 5000 bilder tok ~500s  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "pred_confidence = 0.99\n",
    "new_samples_counter = 0\n",
    "dataset = unlabeled_ds\n",
    "start = time.time()\n",
    "img_list = []\n",
    "lab_list = []\n",
    "\n",
    "for i in range(20):\n",
    "    round_time = time.time()\n",
    "    print (\"Batch {} | \".format(i), end='')\n",
    "    \n",
    "    shard = dataset.take(100)\n",
    "    dataset = dataset.skip(100)\n",
    "    \n",
    "    images, filenames = next(iter(shard.batch(100))) # why batch?\n",
    "\n",
    "    batch_preds = en_model.predict(images, verbose=0)\n",
    "\n",
    "    for pred, image in zip(batch_preds, images):\n",
    "        highest_pred = np.max(pred)\n",
    "\n",
    "        if highest_pred > pred_confidence:\n",
    "            pred_idx = np.argmax(pred).astype(np.int32)\n",
    "            #pred_class = class_names[pred_idx]\n",
    "            img_list.append(image)\n",
    "            lab_list.append(pred_idx)\n",
    "            \n",
    "            new_samples_counter += 1\n",
    "    print (\"{:.3f}s\".format(time.time() - round_time))\n",
    "print (\"Time: {:.3f} s\".format( time.time() - start ))\n",
    "print (\"Added\", new_samples_counter, \"new samples to the training dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get size of dataset\n",
    "- Only works if dataset is not repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_size(ds):\n",
    "    return tf.data.experimental.cardinality(ds).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing augmentation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test augmentation\n",
    "from pipeline import augment_ds\n",
    "conf[\"augment\"] = [\"dcrop\",\"dflip\",\"dbrightness\",\"dsaturation\",\"dcontrast\",\"rotate\"]\n",
    "conf[\"aug_mult\"] = 0.2   # 0 - no aug | 1.0 full aug\n",
    "\n",
    "dataset = clean_train.take(1).cache().repeat(30)\n",
    "dataset = augment_ds(dataset, conf, tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.batch(30)\n",
    "\n",
    "checkout_dataset(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print latex-table of class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "weigts_list = [[key, value] for key, value in class_weights.items()]\n",
    "headers = [\"Class\", \"Weight\"]\n",
    "print(tabulate(weigts_list, headers, tablefmt=\"latex\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
