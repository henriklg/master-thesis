{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR10 dataset split into neg/pos and trained using normal CNN without augmentation.  \n",
    "- Class weighting  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading images\n",
    "https://www.tensorflow.org/tutorials/load_data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some stuff to make utils-function work\n",
    "import sys\n",
    "sys.path.append('/home/henrik/master_thesis/code/utils')\n",
    "from data_prep import create_dataset, print_class_info, show_image\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/cifar10/train/')\n",
    "\n",
    "config = {\n",
    "    \"data_dir\": data_dir,\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"model\": 'cnn',\n",
    "    \"ds_info\": 'binary',\n",
    "    \"resample\": False,\n",
    "    \"neg_class\": ['ship'],\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 64,\n",
    "    \"img_shape\": (32, 32, 3),\n",
    "    \"outcast\": None,\n",
    "    \"optimizer\": 'SGD',\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"decay_rate\": 0.4,              # higher number gives steeper dropoff\n",
    "    \"verbosity\": 1\n",
    "    }\n",
    "\n",
    "model_name = '{}x{}x{}_{}_{}'.format(config[\"num_epochs\"], config[\"batch_size\"], \n",
    "                                     config[\"img_shape\"][1], config[\"ds_info\"], config[\"model\"])\n",
    "\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "fine_tune_at = 130\n",
    "fine_tune_epochs = 150\n",
    "early_stopping_patience = config[\"early_stopping_patience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, testing and validation dataset from utils/data_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative class names:\n",
      "        - ship\n",
      "Positive class names:\n",
      "        - cat\n",
      "        - airplane\n",
      "        - deer\n",
      "        - automobile\n",
      "        - horse\n",
      "        - truck\n",
      "        - dog\n",
      "        - bird\n",
      "        - frog\n",
      "\n",
      "Negative samples:  5000 | 10.00%\n",
      "Positive samples: 45000 | 90.00%\n",
      "\n",
      "Total number of images: 50000\n",
      "Dataset.list_files:  /mnt/sdb/cifar10/train/*/*.*g \n",
      "\n",
      "[1 0 0 1 1 1 1 1 1 1]\n",
      "[1 1 0 1 0 0 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 0 1 1 0 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[1 0 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 0 0 0 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "\n",
      "Full dataset sample size:        50000\n",
      "Train dataset sample size:       35000\n",
      "Test dataset sample size:         7500\n",
      "Validation dataset sample size:   7500\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds, val_ds, params = create_dataset(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate class weights\n",
    "See https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg = params[\"neg_count\"]\n",
    "pos = params[\"pos_count\"]\n",
    "total = params[\"ds_size\"]\n",
    "\n",
    "weight_for_0 = (1/neg)*(total)/2.0\n",
    "weight_for_1 = (1/pos)*(total)/2.0\n",
    "\n",
    "class_weight = {0: weight_for_0,\n",
    "                1: weight_for_1}\n",
    "\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal CNN\n",
    "See https://www.tensorflow.org/tutorials/images/cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = models.Sequential()\n",
    "\n",
    "cnn_model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=config['img_shape']))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "cnn_model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "cnn_model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "cnn_model.add(BatchNormalization())\n",
    "\n",
    "cnn_model.add(layers.Flatten())\n",
    "cnn_model.add(layers.Dense(64, activation='relu'))\n",
    "cnn_model.add(layers.Dropout(0.2))\n",
    "cnn_model.add(layers.Dense(params['num_classes'], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['verbosity'] > 0:\n",
    "    cnn_model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save image of layers\n",
    "tf.keras.utils.plot_model(cnn_model, 'models/{}.png'.format(model_name), show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['optimizer'] == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "cnn_model.compile(optimizer=opt,\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir=\"./logs/{}/\".format(config[\"model\"]) + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "\n",
    "callbacks = [tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cnn_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = params[\"train_size\"] // config[\"batch_size\"],\n",
    "    epochs = config[\"num_epochs\"],\n",
    "    validation_data = test_ds,\n",
    "    validation_steps = params[\"test_size\"] // config[\"batch_size\"],\n",
    "    validation_freq = 1\n",
    "#     class_weight=class_weight\n",
    "#     callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Keras`\n",
    "Save/load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_model.save('models/{}.h5'.format(model_name))\n",
    "# cnn_model = tf.keras.models.load_model('models/{}.h5'.format(model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_evaluate = cnn_model.evaluate(val_ds, verbose=2, steps=params[\"val_size\"] // config[\"batch_size\"])\n",
    "\n",
    "f = open(log_dir+\"/evaluate.txt\",\"w\")\n",
    "f.write( str(cnn_evaluate) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(history.epoch[-1]+1)\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Tensorboard`"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorboard import notebook\n",
    "# Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard\n",
    "\n",
    "# Start tensorboard\n",
    "# %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Clear any logs from previous runs (move to .old instead?)\n",
    "!rm -rf ./logs/\n",
    "\n",
    "# Stop tensorboard\n",
    "notebook.list()\n",
    "!kill 20058"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict new samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one batch\n",
    "images, labels = next(iter(val_ds))\n",
    "\n",
    "# Convert from tensor to numpy array\n",
    "images = images.numpy()\n",
    "labels = labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a random image and label\n",
    "rand = np.random.randint(0, config[\"batch_size\"])\n",
    "image = images[rand]\n",
    "label = labels[rand]\n",
    "\n",
    "# Predict one image\n",
    "predictions = cnn_model.predict(np.expand_dims(image, axis=0))[0]\n",
    "\n",
    "for i, pred in enumerate(predictions,):\n",
    "    print(\"{:0.4f} | {}\".format(pred, params[\"class_names\"][i]))\n",
    "\n",
    "prediction = ('Boat') if np.argmax(predictions)==0 else ('Not boat')\n",
    "\n",
    "print (\"Image {} of {}\".format(rand, config[\"batch_size\"]))\n",
    "\n",
    "plt.figure(frameon=False, facecolor='white');\n",
    "plt.title(prediction, fontdict={'color':'white','size':20})\n",
    "plt.imshow(image)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict one batch\n",
    "predictions = cnn_model.predict(images)\n",
    "\n",
    "print ('{:3}  {:5}  {:3}'.format('idx', 'label', 'pred'))\n",
    "print ('---  -----  ----')\n",
    "\n",
    "for i, pred in enumerate(predictions):\n",
    "    label = params[\"class_names\"][labels[i]][0:3]\n",
    "    prediction = params[\"class_names\"][np.argmax(pred)][0:3]\n",
    "    print ('\\n{:3}  {:5}  {:5}'.format(i, label, prediction), end='')\n",
    "    if (label != prediction): print (\"Wrong\", end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TF2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
