{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kvasir dataset split into neg/pos and trained using Resnet50 without augmentation. Getting some decent results after training on resampled data with large step-size.  \n",
    "- Class weighting  \n",
    "- Resampling  \n",
    "- Initial Bias-estimation\n",
    "- Decreasing learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Some stuff to make utils-function work\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from data_prep import create_dataset, print_class_info, show_image\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/labeled/')\n",
    "\n",
    "config = {\n",
    "    # Dataset\n",
    "    \"data_dir\": data_dir,\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"ds_info\": 'hyp-kva',\n",
    "    \"resample\": False,\n",
    "    \"shuffle_buffer_size\": 0,\n",
    "    \"neg_class\": ['normal-cecum'],\n",
    "    \"outcast\": None,\n",
    "    # Model\n",
    "    \"model\": 'EfficientNetB0',\n",
    "    \"num_epochs\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"img_shape\": (64, 64, 3),\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"final_activation\": 'softmax',\n",
    "    # Callbacks\n",
    "    \"learning_schedule\": True,\n",
    "    \"checkpoint\": False,\n",
    "    \"early_stopping\": False,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"decay_rate\": 0.05,              # higher number gives steeper dropoff\n",
    "    # Misc\n",
    "    \"verbosity\": 1\n",
    "    }\n",
    "\n",
    "model_name = '{}x{}x{}_{}_{}'.format(config[\"num_epochs\"], config[\"batch_size\"], \n",
    "                                     config[\"img_shape\"][1], config[\"ds_info\"], config[\"model\"])\n",
    "\n",
    "fine_tune_from = 130\n",
    "fine_tune_epochs = 30\n",
    "early_stopping_patience = config[\"early_stopping_patience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, testing and validation dataset from utils/data_prep.py.  \n",
    "Returns tf.dataset for shuffled, cached and batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "barretts-short-segment      :   53 | 0.50%\n",
      "bbps-0-1                    :  646 | 6.06%\n",
      "impacted-stool              :  131 | 1.23%\n",
      "bbps-2-3                    : 1148 | 10.77%\n",
      "hemorrhoids                 :    6 | 0.06%\n",
      "ulcerative-colitis-grade-2  :  443 | 4.15%\n",
      "normal-z-line               :  932 | 8.74%\n",
      "retroflex-stomach           :  764 | 7.17%\n",
      "esophagitis-b-d             :  260 | 2.44%\n",
      "dyed-resection-margins      :  989 | 9.28%\n",
      "ileum                       :    9 | 0.08%\n",
      "ulcerative-colitis-0-1      :   35 | 0.33%\n",
      "dyed-lifted-polyps          : 1002 | 9.40%\n",
      "polyps                      : 1028 | 9.64%\n",
      "ulcerative-colitis-2-3      :   28 | 0.26%\n",
      "ulcerative-colitis-1-2      :   11 | 0.10%\n",
      "ulcerative-colitis-grade-3  :  133 | 1.25%\n",
      "retroflex-rectum            :  391 | 3.67%\n",
      "esophagitis-a               :  403 | 3.78%\n",
      "ulcerative-colitis-grade-1  :  201 | 1.89%\n",
      "pylorus                     :  999 | 9.37%\n",
      "cecum                       : 1009 | 9.46%\n",
      "barretts                    :   41 | 0.38%\n",
      "\n",
      "Total number of images: 10662, in 23 classes.\n",
      "\n",
      "Dataset.list_files:  /home/henrik/master-thesis/data/hyper-kvasir/labeled/*/*.*g \n",
      "\n",
      "WARNING:tensorflow:AutoGraph could not transform <function create_dataset.<locals>.get_label at 0x7f9d1ded2ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Cell is empty\n",
      "WARNING: AutoGraph could not transform <function create_dataset.<locals>.get_label at 0x7f9d1ded2ef0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: Cell is empty\n",
      "[20 12  3  6  9 20 13  5 20 22]\n",
      "[18  9  9 18 17  3  2 20 13 12]\n",
      "[ 3 21 13  1  6  7 12 17  5 13]\n",
      "[21  9  7  9 12  7 20 11  9 20]\n",
      "[ 3 13  1 17 21 20  9  6 13  9]\n",
      "[ 6 20  9  5  9 12  9  1  7 12]\n",
      "[19 19  9  1 21 16 17 21  3 18]\n",
      "[ 5  9 21 12  6 21  6 13 12  7]\n",
      "[12  1 18 12 12  6  3 15  9 12]\n",
      "[ 3 20  3  6  6  7  1  6 12  6]\n",
      "\n",
      "Full dataset sample size:        10662\n",
      "Train dataset sample size:        7463\n",
      "Test dataset sample size:         1599\n",
      "Validation dataset sample size:   1600\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds, val_ds, params = create_dataset(config)\n",
    "\n",
    "train_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "test_steps = params[\"test_size\"] // config[\"batch_size\"]\n",
    "val_steps = params[\"val_size\"] // config[\"batch_size\"]\n",
    "class_names = params[\"class_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Train a teacher model on labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from efficientnet import EfficientNetB0 as EfficientNet\n",
    "\n",
    "# from efficientnet import center_crop_and_resize, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_base = EfficientNet(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False, \n",
    "    input_shape=config[\"img_shape\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the layers. I.E we're just using the pre-trained weights as initial weigths and biases and train over them\n",
    "efficientnet_base.trainable = True\n",
    "\n",
    "# Define model\n",
    "en_model = Sequential()\n",
    "en_model.add(efficientnet_base)\n",
    "en_model.add(layers.GlobalAveragePooling2D())\n",
    "# en_model.add(layers.Dropout(0.2))\n",
    "# en_model.add(layers.Dense(256, activation='relu'))\n",
    "en_model.add(layers.Dense(params[\"num_classes\"], activation=config[\"final_activation\"]))\n",
    "\n",
    "if config['optimizer'] == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "en_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b0 (Model)      (None, 2, 2, 1280)        4049564   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 23)                29463     \n",
      "=================================================================\n",
      "Total params: 4,079,027\n",
      "Trainable params: 4,037,011\n",
      "Non-trainable params: 42,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "en_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using LearnignRateScheduler\n",
    "initial_learning_rate = config[\"learning_rate\"]\n",
    "decay_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "batch_size = config['batch_size']\n",
    "decay_rate = config['decay_rate']\n",
    "\n",
    "def schedule(epoch):\n",
    "    # calculate new learning rate\n",
    "    learning_rate = initial_learning_rate / (1 + decay_rate * (epoch*batch_size) / decay_steps)\n",
    "    \n",
    "    # update tensorboard\n",
    "    tf.summary.scalar(name='learning_rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "log_dir=\"./logs/{}/\".format(config[\"model\"]) + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "lr_schedule_cb = LearningRateScheduler(schedule, verbose=1)\n",
    "earlystopp_cb = EarlyStopping(monitor='val_loss',verbose=1, patience=early_stopping_patience, restore_best_weights=True)\n",
    "checkpoint_cb = ModelCheckpoint(filepath='./models/best_cp-{epoch:03d}.hdf', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "\n",
    "callbacks = [tensorboard_cb]\n",
    "if config[\"early_stopping\"]: callbacks.append(earlystopp_cb)\n",
    "if config[\"learning_schedule\"]: callbacks.append(lr_schedule_cb)\n",
    "if config[\"checkpoint\"]: callbacks.append(checkpoint_cb)\n",
    "\n",
    "# Write config dictionary to text file\n",
    "f = open(log_dir+\"/config.txt\",\"w\")\n",
    "f.write(str(config))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 58 steps, validate for 12 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/5\n",
      "58/58 [==============================] - 12s 201ms/step - loss: 1.1345 - sparse_categorical_accuracy: 0.6495 - val_loss: 11163.9991 - val_sparse_categorical_accuracy: 0.0840\n",
      "\n",
      "Epoch 00002: LearningRateScheduler reducing learning rate to 0.009006211180124225.\n",
      "Epoch 2/5\n",
      "58/58 [==============================] - 4s 72ms/step - loss: 0.5690 - sparse_categorical_accuracy: 0.8017 - val_loss: 342.6184 - val_sparse_categorical_accuracy: 0.1374\n",
      "\n",
      "Epoch 00003: LearningRateScheduler reducing learning rate to 0.008192090395480226.\n",
      "Epoch 3/5\n",
      "58/58 [==============================] - 5s 79ms/step - loss: 0.4439 - sparse_categorical_accuracy: 0.8393 - val_loss: 11.6640 - val_sparse_categorical_accuracy: 0.4889\n",
      "\n",
      "Epoch 00004: LearningRateScheduler reducing learning rate to 0.007512953367875648.\n",
      "Epoch 4/5\n",
      "58/58 [==============================] - 4s 71ms/step - loss: 0.3950 - sparse_categorical_accuracy: 0.8657 - val_loss: 8.1856 - val_sparse_categorical_accuracy: 0.4714\n",
      "\n",
      "Epoch 00005: LearningRateScheduler reducing learning rate to 0.006937799043062201.\n",
      "Epoch 5/5\n",
      "58/58 [==============================] - 4s 73ms/step - loss: 0.3947 - sparse_categorical_accuracy: 0.8671 - val_loss: 5.2104 - val_sparse_categorical_accuracy: 0.5924\n"
     ]
    }
   ],
   "source": [
    "history = en_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = train_steps,\n",
    "    epochs = config[\"num_epochs\"],\n",
    "    validation_data = test_ds,\n",
    "    validation_steps = test_steps,\n",
    "    validation_freq = 1,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc =  str(history.history[\"val_sparse_categorical_accuracy\"][-1])[2:4]\n",
    "# en_model.save('./models/{}.h5'.format(model_name+best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_evaluate = en_model.evaluate(val_ds, verbose=2, steps=val_steps)\n",
    "\n",
    "# Write evaluate dictionary to text file\n",
    "f = open(log_dir+\"/evaluate.txt\",\"w\")\n",
    "f.write( str(en_evaluate) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "if config[\"learning_schedule\"]: lr = history.history['lr']\n",
    "epochs_range = range(history.epoch[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"learning_schedule\"]:\n",
    "    # Plot the learning rate\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs_range, lr, label='Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learnign rate')\n",
    "    plt.savefig(log_dir+'/learning_rate.png')\n",
    "    plt.title('Adaptive Learning Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-val accuracy and loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0.0, 3])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(log_dir+'/accuracy_and_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the predictions on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch of validation data\n",
    "for images, labels in val_ds.take(1):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.randint(0, config[\"batch_size\"])\n",
    "\n",
    "# Take one image and convert it to numpy\n",
    "img = images.numpy()[idx]\n",
    "lab = labels.numpy()[idx]\n",
    "# Add one dimension\n",
    "print (\"label:\", class_names[lab], end='\\n\\n')\n",
    "show_image(img)\n",
    "img = np.expand_dims(img, 0)\n",
    "\n",
    "prediction = en_model.predict(img, verbose=0)\n",
    "for i, pred in enumerate(prediction[0]):\n",
    "    if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the unlabeled `test` dataset (which are images taken from the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # the last item of parts is the filename\n",
    "    filename = parts[-1]\n",
    "    return filename\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [config[\"img_shape\"][0], config[\"img_shape\"][1] ])\n",
    "\n",
    "def process_path(file_path):\n",
    "    filename = get_filename(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir_unlabeled_test = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/unlabeled-test/')\n",
    "\n",
    "ds_size_unlabeled_test = len(list(data_dir_unlabeled_test.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled_test/'*.*g')\n",
    "list_ds_unlabeled_test = tf.data.Dataset.list_files(files_string)\n",
    "\n",
    "unlabeled_ds_test = list_ds_unlabeled_test.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction on one random image, to verify the results after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some images and confidence levels to see how to model performs\n",
    "# Take one image of unlabeled-test set\n",
    "for img, name in unlabeled_ds_test.take(1):\n",
    "    # Convert to numpy and add dimension\n",
    "    print (\"Filename:\", str(name.numpy())[2:-1])\n",
    "    print (\"Label:\", str(name.numpy())[38:-5])\n",
    "    show_image(img.numpy())\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    print ()\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "        if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the findings with probability score higher than threshold to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with 1 sample from training dataset\n",
    "for img, lab in train_ds.unbatch().take(1):\n",
    "    pass\n",
    "\n",
    "new_samples = tf.data.Dataset.from_tensors((img, lab))\n",
    "\n",
    "pred_confidence = 0.90\n",
    "new_samples_counter = 0\n",
    "img_list = []\n",
    "lab_list = []\n",
    "\n",
    "for batch in unlabeled_ds_test.batch(20):\n",
    "    images, filenames = batch\n",
    "    batch_preds = en_model.predict(images, verbose=0)\n",
    "    \n",
    "    for pred, image in zip(batch_preds, images):\n",
    "        highest_pred = np.max(pred)\n",
    "        if highest_pred > pred_confidence:\n",
    "            pred_idx = np.argmax(pred).astype(np.int32)\n",
    "            pred_class = class_names[pred_idx]\n",
    "            \n",
    "            img_list.append(image)\n",
    "            lab_list.append(pred_idx)\n",
    "            \n",
    "            # print(\"{:>5.2f}% {}\".format(highest_pred*100, pred_class))\n",
    "            \n",
    "            # Make a tensor of the unlabeled sample\n",
    "          #  new_sample = tf.data.Dataset.from_tensors((image, pred_idx))\n",
    "            # Add tensor to dataset\n",
    "          #  new_samples = new_samples.concatenate(new_sample)\n",
    "            new_samples_counter += 1\n",
    "        \n",
    "print (\"Found\", new_samples_counter, \"new samples from unlabeled_ds_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset out of list of findings\n",
    "new_findings = tf.data.Dataset.from_tensor_slices((img_list, lab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge new dataset with original\n",
    "new_train_ds = train_ds.unbatch().take(params[\"train_size\"]).concatenate(new_findings)\n",
    "\n",
    "print (\"Added new samples from unlabeled dataset to the training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training dataset size:\", params[\"train_size\"])\n",
    "\n",
    "num_elements = new_train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "print (\"New size:\", num_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the `full` unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99417 images into unlabeled_ds.\n"
     ]
    }
   ],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir_unlabeled = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/unlabeled/')\n",
    "\n",
    "ds_size_unlabeled = len(list(data_dir_unlabeled.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled/'*.*g')\n",
    "list_ds_unlabeled = tf.data.Dataset.list_files(files_string)\n",
    "\n",
    "unlabeled_ds = list_ds_unlabeled.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "print (\"Loaded {} images into unlabeled_ds.\".format(ds_size_unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on 1 image\n",
    "for img, name in unlabeled_ds.take(1):\n",
    "    # Convert to numpy and add dimension\n",
    "    print (\"File:\",str(name.numpy())[2:-1], end='\\n\\n')\n",
    "    show_image(img.numpy())\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    \n",
    "    # If probability above 1% print info\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "         if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over batched dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 'append to list and convert to tensor'-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0\n",
      "Batch 1000\n",
      "Batch 2000\n",
      "Batch 3000\n",
      "Batch 4000\n",
      "Batch 5000\n",
      "Batch 6000\n",
      "Batch 7000\n",
      "Batch 8000\n",
      "Batch 9000\n",
      "Batch 10000\n",
      "Batch 11000\n",
      "Batch 12000\n",
      "Batch 13000\n",
      "Batch 14000\n",
      "Batch 15000\n",
      "Batch 16000\n",
      "Batch 17000\n",
      "Batch 18000\n",
      "Batch 19000\n",
      "Time: 599.226 s\n",
      "Found 21392 new samples in unlabeled_ds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "pred_confidence = 0.99\n",
    "new_samples_counter = 0\n",
    "start = time.time()\n",
    "batch_idx = 0\n",
    "img_list = []\n",
    "lab_list = []\n",
    "\n",
    "for batch in unlabeled_ds.batch(5):\n",
    "    if not batch_idx%1000: print (\"Batch {}\".format(batch_idx))\n",
    "    images, filenames = batch\n",
    "    batch_preds = en_model.predict(images, verbose=0)\n",
    "\n",
    "    for pred, image in zip(batch_preds, images):\n",
    "        highest_pred = np.max(pred)\n",
    "\n",
    "        if highest_pred > pred_confidence:\n",
    "            pred_idx = np.argmax(pred).astype(np.int32)\n",
    "            #pred_class = class_names[pred_idx]\n",
    "\n",
    "            img_list.append(image)\n",
    "            lab_list.append(pred_idx)\n",
    "            \n",
    "            new_samples_counter += 1\n",
    "    batch_idx += 1\n",
    "        \n",
    "        \n",
    "print (\"Time: {:.3f} s\".format( time.time() - start ))\n",
    "print (\"Found\", new_samples_counter, \"new samples in unlabeled_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_findings = tf.data.Dataset.from_tensor_slices((img_list, lab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_ds = train_ds.unbatch().take(params[\"train_size\"]).concatenate(new_findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 7463\n",
      "New size: 28855\n"
     ]
    }
   ],
   "source": [
    "print (\"Training dataset size:\", params[\"train_size\"])\n",
    "\n",
    "num_elements = new_train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "print (\"New size:\", num_elements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
