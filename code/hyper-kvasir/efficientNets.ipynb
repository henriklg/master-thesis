{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create a student-teacher model where we first train a teacher on labeled data, and then use this teacher model to label more data, then we swap out the teacher with a student and train again over all the samples. \n",
    "- Add data augmentation\n",
    "- Add dropout and regularization\n",
    "- Resample the dataset (also the one created from unlab_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Some stuff to make utils-function work\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from data_prep import create_dataset, show_image, prepare_for_training\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/labeled/')\n",
    "\n",
    "config = {\n",
    "    # Dataset\n",
    "    \"data_dir\": data_dir,\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"ds_info\": 'hyp-kva-aug',\n",
    "    \"resample\": False,\n",
    "    \"shuffle_buffer_size\": 5000,       # 0=no shuffling\n",
    "    \"seed\": 123,\n",
    "    \"neg_class\": None,\n",
    "    \"outcast\": None,\n",
    "    # Model\n",
    "    \"model\": 'EfficientNetB0',\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 128,\n",
    "    \"img_shape\": (128, 128, 3),\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"final_activation\": 'softmax',\n",
    "    # Callbacks\n",
    "    \"learning_schedule\": True,\n",
    "    \"checkpoint\": False,\n",
    "    \"early_stopping\": True,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"decay_rate\": 0.05,              # higher number gives steeper dropoff\n",
    "    # Misc\n",
    "    \"verbosity\": 1\n",
    "    }\n",
    "\n",
    "model_name = '{}x{}x{}_{}_{}'.format(config[\"num_epochs\"], config[\"batch_size\"], \n",
    "                                     config[\"img_shape\"][1], config[\"ds_info\"], config[\"model\"])\n",
    "\n",
    "fine_tune_from = 130\n",
    "fine_tune_epochs = 30\n",
    "early_stopping_patience = config[\"early_stopping_patience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, testing and validation dataset from utils/data_prep.py.  \n",
    "Returns tf.dataset for shuffled, cached and batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, val_ds, params = create_dataset(config)\n",
    "\n",
    "train_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "test_steps = params[\"test_size\"] // config[\"batch_size\"]\n",
    "val_steps = params[\"val_size\"] // config[\"batch_size\"]\n",
    "class_names = params[\"class_names\"]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# show sample image from training data after augmentation\n",
    "img, lab = next(iter(train_ds))\n",
    "\n",
    "img = img.numpy()[1]\n",
    "\n",
    "show_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Train a teacher model on labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet import EfficientNetB0 as EfficientNet\n",
    "\n",
    "# from efficientnet import center_crop_and_resize, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_base = EfficientNet(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False, \n",
    "    input_shape=config[\"img_shape\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the layers. I.E we're just using the pre-trained weights as initial weigths and biases and train over them\n",
    "efficientnet_base.trainable = True\n",
    "\n",
    "# Define model\n",
    "en_model = Sequential()\n",
    "en_model.add(efficientnet_base)\n",
    "en_model.add(layers.GlobalAveragePooling2D())\n",
    "# en_model.add(layers.Dropout(0.2))\n",
    "# en_model.add(layers.Dense(256, activation='relu'))\n",
    "en_model.add(layers.Dense(params[\"num_classes\"], activation=config[\"final_activation\"]))\n",
    "\n",
    "if config['optimizer'] == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "en_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using LearnignRateScheduler\n",
    "initial_learning_rate = config[\"learning_rate\"]\n",
    "decay_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "batch_size = config['batch_size']\n",
    "decay_rate = config['decay_rate']\n",
    "\n",
    "def schedule(epoch):\n",
    "    # calculate new learning rate\n",
    "    learning_rate = initial_learning_rate / (1 + decay_rate * (epoch*batch_size) / decay_steps)\n",
    "    \n",
    "    # update tensorboard\n",
    "    tf.summary.scalar(name='learning_rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "log_dir=\"./logs/{}/\".format(config[\"model\"]) + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "lr_schedule_cb = LearningRateScheduler(schedule, verbose=1)\n",
    "earlystopp_cb = EarlyStopping(monitor='val_loss', \n",
    "                              verbose=1, patience=config[\"early_stopping_patience\"], \n",
    "                              restore_best_weights=True)\n",
    "checkpoint_cb = ModelCheckpoint(filepath='./models/best_cp-{epoch:03d}.hdf', \n",
    "                                monitor='val_loss', save_best_only=True, mode='auto')\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "\n",
    "callbacks = [tensorboard_cb]\n",
    "if config[\"early_stopping\"]: callbacks.append(earlystopp_cb)\n",
    "if config[\"learning_schedule\"]: callbacks.append(lr_schedule_cb)\n",
    "if config[\"checkpoint\"]: callbacks.append(checkpoint_cb)\n",
    "\n",
    "# Write config dictionary to text file\n",
    "f = open(log_dir+\"/config.txt\",\"w\")\n",
    "f.write(str(config))\n",
    "f.close()\n",
    "\n",
    "# Write params dictionary to text file\n",
    "f = open(log_dir+\"/params.txt\",\"w\")\n",
    "f.write(str(params))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = en_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = train_steps,\n",
    "    epochs = config[\"num_epochs\"],\n",
    "    validation_data = test_ds,\n",
    "    validation_steps = test_steps,\n",
    "    validation_freq = 1,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the metrics from training\n",
    "f = open(log_dir+\"/history.txt\",\"w\")\n",
    "f.write(str(history.history))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save or restore a model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Save the model\n",
    "best_acc =  str(history.history[\"val_sparse_categorical_accuracy\"][-1])[2:4]\n",
    "en_model.save(log_dir+'/{}.h5'.format(model_name+'_'+best_acc))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Load a previously saved model\n",
    "time = \"20200424-222542\"\n",
    "log_dir = \"./logs/{}/\".format(config[\"model\"]) + time\n",
    "en_model = load_model(log_dir + \"/30x128x128_hyp-kva-aug_EfficientNetB0_82.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if validation data is cached\n",
    "cache_dir = config[\"cache_dir\"]\n",
    "img_width = config[\"img_shape\"][0]\n",
    "ds_info = config[\"ds_info\"]\n",
    "filename = \"{}/{}_{}_val.tfcache.index\".format(cache_dir, img_width, ds_info)\n",
    "\n",
    "cached = os.path.isfile(filename)\n",
    "if not cached:\n",
    "    # Iterate over dataset to initialize caching\n",
    "    for batch in val_ds.take(params[\"val_size\"]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_evaluate = en_model.evaluate(val_ds, verbose=2, steps=val_steps)\n",
    "\n",
    "# Write evaluate dictionary to text file\n",
    "f = open(log_dir+\"/val_evaluate.txt\",\"w\")\n",
    "f.write( str(en_evaluate) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "if config[\"learning_schedule\"]: lr = history.history['lr']\n",
    "epochs_range = range(history.epoch[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"learning_schedule\"]:\n",
    "    # Plot the learning rate\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs_range, lr, label='Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learnign rate')\n",
    "    plt.title('Learning Rate development during training');\n",
    "    plt.savefig(log_dir+'/learning_rate.pdf', format='pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-val accuracy and loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0.0, 3])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(log_dir+'/accuracy_and_loss.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: run prediction on images from the validation dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Take one batch of validation data\n",
    "images, labels = next(iter(val_ds))\n",
    "\n",
    "# Take random image of batch and convert it to numpy\n",
    "idx = np.random.randint(0, config[\"batch_size\"])\n",
    "img = images.numpy()[idx]\n",
    "lab = labels.numpy()[idx]\n",
    "\n",
    "print (\"label:\", class_names[lab], end='\\n\\n')\n",
    "show_image(img)\n",
    "\n",
    "# Add one dimension\n",
    "img = np.expand_dims(img, 0)\n",
    "prediction = en_model.predict(img, verbose=0)\n",
    "for i, pred in enumerate(prediction[0]):\n",
    "    if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: run prediction on images from the unlabeled_test dataset\n",
    "(which are images taken from the training data)  \n",
    "TODO: just remove this?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_filename(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # the last item of parts is the filename\n",
    "    filename = parts[-1]\n",
    "    return filename\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [config[\"img_shape\"][0], config[\"img_shape\"][1] ])\n",
    "\n",
    "def process_path(file_path):\n",
    "    filename = get_filename(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, filename\n",
    "\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir_unlabeled_test = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/unlabeled-test/')\n",
    "\n",
    "ds_size_unlabeled_test = len(list(data_dir_unlabeled_test.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled_test/'*.*g')\n",
    "list_ds_unlabeled_test = tf.data.Dataset.list_files(\n",
    "            files_string,\n",
    "            shuffle=config[\"shuffle_buffer_size\"]>1, \n",
    "            seed=tf.constant(config[\"seed\"], tf.int64)\n",
    ")\n",
    "\n",
    "unlabeled_ds_test = list_ds_unlabeled_test.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Print some images and confidence levels to see how to model performs\n",
    "# Take one image of unlabeled-test set\n",
    "img, name = next(iter(unlabeled_ds_test))\n",
    "\n",
    "print (\"Filename:\", str(name.numpy())[2:-1])\n",
    "print (\"Label:\", str(name.numpy())[38:-5])\n",
    "show_image(img.numpy())\n",
    "\n",
    "# add dimension and predict\n",
    "img = np.expand_dims(img.numpy(), 0)\n",
    "prediction = en_model.predict(img, verbose=0)\n",
    "\n",
    "print ()\n",
    "for i, pred in enumerate(prediction[0]):\n",
    "    if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: use the teacher to generate pseudo labels on unlabeled images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # the last item of parts is the filename\n",
    "    filename = parts[-1]\n",
    "    return filename\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [config[\"img_shape\"][0], config[\"img_shape\"][1] ])\n",
    "\n",
    "def process_path(file_path):\n",
    "    filename = get_filename(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir_unlabeled = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/unlabeled/')\n",
    "\n",
    "ds_size_unlabeled = len(list(data_dir_unlabeled.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled/'*.*g')\n",
    "list_ds_unlabeled = tf.data.Dataset.list_files(\n",
    "        files_string, \n",
    "        shuffle=config[\"shuffle_buffer_size\"]>1, \n",
    "        seed=tf.constant(config[\"seed\"], tf.int64)\n",
    ")\n",
    "\n",
    "unlabeled_ds = list_ds_unlabeled.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "print (\"Loaded {} images into unlabeled_ds.\".format(ds_size_unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on 1 image\n",
    "#img, name = next(iter(unlabeled_ds))\n",
    "\n",
    "for img, name in unlabeled_ds.take(1):\n",
    "\n",
    "    print (\"File:\",str(name.numpy())[2:-1], end='\\n\\n')\n",
    "    show_image(img.numpy())\n",
    "\n",
    "    # Add dimension and predict\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "\n",
    "    # If probability above 1% print info\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "         if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predictions on all unlabeled images\n",
    "Using 'append to list and convert to tensor'-method\n",
    "- caching the dataset takes too much space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pred_confidence = 0.80\n",
    "new_samples_counter = 0\n",
    "total_time = time.time()\n",
    "img_list = []\n",
    "lab_list = []\n",
    "\n",
    "tqdm_predicting = tqdm(total=ds_size_unlabeled, desc='Predicting', position=0)\n",
    "tqdm_findings = tqdm(total=ds_size_unlabeled, desc='Findings', position=1, bar_format='{desc}:{bar}{n_fmt}')\n",
    "\n",
    "print (\"Press 'Interrupt Kernel' to exit.\")\n",
    "try:\n",
    "    for count, (image,label) in enumerate(unlabeled_ds):\n",
    "        img = np.expand_dims(image, 0)\n",
    "        pred = en_model.predict(img)\n",
    "        highest_pred = np.max(pred)\n",
    "        if highest_pred > pred_confidence:\n",
    "            pred_idx = np.argmax(pred).astype(np.int32)\n",
    "            #pred_class = class_names[pred_idx]\n",
    "\n",
    "            img_list.append(image)\n",
    "            lab_list.append(pred_idx)\n",
    "\n",
    "            new_samples_counter += 1\n",
    "            tqdm_findings.update(1)\n",
    "        tqdm_predicting.update(1)\n",
    "except KeyboardInterrupt:\n",
    "    print (\"Exiting\")\n",
    "\n",
    "finally:\n",
    "    print (\"\\nTotal run time: {:.3f} s\".format( time.time() - total_time ))\n",
    "    print (\"Found {} new samples in unlabeled_ds after looking at {} images.\".format(new_samples_counter, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the image and labels list as pickle dump"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Dump it as a pickle\n",
    "import pickle\n",
    "\n",
    "with open('{}_lab_list.pkl'.format(config[\"img_shape\"][0]), 'wb') as f:\n",
    "    pickle.dump(lab_list, f)\n",
    "    \n",
    "with open('{}_img_list.pkl'.format(config[\"img_shape\"][0]), 'wb') as f:\n",
    "    pickle.dump(img_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make bar chart of findings from unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count findings\n",
    "findings = np.asarray([(i, lab_list.count(i)) for i in range(params[\"num_classes\"])])\n",
    "\n",
    "assert len(class_names) == len(findings), \"Must be same length.\"\n",
    "\n",
    "x = findings[:,0]\n",
    "width = 0.5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "rects1 = ax.bar(x, findings[:,1], width, label='Model')\n",
    "#rects2 = ax.bar(x + width/2, women_means, width, label='Women')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Number of samples')\n",
    "ax.set_title('Unlabeled data predictions for EfficientNet')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.set_axisbelow(True)\n",
    "ax.legend()\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=25, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "plt.grid(axis='y')\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig(log_dir+'/unlab_data_prediction.pdf', format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The class we want to show images from\n",
    "checkout = 'pylorus'\n",
    "# Which class number correspond to that class name\n",
    "idx = np.where(class_names == checkout)[0][0]\n",
    "# List of img_list-indexes with images corresponding to that class number\n",
    "idx_list = np.where(lab_list == idx)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings\n",
    "h, w = 10, 10        # for raster image\n",
    "nrows, ncols = 4, 4  # array of sub-plots\n",
    "figsize = [10, 10]     # figure size, inches\n",
    "\n",
    "\n",
    "# create figure (fig), and array of axes (ax)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, \n",
    "                       figsize=figsize, frameon=False, facecolor='white')\n",
    "\n",
    "# plot simple raster image on each sub-plot\n",
    "for i, axi in enumerate(ax.flat):\n",
    "    # i runs from 0 to (nrows*ncols-1)\n",
    "    # axi is equivalent with ax[rowid][colid]\n",
    "    img = img_list[idx_list[i]]\n",
    "    axi.imshow(img)\n",
    "    # get indices of row/column\n",
    "    rowid = i // ncols\n",
    "    colid = i % ncols\n",
    "    # write row/col indices as axes' title for identification\n",
    "    #axi.set_title(\"Row:\"+str(rowid)+\", Col:\"+str(colid))\n",
    "    axi.set_axis_off()\n",
    "\n",
    "plt.axis('off')\n",
    "plt.tight_layout(True)\n",
    "plt.savefig(\"{}/unlab_data_checkout-{}.pdf\".format(log_dir, checkout), format='pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert image and label lists to tensors and combine with training_ds to create a new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findings_tensor = tf.data.Dataset.from_tensor_slices((img_list, lab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine with old ds\n",
    "combined_ds = train_ds.unbatch().take(params[\"train_size\"]).concatenate(findings_tensor)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print (\"Training dataset size:\", params[\"train_size\"])\n",
    "\n",
    "# Careful! This might crash if dataset is too large\n",
    "num_elements = new_train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "print (\"New size:\", num_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train a student model on the combination of labeled images and pseudo labeled images\n",
    "\n",
    "Now we have trained a teacher model, and used that model to predict on unlabeled dataset to create more samples with psudo-labels.  \n",
    "It's time for swapping the teacher with the student!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds = prepare_for_training(\n",
    "        combined_ds, \n",
    "        config[\"batch_size\"], \n",
    "        cache=None,\n",
    "        shuffle_buffer_size=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the layers. I.E we're just using the pre-trained weights as initial weigths and biases and train over them\n",
    "efficientnet_base.trainable = True\n",
    "\n",
    "# Define model\n",
    "stud_model = Sequential()\n",
    "stud_model.add(efficientnet_base)\n",
    "stud_model.add(layers.GlobalAveragePooling2D())\n",
    "# en_model.add(layers.Dropout(0.2))\n",
    "# en_model.add(layers.Dense(256, activation='relu'))\n",
    "stud_model.add(layers.Dense(params[\"num_classes\"], activation=config[\"final_activation\"]))\n",
    "\n",
    "if config['optimizer'] == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "stud_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_steps = params[\"train_size\"]+new_samples_counter // config[\"batch_size\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = stud_model.fit(\n",
    "    combined_ds,\n",
    "    steps_per_epoch = train_steps, # might have to reduce\n",
    "    epochs = 2,\n",
    "    validation_data = test_ds, # what to use for validation data?\n",
    "    validation_steps = test_steps,\n",
    "    validation_freq = 1,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Iterate this algorithm a few times by treating the student as a teacher to relabel the unlabeled data and training a new student"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
