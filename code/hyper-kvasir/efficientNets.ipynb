{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kvasir dataset split into neg/pos and trained using Resnet50 without augmentation. Getting some decent results after training on resampled data with large step-size.  \n",
    "- Class weighting  \n",
    "- Resampling  \n",
    "- Initial Bias-estimation\n",
    "- Decreasing learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Some stuff to make utils-function work\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from data_prep import create_dataset, print_class_info, show_image\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/labeled/')\n",
    "\n",
    "config = {\n",
    "    # Dataset\n",
    "    \"data_dir\": data_dir,\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"ds_info\": 'hyp-kva',\n",
    "    \"resample\": True,\n",
    "    \"shuffle_buffer_size\": 0,\n",
    "    \"neg_class\": ['normal-cecum'],\n",
    "    \"outcast\": None,\n",
    "    # Model\n",
    "    \"model\": 'EfficientNetB0',\n",
    "    \"num_epochs\": 80,\n",
    "    \"batch_size\": 128,\n",
    "    \"img_shape\": (64, 64, 3),\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"final_activation\": 'softmax',\n",
    "    # Callbacks\n",
    "    \"learning_schedule\": True,\n",
    "    \"checkpoint\": True,\n",
    "    \"early_stopping\": True,\n",
    "    \"early_stopping_patience\": 10,\n",
    "    \"decay_rate\": 0.05,              # higher number gives steeper dropoff\n",
    "    # Misc\n",
    "    \"verbosity\": 1\n",
    "    }\n",
    "\n",
    "model_name = '{}x{}x{}_{}_{}'.format(config[\"num_epochs\"], config[\"batch_size\"], \n",
    "                                     config[\"img_shape\"][1], config[\"ds_info\"], config[\"model\"])\n",
    "\n",
    "fine_tune_from = 130\n",
    "fine_tune_epochs = 30\n",
    "early_stopping_patience = config[\"early_stopping_patience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, testing and validation dataset from utils/data_prep.py.  \n",
    "Returns tf.dataset for shuffled, cached and batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, test_ds, val_ds, params = create_dataset(config)\n",
    "\n",
    "train_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "test_steps = params[\"test_size\"] // config[\"batch_size\"]\n",
    "val_steps = params[\"val_size\"] // config[\"batch_size\"]\n",
    "class_names = params[\"class_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Train a teacher model on labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet import EfficientNetB0 as EfficientNet\n",
    "\n",
    "# from efficientnet import center_crop_and_resize, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_base = EfficientNet(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False, \n",
    "    input_shape=config[\"img_shape\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the layers. I.E we're just using the pre-trained weights as initial weigths and biases and train over them\n",
    "efficientnet_base.trainable = True\n",
    "\n",
    "# Define model\n",
    "en_model = Sequential()\n",
    "en_model.add(efficientnet_base)\n",
    "en_model.add(layers.GlobalAveragePooling2D())\n",
    "# en_model.add(layers.Dropout(0.2))\n",
    "# en_model.add(layers.Dense(256, activation='relu'))\n",
    "en_model.add(layers.Dense(params[\"num_classes\"], activation=config[\"final_activation\"]))\n",
    "\n",
    "if config['optimizer'] == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "en_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using LearnignRateScheduler\n",
    "initial_learning_rate = config[\"learning_rate\"]\n",
    "decay_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "batch_size = config['batch_size']\n",
    "decay_rate = config['decay_rate']\n",
    "\n",
    "def schedule(epoch):\n",
    "    # calculate new learning rate\n",
    "    learning_rate = initial_learning_rate / (1 + decay_rate * (epoch*batch_size) / decay_steps)\n",
    "    \n",
    "    # update tensorboard\n",
    "    tf.summary.scalar(name='learning_rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "log_dir=\"./logs/{}/\".format(config[\"model\"]) + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "lr_schedule_cb = LearningRateScheduler(schedule, verbose=1)\n",
    "earlystopp_cb = EarlyStopping(monitor='val_loss',verbose=1, patience=early_stopping_patience, restore_best_weights=True)\n",
    "checkpoint_cb = ModelCheckpoint(filepath='./models/best_cp-{epoch:03d}.hdf', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "\n",
    "callbacks = [tensorboard_cb]\n",
    "if config[\"early_stopping\"]: callbacks.append(earlystopp_cb)\n",
    "if config[\"learning_schedule\"]: callbacks.append(lr_schedule_cb)\n",
    "if config[\"checkpoint\"]: callbacks.append(checkpoint_cb)\n",
    "\n",
    "# Write config dictionary to text file\n",
    "f = open(log_dir+\"/config.txt\",\"w\")\n",
    "f.write(str(config))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = en_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = train_steps,\n",
    "    epochs = config[\"num_epochs\"],\n",
    "    validation_data = test_ds,\n",
    "    validation_steps = test_steps,\n",
    "    validation_freq = 1,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc =  str(history.history[\"val_sparse_categorical_accuracy\"][-1])[2:4]\n",
    "en_model.save('./models/{}.h5'.format(model_name+best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if validation data is cached\n",
    "cache_dir = config[\"cache_dir\"]\n",
    "img_width = config[\"img_shape\"][0]\n",
    "ds_info = config[\"ds_info\"]\n",
    "filename = \"{}/{}_{}_val.tfcache.index\".format(cache_dir, img_width, ds_info)\n",
    "\n",
    "cached = os.path.isfile(filename)\n",
    "if not cached:\n",
    "    # Iterate over dataset to initialize caching\n",
    "    for batch in val_ds.take(params[\"val_size\"]):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_evaluate = en_model.evaluate(val_ds, verbose=2, steps=val_steps)\n",
    "\n",
    "# Write evaluate dictionary to text file\n",
    "f = open(log_dir+\"/evaluate.txt\",\"w\")\n",
    "f.write( str(en_evaluate) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "if config[\"learning_schedule\"]: lr = history.history['lr']\n",
    "epochs_range = range(history.epoch[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"learning_schedule\"]:\n",
    "    # Plot the learning rate\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs_range, lr, label='Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learnign rate')\n",
    "    plt.savefig(log_dir+'/learning_rate.png')\n",
    "    plt.title('Adaptive Learning Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-val accuracy and loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0.0, 3])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(log_dir+'/accuracy_and_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the predictions on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch of validation data\n",
    "for images, labels in val_ds.take(1):\n",
    "    pass\n",
    "\n",
    "idx = np.random.randint(0, config[\"batch_size\"])\n",
    "\n",
    "# Take one image and convert it to numpy\n",
    "img = images.numpy()[idx]\n",
    "lab = labels.numpy()[idx]\n",
    "# Add one dimension\n",
    "print (\"label:\", class_names[lab], end='\\n\\n')\n",
    "show_image(img)\n",
    "img = np.expand_dims(img, 0)\n",
    "\n",
    "prediction = en_model.predict(img, verbose=0)\n",
    "for i, pred in enumerate(prediction[0]):\n",
    "    if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the unlabeled `test` dataset (which are images taken from the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # the last item of parts is the filename\n",
    "    filename = parts[-1]\n",
    "    return filename\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [config[\"img_shape\"][0], config[\"img_shape\"][1] ])\n",
    "\n",
    "def process_path(file_path):\n",
    "    filename = get_filename(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir_unlabeled_test = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/unlabeled-test/')\n",
    "\n",
    "ds_size_unlabeled_test = len(list(data_dir_unlabeled_test.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled_test/'*.*g')\n",
    "list_ds_unlabeled_test = tf.data.Dataset.list_files(files_string)\n",
    "\n",
    "unlabeled_ds_test = list_ds_unlabeled_test.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run prediction on one random image, to verify the results after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some images and confidence levels to see how to model performs\n",
    "# Take one image of unlabeled-test set\n",
    "for img, name in unlabeled_ds_test.take(1):\n",
    "    # Convert to numpy and add dimension\n",
    "    print (\"Filename:\", str(name.numpy())[2:-1])\n",
    "    print (\"Label:\", str(name.numpy())[38:-5])\n",
    "    show_image(img.numpy())\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    print ()\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "        if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the findings with probability score higher than threshold to the training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset with 1 sample from training dataset\n",
    "img, lab = next(iter(train_ds.unbatch().take(1)))\n",
    "new_samples = tf.data.Dataset.from_tensors((img, lab))\n",
    "\n",
    "pred_confidence = 0.90\n",
    "new_samples_counter = 0\n",
    "img_list = []\n",
    "lab_list = []\n",
    "\n",
    "for batch in unlabeled_ds_test.batch(20):\n",
    "    images, filenames = batch\n",
    "    batch_preds = en_model.predict(images, verbose=0)\n",
    "    \n",
    "    for pred, image in zip(batch_preds, images):\n",
    "        highest_pred = np.max(pred)\n",
    "        if highest_pred > pred_confidence:\n",
    "            pred_idx = np.argmax(pred).astype(np.int32)\n",
    "            pred_class = class_names[pred_idx]\n",
    "            \n",
    "            img_list.append(image)\n",
    "            lab_list.append(pred_idx)\n",
    "            \n",
    "            new_samples_counter += 1\n",
    "        \n",
    "print (\"Found\", new_samples_counter, \"new samples from unlabeled_ds_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset out of list of findings\n",
    "new_findings = tf.data.Dataset.from_tensor_slices((img_list, lab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge new dataset with original\n",
    "new_train_ds = train_ds.unbatch().take(params[\"train_size\"]).concatenate(new_findings)\n",
    "\n",
    "print (\"Added new samples from unlabeled dataset to the training dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Training dataset size:\", params[\"train_size\"])\n",
    "\n",
    "num_elements = new_train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "print (\"New size:\", num_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the `full` unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # the last item of parts is the filename\n",
    "    filename = parts[-1]\n",
    "    return filename\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [config[\"img_shape\"][0], config[\"img_shape\"][1] ])\n",
    "\n",
    "def process_path(file_path):\n",
    "    filename = get_filename(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir_unlabeled = pathlib.Path('/home/henrik/master-thesis/data/hyper-kvasir/unlabeled/')\n",
    "\n",
    "ds_size_unlabeled = len(list(data_dir_unlabeled.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled/'*.*g')\n",
    "list_ds_unlabeled = tf.data.Dataset.list_files(files_string)\n",
    "\n",
    "unlabeled_ds = list_ds_unlabeled.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "print (\"Loaded {} images into unlabeled_ds.\".format(ds_size_unlabeled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run prediction on 1 image\n",
    "for img, name in unlabeled_ds.take(1):\n",
    "    # Convert to numpy and add dimension\n",
    "    print (\"File:\",str(name.numpy())[2:-1], end='\\n\\n')\n",
    "    show_image(img.numpy())\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    \n",
    "    # If probability above 1% print info\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "         if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate over unlabeled dataset and predict new sampels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 'append to list and convert to tensor'-method\n",
    "- cache takes too much space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "pred_confidence = 0.99\n",
    "new_samples_counter = 0\n",
    "total_time = time.time()\n",
    "img_list = []\n",
    "lab_list = []\n",
    "\n",
    "tqdm_predicting = tqdm(total=ds_size_unlabeled, desc='Predicting', position=0)\n",
    "tqdm_findings = tqdm(desc='Findings', position=1, bar_format='{desc}:{bar}{n_fmt}')\n",
    "\n",
    "for count, (image,label) in enumerate(unlabeled_ds):\n",
    "    img = np.expand_dims(image, 0)\n",
    "    pred = en_model.predict(img)\n",
    "    highest_pred = np.max(pred)\n",
    "    if highest_pred > pred_confidence:\n",
    "        pred_idx = np.argmax(pred).astype(np.int32)\n",
    "        #pred_class = class_names[pred_idx]\n",
    "\n",
    "        img_list.append(image)\n",
    "        lab_list.append(pred_idx)\n",
    "        \n",
    "        new_samples_counter += 1\n",
    "        tqdm_findings.update(1)\n",
    "    tqdm_predicting.update(1)\n",
    "        \n",
    "print (\"\\nTotal time: {:.3f} s\".format( time.time() - total_time ))\n",
    "print (\"Found\", new_samples_counter, \"new samples in unlabeled_ds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_findings = tf.data.Dataset.from_tensor_slices((img_list, lab_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_ds = train_ds.unbatch().take(params[\"train_size\"]).concatenate(new_findings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset is large, could exceed RAM - be careful\n",
    "print (\"Training dataset size:\", params[\"train_size\"])\n",
    "\n",
    "num_elements = new_train_ds.reduce(0, lambda x, _: x + 1).numpy()\n",
    "print (\"New size:\", num_elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Be careful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_ds_repbatch = new_train_ds.repeat().batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache new_train_ds\n",
    "cache_dir = config[\"cache_dir\"]\n",
    "img_width = config[\"img_shape\"][0]\n",
    "ds_info = config[\"ds_info\"]\n",
    "\n",
    "filename = \"{}/{}_{}_new_train_ds.tfcache.index\".format(cache_dir, img_width, ds_info)\n",
    "new_train_ds_repbatch = new_train_ds_repbatch.cache(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cached = os.path.isfile(filename)\n",
    "if not cached:\n",
    "    # Iterate over dataset to initialize caching\n",
    "    for batch in new_train_ds_repbatch.take(new_samples_counter+params[\"train_size\"]+100):\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
