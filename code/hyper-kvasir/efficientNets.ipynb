{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to create a student-teacher model where we first train a teacher on labeled data, and then use this teacher model to label more data, then we swap out the teacher with a student and train again over all the samples. \n",
    "- Try AutoAugment/RandAugment\n",
    "- Add regularization\n",
    "- Resampling  \n",
    "  - Make resampling func for unlab_ds\n",
    "- Create new unlab_ds without previous findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    " \n",
    "# Some stuff to make utils-function work\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from pipeline import create_dataset, split_and_create_dataset, prepare_for_training\n",
    "from utils import show_image, class_distribution, print_split_info, unpipe\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/home/henriklg/master-thesis/data/hyper-kvasir/labeled_ttv/')\n",
    "unlab_dir = pathlib.Path('/home/henriklg/master-thesis/data/hyper-kvasir/unlabeled_ttv/')\n",
    "model_name = \"teacher1\"\n",
    "log_dir = \"./logs/{}/{}\".format(project_time, model_name)\n",
    "\n",
    "conf = {\n",
    "    # Dataset\n",
    "    \"data_dir\": data_dir,\n",
    "    \"unlab_dir\": unlab_dir,\n",
    "    \"log_dir\": log_dir,\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"ds_info\": 'hypkva',\n",
    "    \"augment\": [\"crop\",\"flip\",\"brightness\",\"saturation\",\"contrast\",\"rotate\"],\n",
    "    \"aug_mult\": 0.1,\n",
    "    \"resample\": True,\n",
    "    \"class_weight\": False,\n",
    "    \"shuffle_buffer_size\": 2000,        # 0=no shuffling\n",
    "    \"seed\": 2511,\n",
    "    \"neg_class\": None,                 # select neg class for binary ds (normal class)\n",
    "    \"outcast\": None,                   # list of folders to drop - currently only works for 1 item\n",
    "    # Model\n",
    "    \"model_name\": model_name,\n",
    "    \"model\": 'EfficientNetB0',\n",
    "    \"dropout\": 0.1,\n",
    "    \"num_epochs\": 25,\n",
    "    \"batch_size\": 128,\n",
    "    \"img_shape\": (128, 128, 3),\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"final_activation\": 'softmax',     # sigmoid for binary ds\n",
    "    # Callbacks\n",
    "    \"learning_schedule\": True,\n",
    "    \"decay_rate\": 0.1,                 # higher number gives steeper lr dropoff\n",
    "    \"checkpoint\": False,\n",
    "    \"early_stopp\": True,\n",
    "    \"early_stopp_patience\": 8,\n",
    "    # Misc\n",
    "    \"verbosity\": 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, testing and validation dataset from utils/data_prep.py.  \n",
    "Returns tf.dataset for shuffled, cached and batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = create_dataset(conf)\n",
    "\n",
    "class_names = conf[\"class_names\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import checkout_dataset\n",
    "\n",
    "# Show some images from training dataset - mainly to verify augmentation and distribution\n",
    "# add params for title and log_dir for savefig\n",
    "checkout_dataset(ds[\"train\"], conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Train a teacher model on labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Model, load_model\n",
    "from create_model import create_model\n",
    "\n",
    "teacher_model = create_model(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_model import create_callbacks\n",
    "\n",
    "callbacks = create_callbacks(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import write_to_file\n",
    "\n",
    "write_to_file(conf, conf, \"conf\") #ignore stupid arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_class_weights\n",
    "\n",
    "class_weights = get_class_weights(ds[\"train\"], conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the teacher model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "teacher_history = teacher_model.fit(\n",
    "        ds[\"train\"],\n",
    "        steps_per_epoch = conf[\"steps\"][\"train\"],\n",
    "        epochs = conf[\"num_epochs\"],\n",
    "        validation_data = ds[\"test\"],\n",
    "        validation_steps = conf[\"steps\"][\"test\"],\n",
    "        validation_freq = 1,\n",
    "        class_weight = class_weights,\n",
    "        callbacks = callbacks,\n",
    "        verbose = 1\n",
    ")\n",
    "print (\"Time spent on training: {}\".format(time.time() - start_time))\n",
    "\n",
    "# Save the metrics from training\n",
    "write_to_file(teacher_history.history, conf, \"teacher_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save or restore a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model.save(conf[\"log_dir\"]+'/model')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "model = tf.keras.models.load_model(conf[\"log_dir\"]+'/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_evaluate = teacher_model.evaluate(ds[\"val\"], verbose=2, steps=conf[\"steps\"][\"val\"])\n",
    "\n",
    "write_to_file(teacher_evaluate, conf, \"evaluate_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import plot_lr_and_accuracy\n",
    "\n",
    "plot_lr_and_accuracy(teacher_history, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_evaluation import display_classification_report\n",
    "from model_evaluation import get_metrics, get_confusion_matrix\n",
    "from model_evaluation import show_dataset_predictions\n",
    "from model_evaluation import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = unpipe(ds[\"val\"], conf[\"ds_sizes\"][\"val\"]).as_numpy_iterator()\n",
    "eval_ds = np.array(list(eval_ds))\n",
    "eval_images = np.stack(eval_ds[:,0], axis=0)\n",
    "\n",
    "predictions = teacher_model.predict(eval_images, verbose=1)\n",
    "pred_confidence = [np.max(pred) for pred in predictions]\n",
    "\n",
    "true_labels = list(eval_ds[:,1])\n",
    "pred_labels = [np.argmax(pred) for pred in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics(true_labels, pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_classification_report(\n",
    "        true_labels, \n",
    "        pred_labels, \n",
    "        range(conf[\"num_classes\"]), \n",
    "        target_names=conf[\"class_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = get_confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "plot_confusion_matrix(cm, log_dir, conf[\"class_names\"], figsize=(12,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display grid of some random samples from validation data with the prediction confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_dataset_predictions(\n",
    "        true_labels,\n",
    "        pred_labels,\n",
    "        pred_confidence,\n",
    "        eval_images,\n",
    "        conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: use the teacher to generate pseudo labels on unlabeled images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import create_unlab_ds\n",
    "\n",
    "unlab_ds, unlab_size = create_unlab_ds(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run predictions on all unlabeled images\n",
    "Using 'append to list and convert to tensor'-method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from utils import print_bar_chart\n",
    "from utils import get_tqdm\n",
    "\n",
    "pred_confidence = 0.80\n",
    "new_findings = 0\n",
    "count = 0\n",
    "\n",
    "pred_list = []\n",
    "lab_list = []\n",
    "name_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "tqdm_predicting, tqdm_findings = get_tqdm(unlab_size, count, new_findings)\n",
    "\n",
    "print (\"Press 'Interrupt Kernel' to save and exit.\")\n",
    "try:\n",
    "    for count, (image,path) in enumerate(unlab_ds, start=count):\n",
    "        img = np.expand_dims(image, 0)\n",
    "        pred = teacher_model.predict(img)\n",
    "        highest_pred = np.max(pred)\n",
    "        if highest_pred > pred_confidence:\n",
    "            pred_idx = np.argmax(pred).astype(np.uint8)\n",
    "\n",
    "            lab_list.append(pred_idx)\n",
    "            pred_list.append(highest_pred)\n",
    "            name_list.append(path)\n",
    "            \n",
    "            # Clear old bar chart, generate new one and refresh the tqdm progress bars\n",
    "            # NB, tqdm run-timer is also reset, unfortunately\n",
    "            if not new_findings%500 and new_findings>100:\n",
    "                clear_output(wait=True)\n",
    "                tqdm_predicting, tqdm_findings = get_tqdm(unlab_size, count, new_findings)\n",
    "                lab_array = np.asarray(lab_list, dtype=np.uint8)\n",
    "                findings = np.bincount(lab_array, minlength=int(conf[\"num_classes\"]))\n",
    "                print_bar_chart([findings], conf)\n",
    "                \n",
    "            new_findings += 1\n",
    "            tqdm_findings.update(1)\n",
    "        tqdm_predicting.update(1)\n",
    "except KeyboardInterrupt:\n",
    "    print (\"Exiting\")\n",
    "\n",
    "finally:\n",
    "    print (\"\\nTotal run time: {:.3f} s\".format( time.time() - total_time ))\n",
    "    print (\"Found {} new samples in unlabeled_ds after looking at {} images.\".format(new_findings, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot bar chart and save it (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_array = np.asarray(lab_list, dtype=np.uint8)\n",
    "findings = np.bincount(lab_array, minlength=int(conf[\"num_classes\"]))\n",
    "\n",
    "print_bar_chart(\n",
    "    data=[findings],\n",
    "    conf=conf,\n",
    "    title=None,\n",
    "    fname=\"bar_chart-findings_teacher\",\n",
    "    figsize=(16,7)\n",
    ")\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the image and labels list as pickle dump (optional)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Dump it as a pickle\n",
    "import pickle\n",
    "\n",
    "with open('{}/lab_list.pkl'.format(conf[\"log_dir\"]), 'wb') as f:\n",
    "    pickle.dump(lab_list, f)\n",
    "    \n",
    "with open('{}/img_list.pkl'.format(conf[\"log_dir\"]), 'wb') as f:\n",
    "    pickle.dump(img_list, f)\n",
    "\n",
    "with open('{}/pred_list.pkl'.format(conf[\"log_dir\"]), 'wb') as f:\n",
    "    pickle.dump(pred_list, f)\n",
    "    \n",
    "with open('{}/path_list.pkl'.format(conf[\"log_dir\"]), 'wb') as f:\n",
    "    pickle.dump(path_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the classified images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort new samples after prediction confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import custom_sort\n",
    "\n",
    "pred_list, lab_list, name_list = custom_sort(pred_list, lab_list, name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display samples from all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import checkout_findings\n",
    "\n",
    "unlabeled_findings = [pred_list, lab_list, name_list]\n",
    "checkout_findings(unlabeled_findings, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print grid of images from one of the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils import checkout_class\n",
    "\n",
    "checkout_class(\"pylorus\", unlabeled_findings, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample the new findings from unlabeled dataset to \"fit\" original distribution of samples per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import resample_unlab\n",
    "\n",
    "new_findings, new_filepaths = resample_unlab(unlabeled_findings, ds[\"clean_train\"], conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert image and label lists to tensors and combine with training_ds to create a new dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf.tensor of the new findings\n",
    "findings_tensor = tf.data.Dataset.from_tensor_slices(new_findings)\n",
    "\n",
    "# combine with original training_ds (using clean_ds which is not augmented/repeated etc)\n",
    "ds[\"psuedo_train\"] = ds[\"clean_train\"].concatenate(findings_tensor)\n",
    "\n",
    "# count samples in the original and new/combined dataset\n",
    "_, dist_teacher1 = class_distribution(ds[\"clean_train\"], conf[\"num_classes\"])\n",
    "_, dist_student1 = class_distribution(ds[\"psuedo_train\"], conf[\"num_classes\"])\n",
    "\n",
    "# History of class distribution\n",
    "from utils import print_bar_chart\n",
    "print_bar_chart(\n",
    "    data=[dist_teacher1, dist_student1],\n",
    "    conf=conf,\n",
    "    title=None,\n",
    "    fname=\"bar_chart-distribution2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Refresh' unlabeled dataset by extracting samples already used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import reduce_dataset\n",
    "\n",
    "unlab_ds = reduce_dataset(unlab_ds, remove=new_filepaths)\n",
    "\n",
    "unlab_size_teacher = unlab_size\n",
    "unlab_size = unlab_size - len(new_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"number of samples added to training data and removed from unlab_ds:\", len(new_filepaths))\n",
    "\n",
    "print (\"\\noriginal unlab_ds_size:\", unlab_size_teacher)\n",
    "print (\"new unlab_ds_size:\", unlab_size)\n",
    "\n",
    "print (\"\\noriginal train_size:\", int(np.sum(dist_teacher1)))\n",
    "new_train_size = int(np.sum(dist_student1))\n",
    "print (\"new train dataset size:\", new_train_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Train a student model on the combination of labeled images and pseudo labeled images\n",
    "\n",
    "Now we have trained a teacher model, and used that model to predict on unlabeled dataset to create more samples with psudo-labels.  \n",
    "It's time for swapping the teacher with the student!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save teacher conf\n",
    "teacher_conf = conf\n",
    "\n",
    "# Make changes\n",
    "model_name = \"student1\"\n",
    "log_dir = \"./logs/{}/{}\".format(project_time, model_name)\n",
    "\n",
    "# Dataset\n",
    "conf[\"log_dir\"] = log_dir\n",
    "conf[\"ds_sizes\"][\"train\"] = new_train_size\n",
    "conf[\"aug_mult\"] = 0.5\n",
    "# Model\n",
    "conf[\"model_name\"] = model_name\n",
    "conf[\"model\"] = 'EfficientNetB2'\n",
    "conf[\"dropout\"] = 0.3\n",
    "conf[\"num_epochs\"] = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[\"train\"] = prepare_for_training(\n",
    "        ds=ds[\"psuedo_train\"], \n",
    "        ds_name='train_stud1',\n",
    "        num_classes=conf[\"num_classes\"],\n",
    "        conf=conf,\n",
    "        cache=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_model = create_model(conf)\n",
    "\n",
    "callbacks = create_callbacks(conf) \n",
    "\n",
    "class_weights = get_class_weights(ds[\"train\"], conf)\n",
    "\n",
    "write_to_file(conf, conf, \"conf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "stud_history = student_model.fit(\n",
    "    ds[\"train\"],\n",
    "    steps_per_epoch = conf[\"steps\"][\"train\"], \n",
    "    epochs = conf[\"num_epochs\"],\n",
    "    validation_data = ds[\"test\"],\n",
    "    validation_steps = conf[\"steps\"][\"test\"],\n",
    "    validation_freq = 1,\n",
    "    callbacks = callbacks\n",
    ")\n",
    "print (\"Time spent on training: {}\".format(time.time() - start_time))\n",
    "\n",
    "# Save the metrics from training\n",
    "write_to_file(stud_history.history, conf, \"student_history\")\n",
    "\n",
    "# Save the model\n",
    "student_model.save(conf[\"log_dir\"]+'/model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_ds = unpipe(ds[\"val\"], conf[\"ds_sizes\"][\"val\"]).as_numpy_iterator()\n",
    "eval_ds = np.array(list(eval_ds))\n",
    "eval_images = np.stack(eval_ds[:,0], axis=0)\n",
    "\n",
    "predictions = student_model.predict(eval_images, verbose=1)\n",
    "pred_confidence = [np.max(pred) for pred in predictions]\n",
    "\n",
    "true_labels = list(eval_ds[:,1])\n",
    "pred_labels = [np.argmax(pred) for pred in predictions]\n",
    "\n",
    "student_evaluate = student_model.evaluate(ds[\"val\"], verbose=2, steps=conf[\"steps\"][\"val\"])\n",
    "write_to_file(student_evaluate, conf, \"evaluate_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lr_and_accuracy(teacher_history, conf)\n",
    "# get_metrics(true_labels, pred_labels)\n",
    "display_classification_report(\n",
    "        true_labels, \n",
    "        pred_labels, \n",
    "        range(conf[\"num_classes\"]), \n",
    "        target_names=conf[\"class_names\"]\n",
    ")\n",
    "\n",
    "cm = get_confusion_matrix(true_labels, pred_labels)\n",
    "plot_confusion_matrix(cm, log_dir, conf[\"class_names\"], figsize=(10,8))\n",
    "\n",
    "show_dataset_predictions(\n",
    "        true_labels,\n",
    "        pred_labels,\n",
    "        pred_confidence,\n",
    "        eval_images,\n",
    "        conf,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.2: use the teacher to generate pseudo labels on unlabeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_confidence = 0.80\n",
    "new_findings = 0\n",
    "count = 0\n",
    "\n",
    "lab_list = []\n",
    "pred_list = []\n",
    "path_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = time.time()\n",
    "\n",
    "tqdm_predicting, tqdm_findings = get_tqdm(unlab_size, count, new_findings)\n",
    "\n",
    "print (\"Press 'Interrupt Kernel' to save and exit.\")\n",
    "try:\n",
    "    for count, (image,path) in enumerate(unlab_ds, start=count):\n",
    "        img = np.expand_dims(image, 0)\n",
    "        pred = student_model.predict(img)\n",
    "        highest_pred = np.max(pred)\n",
    "        if highest_pred > pred_confidence:\n",
    "            pred_idx = np.argmax(pred).astype(np.uint8)\n",
    "\n",
    "            lab_list.append(pred_idx)\n",
    "            pred_list.append(highest_pred)\n",
    "            name_list.append(path)\n",
    "            \n",
    "            # Clear old bar chart, generate new one and refresh the tqdm progress bars\n",
    "            # NB, tqdm run-timer is also reset, unfortunately\n",
    "            if not new_findings%500 and new_findings>100:\n",
    "                clear_output(wait=True)\n",
    "                tqdm_predicting, tqdm_findings = get_tqdm(unlab_size, count, new_findings)\n",
    "                lab_array = np.asarray(lab_list, dtype=np.uint8)\n",
    "                findings = np.bincount(lab_array, minlength=int(conf[\"num_classes\"]))\n",
    "                print_bar_chart([findings], conf)\n",
    "                \n",
    "            new_findings += 1\n",
    "            tqdm_findings.update(1)\n",
    "        tqdm_predicting.update(1)\n",
    "except KeyboardInterrupt:\n",
    "    print (\"Exiting\")\n",
    "\n",
    "finally:\n",
    "    print (\"\\nTotal run time: {:.3f} s\".format( time.time() - total_time ))\n",
    "    print (\"Found {} new samples in unlabeled_ds after looking at {} images.\".format(new_findings, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_array = np.asarray(lab_list, dtype=np.uint8)\n",
    "findings = np.bincount(lab_array, minlength=int(conf[\"num_classes\"]))\n",
    "\n",
    "print_bar_chart(\n",
    "    data=[findings],\n",
    "    conf=conf,\n",
    "    title=None,\n",
    "    fname=\"bar_chart-findings_student\",\n",
    "    figsize=(16,7)\n",
    ")\n",
    "clear_output(wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the classified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sort the new findings after confidence\n",
    "pred_list, lab_list, name_list = custom_sort(pred_list, lab_list, name_list)\n",
    "\n",
    "# Display samples for each class\n",
    "unlab_findings = [pred_list, lab_list, name_list]\n",
    "checkout_findings(unlab_findings, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print grid of images from one of the classes\n",
    "checkout_class(\"pylorus\", unlab_findings, conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare new training and unlabeled datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the new findings to original distribution\n",
    "# Create tf.tensors of new findings\n",
    "if conf[\"resample\"]:\n",
    "    new_findings, added_samples = resample_unlab(unlab_findings, ds[\"psuedo_train\"], conf)\n",
    "    findings_tensor = tf.data.Dataset.from_tensor_slices(new_findings)\n",
    "else:\n",
    "    added_samples = len(name_list)\n",
    "    img_list = [fn2img(name, conf[\"unlab_ds\"], conf[\"img_shape\"][0]) for name in name_list]\n",
    "    findings_tensor = tf.data.Dataset.from_tensor_slices([img_list, lab_list])\n",
    "\n",
    "# combine with previous training dataset\n",
    "ds[\"psuedo_train\"] = ds[\"psuedo_train\"].concatenate(findings_tensor)\n",
    "\n",
    "# count samples in the original and new/combined dataset\n",
    "_, dist_teacher2 = class_distribution(ds[\"psuedo_train\"], conf[\"num_classes\"])\n",
    "\n",
    "# Display history of class distribution\n",
    "print_bar_chart(\n",
    "    data=[dist_teacher1, dist_student1, dist_teacher2],\n",
    "    conf=conf,\n",
    "    title=None,\n",
    "    fname=\"bar_chart-distribution3\"\n",
    ")\n",
    "\n",
    "# Refresh the unlabeled dataset\n",
    "unlab_ds = reduce_dataset(unlab_ds, remove=new_filepaths)\n",
    "\n",
    "unlab_size_student = unlab_size\n",
    "unlab_size = unlab_size - len(added_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"number of samples added to training data and removed from unlab_ds:\", len(new_filepaths))\n",
    "\n",
    "print (\"\\noriginal unlab_ds_size:\", unlab_size_teacher)\n",
    "print (\"new unlab_ds_size:\", unlab_size)\n",
    "\n",
    "print (\"\\noriginal train_size:\", int(np.sum(dist_teacher1)))\n",
    "print (\"new train dataset size:\", int(np.sum(dist_student1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Iterate this algorithm a few times by treating the student as a teacher to relabel the unlabeled data and training a new student"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
