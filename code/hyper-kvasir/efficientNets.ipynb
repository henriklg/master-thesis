{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kvasir dataset split into neg/pos and trained using Resnet50 without augmentation. Getting some decent results after training on resampled data with large step-size.  \n",
    "- Class weighting  \n",
    "- Resampling  \n",
    "- Initial Bias-estimation\n",
    "- Decreasing learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some stuff to make utils-function work\n",
    "import sys\n",
    "sys.path.append('../utils')\n",
    "from data_prep import create_dataset, print_class_info, show_image\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Jupyter-specific\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path('/mnt/sdb/hyper-kvasir/labeled/')\n",
    "\n",
    "config = {\n",
    "    # Dataset\n",
    "    \"data_dir\": data_dir,\n",
    "    \"cache_dir\": \"./cache\",\n",
    "    \"ds_info\": 'hyp-kva',\n",
    "    \"resample\": True,\n",
    "    \"shuffle_buffer_size\": 0,\n",
    "    \"neg_class\": ['normal-cecum'],\n",
    "    \"outcast\": None,\n",
    "    # Model\n",
    "    \"model\": 'EfficientNetB0',\n",
    "    \"num_epochs\": 400,\n",
    "    \"batch_size\": 64,\n",
    "    \"img_shape\": (224, 224, 3),\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"optimizer\": 'Adam',\n",
    "    \"final_activation\": 'softmax',\n",
    "    # Callbacks\n",
    "    \"learning_schedule\": True,\n",
    "    \"checkpoint\": True,\n",
    "    \"early_stopping\": False,\n",
    "    \"early_stopping_patience\": 15,\n",
    "    \"decay_rate\": 0.05,              # higher number gives steeper dropoff\n",
    "    # Misc\n",
    "    \"verbosity\": 1\n",
    "    }\n",
    "\n",
    "model_name = '{}x{}x{}_{}_{}'.format(config[\"num_epochs\"], config[\"batch_size\"], \n",
    "                                     config[\"img_shape\"][1], config[\"ds_info\"], config[\"model\"])\n",
    "\n",
    "fine_tune_from = 130\n",
    "fine_tune_epochs = 30\n",
    "early_stopping_patience = config[\"early_stopping_patience\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training, testing and validation dataset from utils/data_prep.py.  \n",
    "Returns tf.dataset for shuffled, cached and batched data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemorrhoids                 :    6 | 0.06%\n",
      "barretts                    :   41 | 0.38%\n",
      "esophagitis-a               :  403 | 3.78%\n",
      "esophagitis-b-d             :  260 | 2.44%\n",
      "ulcerative-colitis-0-1      :   35 | 0.33%\n",
      "barretts-short-segment      :   53 | 0.50%\n",
      "cecum                       : 1009 | 9.46%\n",
      "pylorus                     :  999 | 9.37%\n",
      "retroflex-rectum            :  391 | 3.67%\n",
      "ulcerative-colitis-grade-2  :  443 | 4.15%\n",
      "ulcerative-colitis-grade-1  :  201 | 1.89%\n",
      "bbps-2-3                    : 1148 | 10.77%\n",
      "bbps-0-1                    :  646 | 6.06%\n",
      "ileum                       :    9 | 0.08%\n",
      "retroflex-stomach           :  764 | 7.17%\n",
      "normal-z-line               :  932 | 8.74%\n",
      "ulcerative-colitis-2-3      :   28 | 0.26%\n",
      "impacted-stool              :  131 | 1.23%\n",
      "polyps                      : 1028 | 9.64%\n",
      "dyed-resection-margins      :  989 | 9.28%\n",
      "dyed-lifted-polyps          : 1002 | 9.40%\n",
      "ulcerative-colitis-grade-3  :  133 | 1.25%\n",
      "ulcerative-colitis-1-2      :   11 | 0.10%\n",
      "\n",
      "Total number of images: 10662, in 23 classes.\n",
      "\n",
      "Dataset.list_files:  /mnt/sdb/hyper-kvasir/labeled/*/*.*g \n",
      "\n",
      "WARNING:tensorflow:Entity <function create_dataset.<locals>.get_label at 0x7f2264347dd0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Cell is empty\n",
      "WARNING: Entity <function create_dataset.<locals>.get_label at 0x7f2264347dd0> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Cell is empty\n",
      "[11  6  2 15 15 11  7  6 10  4]\n",
      "[11 19  6  6 20 14 14 18 19  6]\n",
      "[15  7 17 14  7 11 19 19 18 15]\n",
      "[19 15  7  7 11 15 19 11 19 14]\n",
      "[ 9 18 12 11 20  7 20 11 19 21]\n",
      "[18 18 18 18 19 12  9 11 11  9]\n",
      "[ 2  9  7  7 11  6 18  9  8 20]\n",
      "[11 11 19 11 12 11  6  7 20 19]\n",
      "[ 7  2  8 12 18  2 11  8 20  6]\n",
      "[ 9  8 19  2  6 20 11  2 18  7]\n",
      "\n",
      "Beginning resampling..\n",
      "---- Fractions ---- \n",
      "[0.00097656 0.00332031 0.03925781 0.02636719 0.00332031 0.00488281\n",
      " 0.09316406 0.09296875 0.03417969 0.04121094 0.02167969 0.11152343\n",
      " 0.05761719 0.00078125 0.06699219 0.08691406 0.00273437 0.01210937\n",
      " 0.09921875 0.09082031 0.09667969 0.01269531 0.00058594]\n",
      "\n",
      "---- Fractions after resampling ---.\n",
      "[0.   0.1  0.02 0.04 0.06 0.02 0.04 0.02 0.04 0.02 0.04 0.02 0.04 0.04\n",
      " 0.08 0.12 0.   0.02 0.02 0.02 0.02 0.16 0.06]\n",
      "\n",
      "Full dataset sample size:        10662\n",
      "Train dataset sample size:        7463\n",
      "Test dataset sample size:         1599\n",
      "Validation dataset sample size:   1599\n"
     ]
    }
   ],
   "source": [
    "train_ds, test_ds, val_ds, params = create_dataset(config)\n",
    "\n",
    "train_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "test_steps = params[\"test_size\"] // config[\"batch_size\"]\n",
    "val_steps = params[\"val_size\"] // config[\"batch_size\"]\n",
    "class_names = params[\"class_names\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1\n",
    "Train a teacher model on labeled images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from efficientnet import EfficientNetB0 as EfficientNet\n",
    "\n",
    "# from efficientnet import center_crop_and_resize, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnet_base = EfficientNet(\n",
    "    weights=\"imagenet\", # or weights='noisy-student'\n",
    "    include_top=False, \n",
    "    input_shape=config[\"img_shape\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze layers in resnet\n",
    "efficientnet_base.trainable = True\n",
    "\n",
    "# Define model\n",
    "en_model = Sequential()\n",
    "\n",
    "en_model.add(efficientnet_base)\n",
    "en_model.add(layers.GlobalAveragePooling2D())\n",
    "# en_model.add(layers.Dropout(0.2))\n",
    "# en_model.add(layers.Dense(256, activation='relu'))\n",
    "en_model.add(layers.Dense(params[\"num_classes\"], activation=config[\"final_activation\"]))\n",
    "\n",
    "if config['optimizer'] == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=config[\"learning_rate\"])\n",
    "elif config['optimizer'] == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=config[\"learning_rate\"])\n",
    "\n",
    "en_model.compile(\n",
    "    optimizer=opt,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b0 (Model)      (None, 7, 7, 1280)        4049564   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 23)                29463     \n",
      "=================================================================\n",
      "Total params: 4,079,027\n",
      "Trainable params: 4,037,011\n",
      "Non-trainable params: 42,016\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "en_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, LearningRateScheduler\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By using LearnignRateScheduler\n",
    "initial_learning_rate = config[\"learning_rate\"]\n",
    "decay_steps = params[\"train_size\"] // config[\"batch_size\"]\n",
    "batch_size = config['batch_size']\n",
    "decay_rate = config['decay_rate']\n",
    "\n",
    "def schedule(epoch):\n",
    "    # calculate new learning rate\n",
    "    learning_rate = initial_learning_rate / (1 + decay_rate * (epoch*batch_size) / decay_steps)\n",
    "    \n",
    "    # update tensorboard\n",
    "    tf.summary.scalar(name='learning_rate', data=learning_rate, step=epoch)\n",
    "    return learning_rate\n",
    "\n",
    "log_dir=\"./logs/{}/\".format(config[\"model\"]) + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_writer = tf.summary.create_file_writer(log_dir + \"/metrics\")\n",
    "file_writer.set_as_default()\n",
    "\n",
    "lr_schedule_cb = LearningRateScheduler(schedule, verbose=1)\n",
    "earlystopp_cb = EarlyStopping(monitor='val_loss',verbose=1, patience=early_stopping_patience, restore_best_weights=True)\n",
    "checkpoint_cb = ModelCheckpoint(filepath='./models/best_cp-{epoch:03d}.hdf', monitor='val_loss', save_best_only=True, mode='auto')\n",
    "tensorboard_cb = TensorBoard(log_dir=log_dir, update_freq='batch')\n",
    "\n",
    "callbacks = [tensorboard_cb]\n",
    "if config[\"early_stopping\"]: callbacks.append(earlystopp_cb)\n",
    "if config[\"learning_schedule\"]: callbacks.append(lr_schedule_cb)\n",
    "if config[\"checkpoint\"]: callbacks.append(checkpoint_cb)\n",
    "\n",
    "# Write config dictionary to text file\n",
    "f = open(log_dir+\"/config.txt\",\"w\")\n",
    "f.write(str(config))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 116 steps, validate for 24 steps\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 0.01.\n",
      "Epoch 1/400\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv2d/kernel:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'depthwise_conv2d/depthwise_kernel:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'batch_normalization_2/gamma:0', 'batch_normalization_2/beta:0', 'conv2d_4/kernel:0', 'batch_normalization_3/gamma:0', 'batch_normalization_3/beta:0', 'depthwise_conv2d_1/depthwise_kernel:0', 'batch_normalization_4/gamma:0', 'batch_normalization_4/beta:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'conv2d_6/kernel:0', 'conv2d_6/bias:0', 'conv2d_7/kernel:0', 'batch_normalization_5/gamma:0', 'batch_normalization_5/beta:0', 'conv2d_8/kernel:0', 'batch_normalization_6/gamma:0', 'batch_normalization_6/beta:0', 'depthwise_conv2d_2/depthwise_kernel:0', 'batch_normalization_7/gamma:0', 'batch_normalization_7/beta:0', 'conv2d_9/kernel:0', 'conv2d_9/bias:0', 'conv2d_10/kernel:0', 'conv2d_10/bias:0', 'conv2d_11/kernel:0', 'batch_normalization_8/gamma:0', 'batch_normalization_8/beta:0', 'conv2d_12/kernel:0', 'batch_normalization_9/gamma:0', 'batch_normalization_9/beta:0', 'depthwise_conv2d_3/depthwise_kernel:0', 'batch_normalization_10/gamma:0', 'batch_normalization_10/beta:0', 'conv2d_13/kernel:0', 'conv2d_13/bias:0', 'conv2d_14/kernel:0', 'conv2d_14/bias:0', 'conv2d_15/kernel:0', 'batch_normalization_11/gamma:0', 'batch_normalization_11/beta:0', 'conv2d_16/kernel:0', 'batch_normalization_12/gamma:0', 'batch_normalization_12/beta:0', 'depthwise_conv2d_4/depthwise_kernel:0', 'batch_normalization_13/gamma:0', 'batch_normalization_13/beta:0', 'conv2d_17/kernel:0', 'conv2d_17/bias:0', 'conv2d_18/kernel:0', 'conv2d_18/bias:0', 'conv2d_19/kernel:0', 'batch_normalization_14/gamma:0', 'batch_normalization_14/beta:0', 'conv2d_20/kernel:0', 'batch_normalization_15/gamma:0', 'batch_normalization_15/beta:0', 'depthwise_conv2d_5/depthwise_kernel:0', 'batch_normalization_16/gamma:0', 'batch_normalization_16/beta:0', 'conv2d_21/kernel:0', 'conv2d_21/bias:0', 'conv2d_22/kernel:0', 'conv2d_22/bias:0', 'conv2d_23/kernel:0', 'batch_normalization_17/gamma:0', 'batch_normalization_17/beta:0', 'conv2d_24/kernel:0', 'batch_normalization_18/gamma:0', 'batch_normalization_18/beta:0', 'depthwise_conv2d_6/depthwise_kernel:0', 'batch_normalization_19/gamma:0', 'batch_normalization_19/beta:0', 'conv2d_25/kernel:0', 'conv2d_25/bias:0', 'conv2d_26/kernel:0', 'conv2d_26/bias:0', 'conv2d_27/kernel:0', 'batch_normalization_20/gamma:0', 'batch_normalization_20/beta:0', 'conv2d_28/kernel:0', 'batch_normalization_21/gamma:0', 'batch_normalization_21/beta:0', 'depthwise_conv2d_7/depthwise_kernel:0', 'batch_normalization_22/gamma:0', 'batch_normalization_22/beta:0', 'conv2d_29/kernel:0', 'conv2d_29/bias:0', 'conv2d_30/kernel:0', 'conv2d_30/bias:0', 'conv2d_31/kernel:0', 'batch_normalization_23/gamma:0', 'batch_normalization_23/beta:0', 'conv2d_32/kernel:0', 'batch_normalization_24/gamma:0', 'batch_normalization_24/beta:0', 'depthwise_conv2d_8/depthwise_kernel:0', 'batch_normalization_25/gamma:0', 'batch_normalization_25/beta:0', 'conv2d_33/kernel:0', 'conv2d_33/bias:0', 'conv2d_34/kernel:0', 'conv2d_34/bias:0', 'conv2d_35/kernel:0', 'batch_normalization_26/gamma:0', 'batch_normalization_26/beta:0', 'conv2d_36/kernel:0', 'batch_normalization_27/gamma:0', 'batch_normalization_27/beta:0', 'depthwise_conv2d_9/depthwise_kernel:0', 'batch_normalization_28/gamma:0', 'batch_normalization_28/beta:0', 'conv2d_37/kernel:0', 'conv2d_37/bias:0', 'conv2d_38/kernel:0', 'conv2d_38/bias:0', 'conv2d_39/kernel:0', 'batch_normalization_29/gamma:0', 'batch_normalization_29/beta:0', 'conv2d_40/kernel:0', 'batch_normalization_30/gamma:0', 'batch_normalization_30/beta:0', 'depthwise_conv2d_10/depthwise_kernel:0', 'batch_normalization_31/gamma:0', 'batch_normalization_31/beta:0', 'conv2d_41/kernel:0', 'conv2d_41/bias:0', 'conv2d_42/kernel:0', 'conv2d_42/bias:0', 'conv2d_43/kernel:0', 'batch_normalization_32/gamma:0', 'batch_normalization_32/beta:0', 'conv2d_44/kernel:0', 'batch_normalization_33/gamma:0', 'batch_normalization_33/beta:0', 'depthwise_conv2d_11/depthwise_kernel:0', 'batch_normalization_34/gamma:0', 'batch_normalization_34/beta:0', 'conv2d_45/kernel:0', 'conv2d_45/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_47/kernel:0', 'batch_normalization_35/gamma:0', 'batch_normalization_35/beta:0', 'conv2d_48/kernel:0', 'batch_normalization_36/gamma:0', 'batch_normalization_36/beta:0', 'depthwise_conv2d_12/depthwise_kernel:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'conv2d_51/kernel:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'conv2d_52/kernel:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'depthwise_conv2d_13/depthwise_kernel:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_55/kernel:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_56/kernel:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'depthwise_conv2d_14/depthwise_kernel:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'conv2d_59/kernel:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_60/kernel:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'depthwise_conv2d_15/depthwise_kernel:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_61/kernel:0', 'conv2d_61/bias:0', 'conv2d_62/kernel:0', 'conv2d_62/bias:0', 'conv2d_63/kernel:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'conv2d_64/kernel:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['conv2d/kernel:0', 'batch_normalization/gamma:0', 'batch_normalization/beta:0', 'depthwise_conv2d/depthwise_kernel:0', 'batch_normalization_1/gamma:0', 'batch_normalization_1/beta:0', 'conv2d_1/kernel:0', 'conv2d_1/bias:0', 'conv2d_2/kernel:0', 'conv2d_2/bias:0', 'conv2d_3/kernel:0', 'batch_normalization_2/gamma:0', 'batch_normalization_2/beta:0', 'conv2d_4/kernel:0', 'batch_normalization_3/gamma:0', 'batch_normalization_3/beta:0', 'depthwise_conv2d_1/depthwise_kernel:0', 'batch_normalization_4/gamma:0', 'batch_normalization_4/beta:0', 'conv2d_5/kernel:0', 'conv2d_5/bias:0', 'conv2d_6/kernel:0', 'conv2d_6/bias:0', 'conv2d_7/kernel:0', 'batch_normalization_5/gamma:0', 'batch_normalization_5/beta:0', 'conv2d_8/kernel:0', 'batch_normalization_6/gamma:0', 'batch_normalization_6/beta:0', 'depthwise_conv2d_2/depthwise_kernel:0', 'batch_normalization_7/gamma:0', 'batch_normalization_7/beta:0', 'conv2d_9/kernel:0', 'conv2d_9/bias:0', 'conv2d_10/kernel:0', 'conv2d_10/bias:0', 'conv2d_11/kernel:0', 'batch_normalization_8/gamma:0', 'batch_normalization_8/beta:0', 'conv2d_12/kernel:0', 'batch_normalization_9/gamma:0', 'batch_normalization_9/beta:0', 'depthwise_conv2d_3/depthwise_kernel:0', 'batch_normalization_10/gamma:0', 'batch_normalization_10/beta:0', 'conv2d_13/kernel:0', 'conv2d_13/bias:0', 'conv2d_14/kernel:0', 'conv2d_14/bias:0', 'conv2d_15/kernel:0', 'batch_normalization_11/gamma:0', 'batch_normalization_11/beta:0', 'conv2d_16/kernel:0', 'batch_normalization_12/gamma:0', 'batch_normalization_12/beta:0', 'depthwise_conv2d_4/depthwise_kernel:0', 'batch_normalization_13/gamma:0', 'batch_normalization_13/beta:0', 'conv2d_17/kernel:0', 'conv2d_17/bias:0', 'conv2d_18/kernel:0', 'conv2d_18/bias:0', 'conv2d_19/kernel:0', 'batch_normalization_14/gamma:0', 'batch_normalization_14/beta:0', 'conv2d_20/kernel:0', 'batch_normalization_15/gamma:0', 'batch_normalization_15/beta:0', 'depthwise_conv2d_5/depthwise_kernel:0', 'batch_normalization_16/gamma:0', 'batch_normalization_16/beta:0', 'conv2d_21/kernel:0', 'conv2d_21/bias:0', 'conv2d_22/kernel:0', 'conv2d_22/bias:0', 'conv2d_23/kernel:0', 'batch_normalization_17/gamma:0', 'batch_normalization_17/beta:0', 'conv2d_24/kernel:0', 'batch_normalization_18/gamma:0', 'batch_normalization_18/beta:0', 'depthwise_conv2d_6/depthwise_kernel:0', 'batch_normalization_19/gamma:0', 'batch_normalization_19/beta:0', 'conv2d_25/kernel:0', 'conv2d_25/bias:0', 'conv2d_26/kernel:0', 'conv2d_26/bias:0', 'conv2d_27/kernel:0', 'batch_normalization_20/gamma:0', 'batch_normalization_20/beta:0', 'conv2d_28/kernel:0', 'batch_normalization_21/gamma:0', 'batch_normalization_21/beta:0', 'depthwise_conv2d_7/depthwise_kernel:0', 'batch_normalization_22/gamma:0', 'batch_normalization_22/beta:0', 'conv2d_29/kernel:0', 'conv2d_29/bias:0', 'conv2d_30/kernel:0', 'conv2d_30/bias:0', 'conv2d_31/kernel:0', 'batch_normalization_23/gamma:0', 'batch_normalization_23/beta:0', 'conv2d_32/kernel:0', 'batch_normalization_24/gamma:0', 'batch_normalization_24/beta:0', 'depthwise_conv2d_8/depthwise_kernel:0', 'batch_normalization_25/gamma:0', 'batch_normalization_25/beta:0', 'conv2d_33/kernel:0', 'conv2d_33/bias:0', 'conv2d_34/kernel:0', 'conv2d_34/bias:0', 'conv2d_35/kernel:0', 'batch_normalization_26/gamma:0', 'batch_normalization_26/beta:0', 'conv2d_36/kernel:0', 'batch_normalization_27/gamma:0', 'batch_normalization_27/beta:0', 'depthwise_conv2d_9/depthwise_kernel:0', 'batch_normalization_28/gamma:0', 'batch_normalization_28/beta:0', 'conv2d_37/kernel:0', 'conv2d_37/bias:0', 'conv2d_38/kernel:0', 'conv2d_38/bias:0', 'conv2d_39/kernel:0', 'batch_normalization_29/gamma:0', 'batch_normalization_29/beta:0', 'conv2d_40/kernel:0', 'batch_normalization_30/gamma:0', 'batch_normalization_30/beta:0', 'depthwise_conv2d_10/depthwise_kernel:0', 'batch_normalization_31/gamma:0', 'batch_normalization_31/beta:0', 'conv2d_41/kernel:0', 'conv2d_41/bias:0', 'conv2d_42/kernel:0', 'conv2d_42/bias:0', 'conv2d_43/kernel:0', 'batch_normalization_32/gamma:0', 'batch_normalization_32/beta:0', 'conv2d_44/kernel:0', 'batch_normalization_33/gamma:0', 'batch_normalization_33/beta:0', 'depthwise_conv2d_11/depthwise_kernel:0', 'batch_normalization_34/gamma:0', 'batch_normalization_34/beta:0', 'conv2d_45/kernel:0', 'conv2d_45/bias:0', 'conv2d_46/kernel:0', 'conv2d_46/bias:0', 'conv2d_47/kernel:0', 'batch_normalization_35/gamma:0', 'batch_normalization_35/beta:0', 'conv2d_48/kernel:0', 'batch_normalization_36/gamma:0', 'batch_normalization_36/beta:0', 'depthwise_conv2d_12/depthwise_kernel:0', 'batch_normalization_37/gamma:0', 'batch_normalization_37/beta:0', 'conv2d_49/kernel:0', 'conv2d_49/bias:0', 'conv2d_50/kernel:0', 'conv2d_50/bias:0', 'conv2d_51/kernel:0', 'batch_normalization_38/gamma:0', 'batch_normalization_38/beta:0', 'conv2d_52/kernel:0', 'batch_normalization_39/gamma:0', 'batch_normalization_39/beta:0', 'depthwise_conv2d_13/depthwise_kernel:0', 'batch_normalization_40/gamma:0', 'batch_normalization_40/beta:0', 'conv2d_53/kernel:0', 'conv2d_53/bias:0', 'conv2d_54/kernel:0', 'conv2d_54/bias:0', 'conv2d_55/kernel:0', 'batch_normalization_41/gamma:0', 'batch_normalization_41/beta:0', 'conv2d_56/kernel:0', 'batch_normalization_42/gamma:0', 'batch_normalization_42/beta:0', 'depthwise_conv2d_14/depthwise_kernel:0', 'batch_normalization_43/gamma:0', 'batch_normalization_43/beta:0', 'conv2d_57/kernel:0', 'conv2d_57/bias:0', 'conv2d_58/kernel:0', 'conv2d_58/bias:0', 'conv2d_59/kernel:0', 'batch_normalization_44/gamma:0', 'batch_normalization_44/beta:0', 'conv2d_60/kernel:0', 'batch_normalization_45/gamma:0', 'batch_normalization_45/beta:0', 'depthwise_conv2d_15/depthwise_kernel:0', 'batch_normalization_46/gamma:0', 'batch_normalization_46/beta:0', 'conv2d_61/kernel:0', 'conv2d_61/bias:0', 'conv2d_62/kernel:0', 'conv2d_62/bias:0', 'conv2d_63/kernel:0', 'batch_normalization_47/gamma:0', 'batch_normalization_47/beta:0', 'conv2d_64/kernel:0', 'batch_normalization_48/gamma:0', 'batch_normalization_48/beta:0'] when minimizing the loss.\n"
     ]
    }
   ],
   "source": [
    "history = en_model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch = train_steps,\n",
    "    epochs = config[\"num_epochs\"],\n",
    "    validation_data = test_ds,\n",
    "    validation_steps = test_steps,\n",
    "    validation_freq = 1,\n",
    "    callbacks = callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc =  str(history.history[\"val_sparse_categorical_accuracy\"][-1])[2:4]\n",
    "en_model.save('./models/{}.h5'.format(model_name+best_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_evaluate = en_model.evaluate(val_ds, verbose=2, steps=val_steps)\n",
    "\n",
    "# Write evaluate dictionary to text file\n",
    "f = open(log_dir+\"/evaluate.txt\",\"w\")\n",
    "f.write( str(en_evaluate) )\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['sparse_categorical_accuracy']\n",
    "val_acc = history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "if config[\"learning_schedule\"]: lr = history.history['lr']\n",
    "epochs_range = range(history.epoch[-1]+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if config[\"learning_schedule\"]:\n",
    "    # Plot the learning rate\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(epochs_range, lr, label='Learning Rate')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Learnign rate')\n",
    "    plt.savefig(log_dir+'/learning_rate.png')\n",
    "    plt.title('Adaptive Learning Rate');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot train-val accuracy and loss\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "# plt.ylim([0.5, 1])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim([0.0, 3])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.savefig(log_dir+'/accuracy_and_loss.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the predictions on validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one batch of validation data\n",
    "idx = np.random.randint(0, config[\"batch_size\"])\n",
    "for images, labels in val_ds.take(1):\n",
    "    # Take one image and convert it to numpy\n",
    "    img = images.numpy()[idx]\n",
    "    lab = labels.numpy()[idx]\n",
    "    # Add one dimension\n",
    "    print (\"label:\", class_names[lab], end='\\n\\n')\n",
    "    show_image(img)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "        if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filename(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    # the last item of parts is the filename\n",
    "    filename = parts[-1]\n",
    "    print (type(filename))\n",
    "    return filename\n",
    "\n",
    "def decode_img(img):\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "    # resize the image to the desired size.\n",
    "    return tf.image.resize(img, [config[\"img_shape\"][0], config[\"img_shape\"][1] ])\n",
    "\n",
    "def process_path(file_path):\n",
    "    filename = get_filename(file_path)\n",
    "    # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the unlabeled `test` dataset (which are images taken from the training data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "data_dir_unlabeled_test = pathlib.Path('/mnt/sdb/hyper-kvasir/unlabeled-test/')\n",
    "\n",
    "ds_size_unlabeled_test = len(list(data_dir_unlabeled_test.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled_test/'*.*g')\n",
    "list_ds_unlabeled_test = tf.data.Dataset.list_files(files_string)\n",
    "\n",
    "unlabeled_ds_test = list_ds_unlabeled_test.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next hurdle: get access to both dataset sample and prediction.  \n",
    "- Predict one and one image?\n",
    "- Predict all at once?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method works, but predicts one image at a time.. Slow?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one image of unlabeled-test set\n",
    "for img, name in unlabeled_ds_test.take(1):\n",
    "    # Convert to numpy and add dimension\n",
    "    print (\"Filename:\", str(name.numpy())[2:-1], end='\\n\\n')\n",
    "    show_image(img.numpy())\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "        if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the `full` unlabeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_unlabeled = pathlib.Path('/mnt/sdb/hyper-kvasir/unlabeled/')\n",
    "\n",
    "ds_size_unlabeled = len(list(data_dir_unlabeled.glob('*.*g')))\n",
    "\n",
    "files_string = str(data_dir_unlabeled/'*.*g')\n",
    "list_ds_unlabeled = tf.data.Dataset.list_files(files_string)\n",
    "\n",
    "unlabeled_ds = list_ds_unlabeled.map(process_path, num_parallel_calls=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one image of unlabeled-test set\n",
    "for img, name in unlabeled_ds.take(1):\n",
    "    # Convert to numpy and add dimension\n",
    "    print (\"File:\",str(name.numpy())[2:-1], end='\\n\\n')\n",
    "    show_image(img.numpy())\n",
    "    img = np.expand_dims(img.numpy(), 0)\n",
    "    prediction = en_model.predict(img, verbose=0)\n",
    "    \n",
    "    for i, pred in enumerate(prediction[0]):\n",
    "         if pred > 0.01: print(\"{:>5.2f}% {}\".format(pred*100, class_names[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
