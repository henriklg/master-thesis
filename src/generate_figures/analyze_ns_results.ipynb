{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze data from split0 and split1 folders generated by noisy_student and saves all figures in experiment/figures folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exp_dir = \"/home/henriklg/master-thesis/code/kvasir-capsule/experiments/cl_500\"\n",
    "exp_dir = \"/home/henriklg/master-thesis/code/kvasir-capsule/experiments/cl_500\"\n",
    "\n",
    "split0 = exp_dir+\"/split0\"\n",
    "split1 = exp_dir+\"/split1\"\n",
    "\n",
    "sub_dirs = [\"0_teacher\", \"0_student\", \"1_teacher\", \"1_student\", \"2_teacher\", \"2_student\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 16\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)    # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)   # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2d_old(line):\n",
    "    return \" \".join(line.split()).split(\" \")\n",
    "\n",
    "def l2d(line):\n",
    "    name = (line.split(\"  \")) # split on double whitespace\n",
    "    name = next(sub for sub in name if sub) # fetch first non-empty cell\n",
    "    name = name.strip() # remove whitespace from string\n",
    "    \n",
    "    metrics = (\" \".join(line.split()).split(\" \")[-4:])\n",
    "    return ([name]+metrics)\n",
    "\n",
    "def parse_classification_report(path):\n",
    "    class_m = {}\n",
    "    tot_m = {}\n",
    "    \n",
    "    if \"teacher\" in path:\n",
    "        model = \"teacher\"\n",
    "    else:\n",
    "        model = \"student\"\n",
    "    \n",
    "    with open(path) as file:\n",
    "        line = file.readline()\n",
    "        line = file.readline() # skip first line\n",
    "        line = file.readline()\n",
    "        while line:\n",
    "            data = l2d(line)\n",
    "            \n",
    "            class_m[data[0]] = {\n",
    "                \"prec\": float(data[1]),\n",
    "                \"rec\": float(data[2]),\n",
    "                \"f1\": float(data[3]),\n",
    "                \"model\": model\n",
    "            }\n",
    "            line = file.readline()\n",
    "\n",
    "            if len(line) == 1:\n",
    "                line = False\n",
    "                \n",
    "        line = file.readline()\n",
    "        tot_m[\"acc\"] = l2d_old(line)[1]\n",
    "        line = file.readline()\n",
    "        tot_m[\"macro\"] = l2d_old(line)[2:5]\n",
    "        line = file.readline()\n",
    "        tot_m[\"weighted\"] = l2d_old(line)[2:5]\n",
    "                \n",
    "    return class_m, tot_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def parse_history(path):\n",
    "    with open(path) as file:\n",
    "        # loss - acc - val_los - val_acc\n",
    "        loss = file.readline()\n",
    "        acc = file.readline()\n",
    "        val_loss = file.readline()\n",
    "        val_acc = file.readline()\n",
    "        \n",
    "        history = {\n",
    "            \"loss\": ast.literal_eval(loss.split(\":\")[-1].strip()),\n",
    "            \"acc\": ast.literal_eval(acc.split(\":\")[-1].strip()),\n",
    "            \"val_loss\": ast.literal_eval(val_loss.split(\":\")[-1].strip()),\n",
    "            \"val_acc\": ast.literal_eval(val_acc.split(\":\")[-1].strip())\n",
    "        }\n",
    "        \n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_history(history_list):\n",
    "    \n",
    "    result = [0]*int(len(history_list[0]))\n",
    "    for history in (history_list):\n",
    "        for idx, val in enumerate(history):\n",
    "            result[idx] += val\n",
    "            \n",
    "    result = [res/(len(history_list)) for res in result]\n",
    "    return (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data from experiment results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = [\"loss\", \"acc\", \"val_loss\", \"val_acc\"]\n",
    "hist0 = {key: [] for key in mets}\n",
    "hist1 = {key: [] for key in mets}\n",
    "\n",
    "lowest_epoch = 200\n",
    "for hist, split in zip([hist0, hist1], [split0, split1]):\n",
    "    for dir_ in sub_dirs:\n",
    "        path = \"{}/{}/history.txt\".format(split, dir_)\n",
    "        history = parse_history(path)\n",
    "    \n",
    "        hist[\"loss\"].append(history[\"loss\"])\n",
    "        hist[\"acc\"].append(history[\"acc\"])\n",
    "        hist[\"val_loss\"].append(history[\"val_loss\"])\n",
    "        hist[\"val_acc\"].append(history[\"val_acc\"])\n",
    "\n",
    "        if (len(history[\"loss\"]) < lowest_epoch):\n",
    "            lowest_epoch = len(history[\"loss\"])\n",
    "            \n",
    "# Shorten the history lists to the lowest epoch\n",
    "for hist in [hist0, hist1]:\n",
    "    for key, outerlist in hist.items():\n",
    "        hist[key] = [sublist[0:lowest_epoch] for sublist in outerlist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "def savefig(name):\n",
    "    path = exp_dir+\"/figures/\"\n",
    "    pathlib.Path(path).mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(path+name+\".pdf\", format=\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get results from training on split_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(lowest_epoch)\n",
    "\n",
    "plt.figure(figsize=(14,6));\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "for model in hist0[\"acc\"]:\n",
    "    plt.plot(x, model, linewidth=1.5)\n",
    "plt.legend(sub_dirs);\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");\n",
    "plt.title('Training accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "for model in hist0[\"loss\"]:\n",
    "    plt.plot(x, model, linewidth=1.5)\n",
    "plt.legend(sub_dirs);\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\");\n",
    "plt.tight_layout()\n",
    "plt.title('Training loss')\n",
    "savefig(\"split0_history_training\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,6));\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "for model in hist0[\"val_acc\"]:\n",
    "    plt.plot(x, model, linewidth=1.5)\n",
    "plt.legend(sub_dirs);\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");\n",
    "plt.title('Validation accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "for model in hist0[\"val_loss\"]:\n",
    "    plt.plot(x, model, linewidth=1.5)\n",
    "plt.legend(sub_dirs);\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\");\n",
    "plt.tight_layout()\n",
    "plt.title('Validation loss')\n",
    "savefig(\"split0_history_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_val_acc_0 = average_history(hist0[\"val_acc\"])\n",
    "avg_val_loss_0 = average_history(hist0[\"val_loss\"])\n",
    "\n",
    "plt.figure(figsize=(14,6));\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, avg_val_acc_0, linewidth=1.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");\n",
    "plt.title('Average validation accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, avg_val_loss_0, linewidth=1.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\");\n",
    "plt.tight_layout()\n",
    "plt.title('Average validation loss')\n",
    "savefig(\"split0_average_validation_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_contents_0 = []\n",
    "\n",
    "for model in sub_dirs:\n",
    "    report_file = \"{}/{}/classification_report.txt\".format(split0, model)\n",
    "    \n",
    "    _, tot_m = parse_classification_report(report_file)\n",
    "    report_contents_0.append(tot_m);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weighted and macro precision, recall and f1-score\n",
    "metrics0 = {\n",
    "    \"prec\": [],\n",
    "    \"rec\": [],\n",
    "    \"f1\": [],\n",
    "    \"acc\": []\n",
    "}\n",
    "\n",
    "metric = \"weighted\"\n",
    "\n",
    "for idx, content in enumerate(report_contents_0):\n",
    "    metrics0[\"acc\"].append(float(content[\"acc\"]))\n",
    "    metrics0[\"prec\"].append(float(content[metric][0]))\n",
    "    metrics0[\"rec\"].append(float(content[metric][1]))\n",
    "    metrics0[\"f1\"].append(float(content[metric][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(len(sub_dirs)))\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(\n",
    "    x,metrics0[\"prec\"],'r', \n",
    "    x,metrics0[\"rec\"],'b', \n",
    "    x,metrics0[\"f1\"],'g', \n",
    "    linewidth=1.5, marker='o'\n",
    ")\n",
    "plt.legend([\"Precision\",\"Recall\",\"F1-score\", \"acc\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weighted average score\")\n",
    "plt.tight_layout(pad=1.5)\n",
    "savefig(\"split0_prec_rec_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(\n",
    "    x,metrics0[\"acc\"],'r--', \n",
    "    x,metrics0[\"f1\"],'g-', \n",
    "    linewidth=1.5, marker='o'\n",
    ")\n",
    "plt.legend([\"Accuracy\", \"F1-Score\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weighted average score\")\n",
    "plt.tight_layout(pad=1.5)\n",
    "savefig(\"split0_acc_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get results from training on split_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(lowest_epoch)\n",
    "\n",
    "plt.figure(figsize=(14,6));\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "for model in hist1[\"acc\"]:\n",
    "    plt.plot(x, model, linewidth=1.5)\n",
    "plt.legend(sub_dirs);\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy on training data\");\n",
    "plt.title('Training accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "for model in hist1[\"loss\"]:\n",
    "    plt.plot(x, model, linewidth=1.5)\n",
    "plt.legend(sub_dirs);\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss on training data\");\n",
    "plt.tight_layout()\n",
    "plt.title('Training loss')\n",
    "savefig(\"split1_history_training\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(14,6));\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "for model in hist1[\"val_acc\"]:\n",
    "    plt.plot(x, model, linewidth=1.5)\n",
    "plt.legend(sub_dirs);\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");\n",
    "plt.title('Validation accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "for model in hist1[\"val_loss\"]:\n",
    "    plt.plot(x, model, linewidth=1.5)\n",
    "plt.legend(sub_dirs);\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\");\n",
    "plt.tight_layout()\n",
    "plt.title('Validation loss')\n",
    "savefig(\"split1_history_validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_val_acc_1 = average_history(hist1[\"val_acc\"])\n",
    "avg_val_loss_1 = average_history(hist1[\"val_loss\"])\n",
    "\n",
    "plt.figure(figsize=(14,6));\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, avg_val_acc_1, linewidth=1.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");\n",
    "plt.title('Average validation accuracy')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, avg_val_loss_1, linewidth=1.5)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\");\n",
    "plt.tight_layout()\n",
    "plt.title('Average validation loss')\n",
    "savefig(\"split1_average_validation_history\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_contents_1 = []\n",
    "\n",
    "for model in sub_dirs:\n",
    "    report_file = \"{}/{}/classification_report.txt\".format(split1, model)\n",
    "    \n",
    "    _, tot_m = parse_classification_report(report_file)\n",
    "    report_contents_1.append(tot_m);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare weighted and macro precision, recall and f1-score\n",
    "metrics1 = {\n",
    "    \"prec\": [],\n",
    "    \"rec\": [],\n",
    "    \"f1\": [],\n",
    "    \"acc\": []\n",
    "}\n",
    "\n",
    "metric = \"weighted\"\n",
    "\n",
    "for idx, content in enumerate(report_contents_1):\n",
    "    metrics1[\"acc\"].append(float(content[\"acc\"]))\n",
    "    metrics1[\"prec\"].append(float(content[metric][0]))\n",
    "    metrics1[\"rec\"].append(float(content[metric][1]))\n",
    "    metrics1[\"f1\"].append(float(content[metric][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(range(len(sub_dirs)))\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(\n",
    "    x,metrics1[\"prec\"],'r', \n",
    "    x,metrics1[\"rec\"],'b', \n",
    "    x,metrics1[\"f1\"],'g', \n",
    "    linewidth=1.5, marker='o'\n",
    ")\n",
    "plt.legend([\"Precision\",\"Recall\",\"F1-score\", \"acc\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weighted average score\")\n",
    "plt.tight_layout(pad=1.5)\n",
    "savefig(\"split1_prec_rec_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(\n",
    "    x,metrics1[\"acc\"],'r--', \n",
    "    x,metrics1[\"f1\"],'g-', \n",
    "    linewidth=1.5, marker='o'\n",
    ")\n",
    "plt.legend([\"Accuracy\", \"F1-Score\"])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weighted average score\")\n",
    "plt.tight_layout(pad=1.5)\n",
    "savefig(\"split1_acc_f1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Average the results of both splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(lowest_epoch)\n",
    "\n",
    "plt.figure(figsize=(14,6));\n",
    "# Subplot 1\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(\n",
    "    x, avg_val_acc_0, \n",
    "    x, avg_val_acc_1, \n",
    "    linewidth=1.5\n",
    ")\n",
    "plt.legend([\"split_0\", \"split_1\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\");\n",
    "plt.title('Average validation accuracy split_0 and split_1')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Subplot 2\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(\n",
    "    x, avg_val_loss_0, \n",
    "    x, avg_val_loss_1, \n",
    "    linewidth=1.5\n",
    ")\n",
    "plt.legend([\"split_0\", \"split_1\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\");\n",
    "plt.tight_layout()\n",
    "plt.title('Average validation loss split_0 and split_1')\n",
    "savefig(\"both_average_validation_history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = range(len(sub_dirs))\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(\n",
    "    x, metrics0[\"f1\"],\n",
    "    x, metrics1[\"f1\"], \n",
    "    linewidth=1.5\n",
    ")\n",
    "plt.legend([\"split_0\", \"split_1\"])\n",
    "plt.title(\"F1 score for split_0 and split_1\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"F1 score\")\n",
    "plt.tight_layout(pad=1.5)\n",
    "savefig(\"both_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_acc = average_history([metrics0[\"acc\"], metrics1[\"acc\"]])\n",
    "average_f1 = average_history([metrics0[\"f1\"], metrics1[\"f1\"]])\n",
    "\n",
    "plt.figure(figsize=(9,6))\n",
    "plt.plot(\n",
    "    x,average_acc,'r--', \n",
    "    x,average_f1,'g-', \n",
    "    linewidth=1.5, marker='o'\n",
    ")\n",
    "plt.legend([\"Accuracy\", \"F1-Score\"])\n",
    "plt.title(\"Average F1 and Accuracy metrics for split0 and split1\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Weighted average score\")\n",
    "plt.tight_layout(pad=1.5)\n",
    "savefig(\"both_avg_acc_f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for acc, f1 in zip(average_acc, average_f1):\n",
    "    print (round(acc,3), round(f1,3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
